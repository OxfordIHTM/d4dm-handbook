[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data for Decision Makers: Data Concepts and Applications",
    "section": "",
    "text": "Preface\nIn today’s data-driven world, the responsibility of public service demands more than experience and intuition; it requires evidence-based decision-making grounded in a deep understanding of data. For government officials at all levels, from local administrators to national policymakers, data is not just a tool - it is an indispensable asset in crafting policies that are effective, equitable, and accountable. Data for Decision Makers is developed with you in mind: to support those entrusted with public leadership in leveraging data to serve communities more effectively.\nAcross the domains of public health, education, transportation, environmental policy, and beyond, the availability of data has never been greater. But with this abundance comes complexity. Making sense of it - identifying relevant patterns, understanding root causes, evaluating outcomes, and anticipating future trends - requires more than access. It demands a strong foundation in the principles and practices of modern data use.\nThis course highlights how data literacy empowers government officials to navigate uncertainty, combat misinformation, and design policies that truly respond to the needs of the public. From statistical reasoning and geographic information systems to predictive modelling and real-time dashboards, the tools of data are transforming governance. Understanding these tools is essential to strengthening transparency, accountability, and public trust.\nThis course bridges the gap between technical expertise and policy leadership. It offers clear, accessible explanations of core data concepts alongside practical examples from the public sector. Whether your role involves strategic planning, budget allocation, programme evaluation, or legislative development, this course will help you make more informed, timely, and impactful decisions.\nPublic service is a profound responsibility. By embracing the potential of data, government leaders can enhance their ability to meet that responsibility with clarity, foresight, and integrity.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Data-driven decision-making\nIn an era defined by information, the ability to make sound decisions increasingly hinges on the intelligent use of data. Across sectors and industries, from healthcare and education to finance and public policy, decision-makers are confronted with unprecedented volumes of information. Yet, it is not the sheer quantity of data that holds value, but our capacity to interpret, understand, and apply it effectively.\nData is more than numbers on a spreadsheet; it is the language of modern insight. When approached with the right tools and understanding, it becomes a powerful asset for identifying patterns, predicting outcomes, evaluating strategies, and ultimately, improving results. For decision-makers, this means developing fluency not just in reading reports, but in questioning assumptions, validating sources, and interpreting results within context.\nUnderstanding modern data concepts - from statistical reasoning and data visualisation to machine learning and real-time analytics - is no longer optional. It is foundational. These concepts empower leaders to move beyond intuition and anecdote, and toward evidence-based action. As data continues to shape the world around us, the ability to engage with it critically and creatively is becoming an essential skill.\nThis course aims to equip its participants with both the conceptual grounding and practical knowledge to navigate this landscape. Whether you are a seasoned executive, a policy analyst, or an emerging leader, this course is designed to bridge the gap between data science and decision-making. It demystifies the tools and techniques of modern data analysis and offers real-world applications that demonstrate how data can drive progress and innovation.\nGood decisions are not just supported by data; they are shaped by those who know how to use it wisely.\nData-driven decision-making or DDDM refers to the process of making decisions based on data and information rather than intuition or experience alone. It involves collecting, analysing, interpreting, and presenting data to support decision-making processes(Choi et al., 2021; Ivacko et al., 2013; Stobierski, 2019).\nIn this approach, decisions are made by relying on facts, figures, trends patterns, and insights derived from data. The goal is to make objective, evidence-based decisions that are more accurate, consistent, and transparent.\nData-driven decision-making often involves the use of tools, techniques, and technologies such as data analytics, machine learning, artificial intelligence, and visualisation software. By leveraging these tools, organisations can transform raw data into actionable insights that drive better outcomes.\nIn today’s organisations, this approach has become increasingly important as it allows for more objective and accurate decision-making. The process typically includes identifying relevant data sources, applying analytical techniques, and leveraging technologies like machine learning, artificial intelligence, and visualisation tools to transform raw data to actionable insights that drive better outcomes.\nAn organisation that is data-driven also benefits in being able to spot opportunities and threats early. By analysing data regularly, organisations can anticipate changes and act before problems arise.\nSaving costs is another advantage. In a survey of executives of Fortune 1000 companies regarding their data investments since 2012 commissioned by the Harvard Business Review, nearly half (48.4%) of respondents report that they are documenting measurable results from their investments in big data and 80.7% of the executives describing their investments in big data as being successful (Bean, 2017; Stobierski, 2019).",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#sec-data-driven-decision",
    "href": "introduction.html#sec-data-driven-decision",
    "title": "1  Introduction",
    "section": "",
    "text": "Note 1.1: Features of data-driven decision-making\n\n\n\nData-driven decision-making is widely used in various fields such as business, healthcare, finance, education, and government. It allows organisations and individuals to:\n\nInformed Decisions - make decisions based on data rather than assumptions or guesswork;\nImproved Accuracy - educe errors and biases by relying on objective information;\nEfficiency - Optimise resources and processes by identifying trends, patterns, and inefficiencies;\nTransparency - ensure that decisions are made in an open and transparent manner; and,\nScalability - Apply to large-scale operations or complex problems where traditional methods may be insufficient.",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#sec-about-this-course",
    "href": "introduction.html#sec-about-this-course",
    "title": "1  Introduction",
    "section": "1.2 About this course",
    "text": "1.2 About this course\nIn this course, we will explore everything from the basics such as what data is and why it matters to more advanced topics like data collection, storage, analysis, and visualisation. Through practical examples and real-world applications, you’ll learn how to harness the power of data to drive insights, solve problems, and make informed decisions in fields ranging from business and technology to healthcare and beyond. By the end of this course, you’ll not only understand the importance of data but also be prepared to apply these concepts in your own work.\n\n1.2.1 Objectives\nAll these towards the overall objective of making a case for shifting to more data-driven decision-making processes.\nSpecifically, by the end of the course, participants are expected to be able to:\n\nArticulate the value of data driven decision making and programming;\nCritically assess a data by it source, format, structure, types, and classes;\nCritically evaluate the state of their own dataset based on stated best practices;\nOutline the strengths and weaknesses of various types of data tools;\nDemonstrate capacity to use spreadsheet software to clean, process, and structure data; and,\nDemonstrate capacity to use spreadsheet software to perform data analysis.\n\n\n\n1.2.2 Case studies\nTo achieve these objectives, the course employs the case-study method, an approach that involves in-depth examination of a specific individual, group, organisation, or event to understand a complex issue in its real-life context.\nFor this course, the five case studies (one for each of the next five chapters) provide a more nuanced narrative of opportunities and challenges of adopting a data-driven approach to decision-making specifically in the context of governance within governments (rather than just in businesses).\n\n\n1.2.3 The who, what, when, where, how, and why framework\nWhen going through these five case studies, it is recommended to first go through them using the who, what, when, where, how, and why framework as a way to get a firm grounding on the case study details.\nThe “who, what, when, where, how, and why” framework is a systematic approach to understanding and analysing data. Another term that can be used for this framework is descriptive metadata which is data that provides information about other data, but not the content itself. So, if I have an image, the metadata wouldn’t be the actual picture, but the details about who took it, when, or where.\nHere’s a structured explanation of each component within this framework:\n\nWho\nRefers to the individuals or entities involved with the data. This includes stakeholders, users, customers, employees, or business partners who interact with or are affected by the data. More specifically, this may include, among others, information on:\n\nwho owns the data;\nwho manages the data;\nwho collects the data;\nwho stores the data; and,\nwho protects/safeguards the data.\n\n\n\nWhat\nDescribes what the data is about and its type, nature, and provenance. It specifies what information is available, such as numerical data, text, images, etc., which helps in understanding the scope and relevance of the data, and how to work with the data.\n\n\nWhen\nPertains to the timing, period, and/or frequency in which the data was/is being collected, recorded, or analysed.\n\n\nWhere\nIndicates the location where the data is stored or accessed. This could be within a database, on a server, or even from external sources like devices or sensors, providing context about data accessibility and storage.\n\n\nHow\nFocuses on the methods used to collect, process, or extract the data. This includes techniques such as surveys, sensor readings, or existing records, which helps in understanding how reliable and comprehensive the data is.\n\n\nWhy\nAsks for the purpose behind collecting and analysing the data. It clarifies why this information is being gathered i.e., whether it’s for reporting, decision-making, monitoring performance, or other objectives. This in turn guides appropriate actions based on the data insights.\n\n\nSummary\nUsing this structured approach helps clarify each aspect of data, ensuring clarity and focus. It is particularly useful for complex datasets and can help address varying questions based on the user’s role, such as an analyst versus a stakeholder.\nIn summary, using the “who, what, when, where, how, and why” framework provides a systematic method to identify key elements of data, ensuring clarity and focus in data management and analysis.\n \n\n\n\n\nBean, R. (2017). How companies say they’re using big data. Harvard Business Review. https://hbr.org/2017/04/how-companies-say-theyre-using-big-data\n\n\nChoi, Y., Gil-Garcia, J., Burke, G. B., Costello, J., Werthmuller, D., & Aranay, O. (2021). Towards data-driven decision-making in government: Identifying opportunities and challenges for data use and analytics. Hawaii international conference on system sciences. https://doi.org/10.24251/HICSS.2021.268\n\n\nIvacko, T. M., Horner, D., & Crawford, M. Q. (2013). Data-driven decision-making in michigan local government. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2351916\n\n\nStobierski, T. (2019, August 26). The advantages of data-driven decision-making. Business insights. https://online.hbs.edu/blog/post/data-driven-decision-making",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "all-about-data.html",
    "href": "all-about-data.html",
    "title": "2  All about data",
    "section": "",
    "text": "2.1 Data Sources\nIn this chapter, we go further into data concepts with a discussion on the sources, formats, structures, types, classes, and systems of data.\nData can be classified as either being of primary or secondary source.\nData sources also refer to where data was obtained or sourced from. These encompass a wide range of information repositories, from traditional databases and files to emerging online platforms and application programming interfaces (APIs).",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All about data</span>"
    ]
  },
  {
    "objectID": "all-about-data.html#sec-data-sources",
    "href": "all-about-data.html#sec-data-sources",
    "title": "2  All about data",
    "section": "",
    "text": "Primary data includes original data collected directly from primary sources such as experiments surveys, or interviews.\nSecondary data exists in various forms like reports, government statistics, or academic publications which are data that have been already collected primarily by some other person and/or organisation/entity who make such data available for others to use for either the same purpose or a totally different use-case altogether from the original purpose.",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All about data</span>"
    ]
  },
  {
    "objectID": "all-about-data.html#sec-data-formats",
    "href": "all-about-data.html#sec-data-formats",
    "title": "2  All about data",
    "section": "2.2 Data Formats",
    "text": "2.2 Data Formats\nData formats define how information is organised, stored, and accessed within a file or database. They determine the structure of data, such as text, numbers, or multimedia, using common formats like CSV, JSON, and XML, each with unique methods for representing data.\nData formats may specifically refer to the following:\n\nRecording format - a format for encoding data for storage on a storage medium\nFile format - a format for encoding data for storage in a computer file\nContainer format (digital) - a format for encoding data for storage by means of a standardised audio/video codecs file format\nContent format - a format for representing media content as data\nAudio format - format for encoded sound data",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All about data</span>"
    ]
  },
  {
    "objectID": "all-about-data.html#sec-data-structure",
    "href": "all-about-data.html#sec-data-structure",
    "title": "2  All about data",
    "section": "2.3 Data Structures",
    "text": "2.3 Data Structures\nA data structure is an organised format for storing data, designed to allow efficient access and modification. It encompasses not just the storage of data but also the relationships between data elements and the operations that can be performed on them. These operations are structured with defined behaviors where operations have specific properties.\nExamples of data structures include:\n\nRelational Databases - Organised into tables with defined relationships (e.g., SQL).\nNoSQL Systems - Flexible storage solutions like document stores or key-value systems.\nHierarchical Structures - Data organised in a tree-like structure, such as XML or JSON.\nFlat Structures - All data resides at the same logical level without hierarchy (e.g., JSON arrays).\nSemi-Structured Formats - Use tags and nested structures for complex data (e.g., JSON).",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All about data</span>"
    ]
  },
  {
    "objectID": "all-about-data.html#sec-data-types",
    "href": "all-about-data.html#sec-data-types",
    "title": "2  All about data",
    "section": "2.4 Data Types",
    "text": "2.4 Data Types\n\nCategorical - Data divided into categories (e.g., gender, color).\nNumerical - Involves numbers, which can be discrete or continuous.\nTemporal - Data with time-based attributes (e.g., dates, times).\nTextual -Includes natural language text and speech data.\nBinary - Represents presence/absence of a feature.\nSpatial - Geospatial data indicating locations (e.g., coordinates).\nMultimedia - Combines multiple types like images, audio, and video.",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All about data</span>"
    ]
  },
  {
    "objectID": "all-about-data.html#sec-data-systems",
    "href": "all-about-data.html#sec-data-systems",
    "title": "2  All about data",
    "section": "2.5 Data Systems",
    "text": "2.5 Data Systems\n\nDatabases - Platforms for managing and querying structured data, including relational (SQL) and NoSQL systems.\nData Lakes - repositories storing raw, unstructured, or semi-structured data in a lake-like structure.\nBig Data Systems - Designed to handle large-scale datasets with distributed processing.\nBusiness Intelligence Tools - Provide analytics capabilities for transforming data into actionable insights.",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All about data</span>"
    ]
  },
  {
    "objectID": "all-about-data.html#sec-data-integration",
    "href": "all-about-data.html#sec-data-integration",
    "title": "2  All about data",
    "section": "2.6 Integration and Considerations",
    "text": "2.6 Integration and Considerations\n\n2.6.1 Data flow\nData is collected from sources, processed or formatted as needed, organised into appropriate types and structures, and managed by suitable systems.\n\n\n2.6.2 Interconnected Components\nEach component (sources, formats, structures) plays a role in ensuring data compatibility with various systems, which are then used for classification based on specific needs.",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All about data</span>"
    ]
  },
  {
    "objectID": "case_study_one.html",
    "href": "case_study_one.html",
    "title": "3  Data use and analytics in water quality management",
    "section": "",
    "text": "3.1 Leadership role\nThis is a case study about the Division of Water (DOW), a local government agency in the State of New York, which has attempted to improve its analytic capabilities by developing efficient data management practices, suggest governance models, and identify analytic techniques potentially beneficial to addressing harmful algal blooms (HABs; see Figure 3.1) and high chloride concentrations(Choi et al., 2021).\nThe DOW faces challenges in using its legacy systems and traditional analytical methods effectively in addressing the problems of HABs and high chloride levels. DOW aims to enhance its decision-making processes through DDDM by improving its ability to gather and analyse data more effectively, beyond their current capabilities, to better inform policy decisions.\nFrom this process, nine key factors across four overarching determinants have been observed and articulated as being crucial to consider by an organisation in implementing a comprehensive strategy for DDDM (see Note 3.1). These factors interrelate and influence each other, requiring a holistic approach to ensure successful adoption.\nThese key determinants are interrelated and interdependent. For example, if an organisation has strong data infrastructure (determinant 1) but lacks the right analytical tools or skilled personnel (determinant 2), their DDDM efforts will be hampered. Similarly, even with good internal structures (determinant 3), if external regulations make it hard to access necessary tools or collaborate externally (determinants 7 and 9), progress is still limited. Without proper stakeholder engagement (determinant 6) and user involvement (determinant 5), the organisation might develop solutions in isolation, leading to less effective decisions. Moreover, privacy constraints (determinant 8) can affect data availability, which in turn impacts analytical capabilities since data is a key input.\nWhile DDDM is often seen as a technical issue involving tools and data, it’s also deeply influenced by organisational and institutional factors. This makes sense because any significant change requires not just new technology but also cultural shifts within the organisation to embrace these changes.\nThese determinants also influence the ability of an organisation to adapt over time. For example, if the organisation faces challenges in public procurement, which is a structural issue, this could create delays that affect the organisation’s overall strategy. Conversely, strong stakeholder engagement might mitigate some of these delays by providing alternative solutions or resources.\nLeadership plays a critical part in driving organisational change. Without supportive leadership, many of these determinants could be obstacles rather than opportunities. For instance, if leaders aren’t committed to DDDM, they might not push for necessary cultural shifts or investment in new tools.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data use and analytics in water quality management</span>"
    ]
  },
  {
    "objectID": "case_study_one.html#sec-balance-practices",
    "href": "case_study_one.html#sec-balance-practices",
    "title": "3  Data use and analytics in water quality management",
    "section": "3.2 Balancing existing practices",
    "text": "3.2 Balancing existing practices\nThe balance between existing practices and new methods is important. While the state agency was implementing DDDM, traditional approaches were still relied upon. This blend can be beneficial initially but may need careful management to avoid conflicts or inefficiencies as newer methods prove their worth.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data use and analytics in water quality management</span>"
    ]
  },
  {
    "objectID": "case_study_one.html#sec-measuring-success",
    "href": "case_study_one.html#sec-measuring-success",
    "title": "3  Data use and analytics in water quality management",
    "section": "3.3 Measuring success",
    "text": "3.3 Measuring success\nHow would this state agency assess its progress in implementing DDDM? They might look at metrics like the quality and timeliness of decisions, reduction in issues (like HABs), efficiency improvements, and user satisfaction. These outcomes can help gauge whether their efforts are paying off despite facing various challenges.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data use and analytics in water quality management</span>"
    ]
  },
  {
    "objectID": "case_study_one.html#sec-cs1-conclusion",
    "href": "case_study_one.html#sec-cs1-conclusion",
    "title": "3  Data use and analytics in water quality management",
    "section": "3.4 Conclusion",
    "text": "3.4 Conclusion\nA tailored strategy that evaluates specific organisational strengths and weaknesses across these determinants is essential for effective DDDM implementation. This approach ensures that each organisation maximises opportunities while minimising challenges, leading to more informed and efficient decision-making processes.\n \n\n\n\n\nChoi, Y., Gil-Garcia, J., Burke, G. B., Costello, J., Werthmuller, D., & Aranay, O. (2021). Towards data-driven decision-making in government: Identifying opportunities and challenges for data use and analytics. Hawaii international conference on system sciences. https://doi.org/10.24251/HICSS.2021.268",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data use and analytics in water quality management</span>"
    ]
  },
  {
    "objectID": "case_study_two.html",
    "href": "case_study_two.html",
    "title": "4  Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia",
    "section": "",
    "text": "4.1 Context\nIn an era where technology and data are transforming governance, adopting a data-driven approach is crucial for improving decision-making and fostering transparency. This case study explores Indonesia’s journey toward integrating data into local governance, highlighting both challenges and opportunities, and offers recommendations for mid-level government officials to enhance their governance strategies(Sayogo et al., 2024).\nIndonesia, the largest archipelagic nation in the world, operates under a federalist system with provinces and regencies. With a diverse population of over 270 million people, it faces significant challenges such as inequality, environmental degradation, and sustainable development. These issues necessitate effective local governance to ensure equitable growth and environmental preservation.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia</span>"
    ]
  },
  {
    "objectID": "case_study_two.html#sec-cs2-current-situation",
    "href": "case_study_two.html#sec-cs2-current-situation",
    "title": "4  Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia",
    "section": "4.2 Current Situation",
    "text": "4.2 Current Situation\nCurrently, Indonesia’s policy-making is often influenced by top-down directives rather than data-driven insights. Decisions are frequently based on the instructions of superior officials due to a history of autocratic administration. Additionally, there is a lack of standardised data quality frameworks, leading to fragmented and siloed data systems. Limited analytics capacity and reliance on outdated technologies further hinder effective decision-making.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia</span>"
    ]
  },
  {
    "objectID": "case_study_two.html#sec-cs2-challenges",
    "href": "case_study_two.html#sec-cs2-challenges",
    "title": "4  Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia",
    "section": "4.3 Challenges",
    "text": "4.3 Challenges\n\nAutocratic Administration A cultural tendency towards hierarchical decision-making limits the use of data in governance.\nFragmented Data Systems Siloed systems across different levels of government result in data inconsistencies and inefficiencies.\nLack of Skilled Personnel Insufficient training and expertise in data analysis impede effective data utilisation.\nPublic Distrust Concerns about data accuracy and misuse erode public confidence in data-driven decisions.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia</span>"
    ]
  },
  {
    "objectID": "case_study_two.html#sec-cs2-opportunities",
    "href": "case_study_two.html#sec-cs2-opportunities",
    "title": "4  Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia",
    "section": "4.4 Opportunities",
    "text": "4.4 Opportunities\n\nRecent Regulations The 2022 Data Governance Regulation provides a framework to standardize data collection and use.\nInternational Collaboration Partnerships with international organizations offer resources for capacity-building and technological support.\nAvailable Data Sources Rich datasets on demographics, environment, and economy can enhance policy-making, such as managing forest fires or coral reef preservation.\nCapacity-Building Training programs can equip officials with data analysis skills, fostering a culture of evidence-based decision-making.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia</span>"
    ]
  },
  {
    "objectID": "case_study_two.html#sec-cs2-recommendations",
    "href": "case_study_two.html#sec-cs2-recommendations",
    "title": "4  Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia",
    "section": "4.5 Recommendations",
    "text": "4.5 Recommendations\n\nDevelop Data Quality Frameworks Establish standardized protocols to ensure data accuracy and consistency across all levels of government.\nEnhance Analytical Skills Implement training programs to build expertise in data analysis and visualisation tools.\nFoster Public Trust Promote initiatives that demonstrate the benefits of data-driven decisions, such as improving public services or environmental outcomes.\nEncourage Collaboration Facilitate intergovernmental cooperation to share best practices and resources for effective data use.\nAdopt Technology Invest in integrated digital platforms to streamline data collection and sharing processes.\nEstablish Feedback Mechanisms Create channels for public input to ensure that data-driven policies reflect community needs and concerns.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia</span>"
    ]
  },
  {
    "objectID": "case_study_two.html#sec-cs2-conclusion",
    "href": "case_study_two.html#sec-cs2-conclusion",
    "title": "4  Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia",
    "section": "4.6 Conclusion",
    "text": "4.6 Conclusion\nIndonesia’s shift towards data-driven governance presents a transformative opportunity to address pressing challenges and enhance decision-making effectiveness. By overcoming existing barriers and leveraging available resources, Indonesia can set a precedent for other developing nations. Mid-level officials worldwide are encouraged to consider these insights in their own governance strategies, fostering a global culture of transparency, collaboration, and innovation in public service.\n \n\n\n\n\nSayogo, D. S., Yuli, S. B. C., & Amalia, F. A. (2024). Data-driven decision-making challenges of local government in indonesia. Transforming Government: People, Process and Policy, 18(1), 145–156. https://doi.org/10.1108/TG-05-2023-0058",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia</span>"
    ]
  },
  {
    "objectID": "case-study-three.html",
    "href": "case-study-three.html",
    "title": "5  The Use of Data in Local Governance - A Michigan Perspective",
    "section": "",
    "text": "5.1 Michigan Public Policy Survey\nThis case study explores the current state of data-driven decision-making in Michigan’s local governments, highlighting challenges and opportunities for integrating data into policy and governance based on the results of the Michigan Public Policy Survey (MPPS)(Ivacko et al., 2013).\nThe MPPS, established post the 2009 Great Recession, is the first ongoing survey of local leaders across an entire state in the United States, involving over 1,856 jurisdictions in Michigan. It addresses a critical gap by providing insights into local officials’ perspectives, crucial for informed policymaking. Conducted biannually, it tracks long-term trends on fiscal and operational policies while addressing current issues like the COVID-19 pandemic and infrastructure. Collaborations with key associations enhance its credibility and scope.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Use of Data in Local Governance - A Michigan Perspective</span>"
    ]
  },
  {
    "objectID": "case-study-three.html#sec-cs3-current-situation",
    "href": "case-study-three.html#sec-cs3-current-situation",
    "title": "5  The Use of Data in Local Governance - A Michigan Perspective",
    "section": "5.2 Current Situation of Policy and Decision-Making in Michigan Local Governments",
    "text": "5.2 Current Situation of Policy and Decision-Making in Michigan Local Governments\nMichigan’s local governments have seen significant growth in data-driven decision-making (see Figure 5.1 and Figure 5.2).\n\n\n\n\n\n\n\n\n\nFigure 5.1: Percentage of Michigan jurisdictions reporting use of performance data\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: Michigan jurisdictions’ history of data use (among current data users), by population density\n\n\n\n\nThis approach is now widespread across jurisdictions of all population sizes (see Figure 5.3) and across regions, with many jurisdictions using data to inform budgeting and resource allocation.\n\n\n\n\n\n\n\n\n\nFigure 5.3: Percentage of Michigan jurisdictions reporting data use, by population density\n\n\n\n\nDespite this progress, most data use remains informal or ad hoc (see Figure 5.4), particularly among smaller communities (see Figure 5.5).\n\n\n\n\n\n\n\n\n\nFigure 5.4: Percentage of Michigan jurisdictions reporting ad hoc vs. systematic data use (among data users)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.5: Percentage of Michigan jurisdictions reporting ad hoc vs. systematic data use (among data users), by population size\n\n\n\n\nThe MPPS reveals that while larger jurisdictions are more likely to engage in formal performance measurement, over half of the state’s smallest jurisdictions also incorporate some form of data into their decision-making processes (see Figure 5.2). This indicates a trend towards broader adoption, albeit at varying levels of formality.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Use of Data in Local Governance - A Michigan Perspective</span>"
    ]
  },
  {
    "objectID": "case-study-three.html#sec-cs3-challenges",
    "href": "case-study-three.html#sec-cs3-challenges",
    "title": "5  The Use of Data in Local Governance - A Michigan Perspective",
    "section": "5.3 Challenges and Concerns",
    "text": "5.3 Challenges and Concerns\n\nCost Concerns\n\nMany local governments, especially smaller ones with limited resources, perceive data use as costly. The MPPS found that 62% of non-data users cited cost concerns, though only 28% of current users reported significant issues, suggesting costs may be manageable.\n\nInformal Practices\n\nThe reliance on informal methods can lead to inconsistent outcomes and less accountability. Only about 16% of jurisdictions have formal performance measurement practices, indicating a gap in structured data use.\n\nResource Constraints\n\nSmaller jurisdictions often face limitations in staff and financial resources, hindering their ability to adopt more formal data practices.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Use of Data in Local Governance - A Michigan Perspective</span>"
    ]
  },
  {
    "objectID": "case-study-three.html#sec-cs3-opportunities",
    "href": "case-study-three.html#sec-cs3-opportunities",
    "title": "5  The Use of Data in Local Governance - A Michigan Perspective",
    "section": "5.4 Opportunities and Benefits",
    "text": "5.4 Opportunities and Benefits\n\nFiscal Efficiency\n\nData-driven approaches help identify cost savings and program efficiencies, crucial for jurisdictions grappling with fiscal challenges.\n\nImproved Service Delivery\n\nBy aligning services with community needs, data can enhance service quality and responsiveness.\n\nEnhanced Transparency and Trust\n\nEffective use of data fosters transparency, improving public trust in government decisions.\n\nPolicy Communication\n\nData provides a clear evidence base for policy-making, aiding communication between governments and stakeholders.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Use of Data in Local Governance - A Michigan Perspective</span>"
    ]
  },
  {
    "objectID": "case-study-three.html#sec-cs3-conclusion",
    "href": "case-study-three.html#sec-cs3-conclusion",
    "title": "5  The Use of Data in Local Governance - A Michigan Perspective",
    "section": "5.5 Conclusion",
    "text": "5.5 Conclusion\nThe integration of data into Michigan’s local governance has proven valuable despite challenges like cost concerns and resource limitations. The broader adoption of data-driven practices, even informally, highlights its potential to improve decision-making and service delivery.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Use of Data in Local Governance - A Michigan Perspective</span>"
    ]
  },
  {
    "objectID": "case-study-three.html#sec-csc3-recommendations",
    "href": "case-study-three.html#sec-csc3-recommendations",
    "title": "5  The Use of Data in Local Governance - A Michigan Perspective",
    "section": "5.6 Recommendations",
    "text": "5.6 Recommendations\n\nCapacity Building - Invest in training to enhance technical and analytical skills among local officials.\nEncourage Collaboration - Foster partnerships with academic institutions or tech firms to support data initiatives.\nLeverage Resources - Utilise available tools and frameworks, such as those provided by Michigan’s MPPS, to guide data practices.\nPromote Leadership and Cultural Change - Champion leadership roles that prioritize data use and cultivate a culture of evidence-based decision-making.\n\nBy adopting these strategies, countries can effectively integrate data into local governance, enhancing policy outcomes and public trust.\n \n\n\n\n\nIvacko, T. M., Horner, D., & Crawford, M. Q. (2013). Data-driven decision-making in michigan local government. SSRN Electronic Journal. https://doi.org/10.2139/ssrn.2351916",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Use of Data in Local Governance - A Michigan Perspective</span>"
    ]
  },
  {
    "objectID": "case-study-four.html",
    "href": "case-study-four.html",
    "title": "6  Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County",
    "section": "",
    "text": "6.1 Introduction\nThis case study describes the significant steps that the Turkana County local government are taking to modernising early childhood development and education services management through the use of digital technology(Onunga & Odongo, 2025).\nTurkana County, located in northwest Kenya, is a region marked by significant natural resource wealth and cultural diversity. However, it faces challenges such as poverty, infrastructure gaps, and governance inefficiencies. The county’s recent efforts to embrace data-driven decision-making offer valuable insights for enhancing local governance through improved policy formulation and implementation.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County</span>"
    ]
  },
  {
    "objectID": "case-study-four.html#sec-cs4-background",
    "href": "case-study-four.html#sec-cs4-background",
    "title": "6  Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County",
    "section": "6.2 Background",
    "text": "6.2 Background\nTurkana County was established under Kenya’s devolution framework in 2013, with its administrative structure comprising several wards and sub-counties. The county has made strides in adopting digital tools like the Turkana Early Childhood Development and Education (ECDE) Management Information System or TECDEMIS and the Continuous Database Updating System or CODUSYS for education management, reflecting a commitment to modernise governance.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County</span>"
    ]
  },
  {
    "objectID": "case-study-four.html#sec-cs4-current-situation",
    "href": "case-study-four.html#sec-cs4-current-situation",
    "title": "6  Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County",
    "section": "6.3 Current Situation of Policy and Decision-Making",
    "text": "6.3 Current Situation of Policy and Decision-Making\nPolicy-making in Turkana County is characterised by structured processes involving the County Assembly and Executive. Data utilisation is integral to planning and budgeting, with systems like TECDEMIS facilitating real-time data collection and analysis. These tools support decision-makers in tracking program outcomes and resource allocation efficiency.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County</span>"
    ]
  },
  {
    "objectID": "case-study-four.html#sec-cs4-challenges",
    "href": "case-study-four.html#sec-cs4-challenges",
    "title": "6  Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County",
    "section": "6.4 Challenges in Data Utilisation for Governance",
    "text": "6.4 Challenges in Data Utilisation for Governance\nDespite progress, several challenges impede effective data use:\n\nTechnological Barriers: Limited internet access hampers system functionality.\nInstitutional Weaknesses: Insufficient skilled personnel affect system implementation.\nFinancial Constraints: Inadequate funding limits infrastructure development and capacity building.\nSocio-Political Factors: Resistance to change and lack of awareness about data’s value.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County</span>"
    ]
  },
  {
    "objectID": "case-study-four.html#sec-cs4-opportunities",
    "href": "case-study-four.html#sec-cs4-opportunities",
    "title": "6  Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County",
    "section": "6.5 Opportunities for Enhancing Data Use",
    "text": "6.5 Opportunities for Enhancing Data Use\nThe county presents several opportunities:\n\nInvestments in Digital Infrastructure: Initiatives like TECDEMIS and CODUSYS provide a solid foundation.\nPartnerships with Development Agencies: Collaborations with organisations like the Japan International Cooperation Agency or JICA offer resources and expertise.\nCapacity Building: Training programs enhance staff skills in data management and analysis.\nCommunity Engagement: Involving citizens fosters trust and ownership of data initiatives.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County</span>"
    ]
  },
  {
    "objectID": "case-study-four.html#sec-cs4-conclusion",
    "href": "case-study-four.html#sec-cs4-conclusion",
    "title": "6  Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County",
    "section": "6.6 Conclusion",
    "text": "6.6 Conclusion\nEmbracing data-driven governance is crucial for Turkana County to overcome challenges and achieve sustainable development. Effective data use aligns with broader goals of accountability, service efficiency, and inclusive growth.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County</span>"
    ]
  },
  {
    "objectID": "case-study-four.html#sec-cs4-recommendations",
    "href": "case-study-four.html#sec-cs4-recommendations",
    "title": "6  Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County",
    "section": "6.7 Recommendations",
    "text": "6.7 Recommendations\n\nInvest in IT Infrastructure: Expand internet access and upgrade digital tools.\nEnhance Training Programs: Prioritise skills development in data management and analysis.\nFoster Multi-Sectoral Partnerships: Strengthen collaborations with development agencies and the private sector.\nImprove Stakeholder Engagement: Involve communities to build trust and ownership of data initiatives.\nEstablish Monitoring Frameworks: Develop systems to evaluate the impact of data-driven policies.\n\n \n\n\n\n\nOnunga, J., & Odongo, P. (2025). Digital transformation in public administration and data-driven decision-making: A review of turkana county government. International Journal of Research and Innovation in Applied Science, IX, 234–240. https://doi.org/10.51584/IJRIAS.2024.912022",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County</span>"
    ]
  },
  {
    "objectID": "case-study-five.html",
    "href": "case-study-five.html",
    "title": "7  Indigenous Data Governance in the United States",
    "section": "",
    "text": "7.1 Introduction\nThis case study describes the challenges and opportunities with regard to Indigenous data governance in the United States(Carroll et al., 2019).\nIndigenous nations in the United States exercise sovereignty over their data, recognising their right to control and manage their own information. This sovereignty is supported by federal laws such as Native American Graves Protection and Repatriation Act or NAGPRA which provide frameworks for protecting Indigenous rights, including those related to data governance.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Indigenous Data Governance in the United States</span>"
    ]
  },
  {
    "objectID": "case-study-five.html#sec-cs5-current-strategies",
    "href": "case-study-five.html#sec-cs5-current-strategies",
    "title": "7  Indigenous Data Governance in the United States",
    "section": "7.2 Current Strategies",
    "text": "7.2 Current Strategies\n\nTribal Data Sovereignty\n\nIndigenous nations establish policies and institutions, like tribal councils, to own and control their data, ensuring it aligns with cultural values.\n\nCollaboration\n\nPartnerships with federal and state governments are facilitated through initiatives like the National Historic Preservation Act or NHPA promoting shared goals in data governance.\n\nCapacity Building\n\nTraining programs and technological infrastructure development enhance technical skills, though resources vary among tribes.\n\nLegal Frameworks\n\nTreaties and international agreements, such as the United Nations Declaration on the Rights of Indigenous Peoples or UNDRIP(United Nations General Assembly, 2007), provide legal backing for data governance, ensuring respect for Indigenous rights.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Indigenous Data Governance in the United States</span>"
    ]
  },
  {
    "objectID": "case-study-five.html#sec-cs5-challenges",
    "href": "case-study-five.html#sec-cs5-challenges",
    "title": "7  Indigenous Data Governance in the United States",
    "section": "7.3 Challenges",
    "text": "7.3 Challenges\n\nLegal Complexities: Overlapping jurisdictions complicate data governance, requiring clear resolution mechanisms.\nResource Limitations: Financial and technical constraints affect smaller tribes’ ability to implement strategies.\nCultural Preservation: Balancing modern data practices with cultural preservation is complex but crucial.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Indigenous Data Governance in the United States</span>"
    ]
  },
  {
    "objectID": "case-study-five.html#sec-cs5-opportunities",
    "href": "case-study-five.html#sec-cs5-opportunities",
    "title": "7  Indigenous Data Governance in the United States",
    "section": "7.4 Opportunities",
    "text": "7.4 Opportunities\n\nGlobal Networks: Engagement with international bodies like the UNDRIP offers support and recognition, enhancing governance effectiveness.\nCapacity Building: Support through grants and partnerships can bridge resource gaps.\nCollaboration: Inter-tribal agreements strengthen collective data management efforts.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Indigenous Data Governance in the United States</span>"
    ]
  },
  {
    "objectID": "case-study-five.html#sec-cs5-conclusion",
    "href": "case-study-five.html#sec-cs5-conclusion",
    "title": "7  Indigenous Data Governance in the United States",
    "section": "7.5 Conclusion",
    "text": "7.5 Conclusion\nIndigenous data governance in the U.S. is advancing through sovereignty assertion collaboration, capacity building, and international frameworks. While challenges persist, opportunities for improvement are significant. Government officials must support Indigenous nations by respecting their sovereignty, providing resources, and fostering international engagement to enhance data governance effectively.\n \n\n\n\n\nCarroll, S. R., Rodriguez-Lonebear, D., & Martinez, A. (2019). Indigenous data governance: Strategies from united states native nations. Data Science Journal, 18(1), 31. https://doi.org/10.5334/dsj-2019-031\n\n\nUnited Nations General Assembly. (2007). United nations declaration on the rights of indigenous peoples : Resolution / adopted by the general assembly. United Nations. https://www.refworld.org/legal/resolution/unga/2007/en/49353",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Indigenous Data Governance in the United States</span>"
    ]
  },
  {
    "objectID": "data-security.html",
    "href": "data-security.html",
    "title": "8  Data privacy, security, and protection",
    "section": "",
    "text": "8.1 Definitions\nThe increasing volume of digital data collected in today’s world necessitates robust protection mechanisms. Breaches can lead to devastating consequences, such as identity theft, financial loss, and potential public health risks, particularly in sectors like healthcare where patient privacy is paramount under regulations such as the General Data Protection Regulation (GDPR).\nFor institutions, safeguarding sensitive data is crucial for maintaining customer trust, preventing identity theft, and avoiding the loss of valuable customers due to data breaches.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data privacy, security, and protection</span>"
    ]
  },
  {
    "objectID": "data-security.html#sec-data-security-definitions",
    "href": "data-security.html#sec-data-security-definitions",
    "title": "8  Data privacy, security, and protection",
    "section": "",
    "text": "8.1.1 Data privacy\nData privacy is about controlling how your personal information is collected, used, and shared. It’s about protecting your right to know who has your data, how it’s being used, and who else it’s being shared with. Essentially, it’s the right to privacy in the digital world.\n\n\n8.1.2 Data protection\nData protection encompasses the security strategies and processes designed to safeguard sensitive data against unauthorised access, misuse, corruption, and loss. It aims to maintain the integrity, availability, and confidentiality of data, while also ensuring compliance with relevant regulations and ethical standards.\n\n\n8.1.3 Data security\nData privacy and data security are distinct but related disciplines. Both are core components of an institution’s broader data governance strategy.\nData privacy focuses on the individual rights of data subjects or the users who own the data. For organisations, the practice of data privacy is a matter of implementing policies and processes that allow users to control their data in accordance with relevant data privacy regulations.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data privacy, security, and protection</span>"
    ]
  },
  {
    "objectID": "data-security.html#sec-data-security-legal-frameworks",
    "href": "data-security.html#sec-data-security-legal-frameworks",
    "title": "8  Data privacy, security, and protection",
    "section": "8.2 Legal frameworks",
    "text": "8.2 Legal frameworks\nThe General Data Protection Regulation (GDPR) is a European Union law that controls how organizations handle the personal data of EU residents. It was adopted in 2016 and became effective on May 25, 2018. It aims to give individuals more control over their personal data and to ensure that organizations are more transparent and accountable for how they process that data.\nSince then, other countries have followed suit in creating their own legislation similar to the GDPR. Generally, countries created such laws as a response to the EU’s rollout of GDPR given that the GDPR applies to any organisation that processes the personal data of EU residents, regardless of whether the organisation is located within the EU.\nThe Seychelles has passed into law the Data Protection Act, 2023 otherwise entitled as\n\nAn act for the protection of individuals with regard to the processing of personal data, to recognise the right to privacy envisaged in article 20 of the constitution, to promote and facilitate responsible and transparent flow of information by private and public entities and to provide for other related matters.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data privacy, security, and protection</span>"
    ]
  },
  {
    "objectID": "data-security.html#sec-data-security-principles",
    "href": "data-security.html#sec-data-security-principles",
    "title": "8  Data privacy, security, and protection",
    "section": "8.3 Principles of data protection",
    "text": "8.3 Principles of data protection\nArticle 5 of the GDPR sets out key principles which lie at the heart of the general data protection regime. These key principles are set out right at the beginning of the GDPR and they both directly and indirectly influence the other rules and obligations found throughout the legislation. Therefore, compliance with these fundamental principles of data protection is the first step for controllers in ensuring that they fulfil their obligations under the GDPR. The following is a brief overview of the Principles of Data Protection found in article 5 GDPR:\n\n8.3.1 Lawfulness, fairness, and transparency\nAny processing of personal data should be lawful and fair. It should be transparent to individuals that personal data concerning them are collected, used, consulted, or otherwise processed and to what extent the personal data are or will be processed. The principle of transparency requires that any information and communication relating to the processing of those personal data be easily accessible and easy to understand, and that clear and plain language be used.\n\n\n8.3.2 Purpose Limitation\nPersonal data should only be collected for specified, explicit, and legitimate purposes and not further processed in a manner that is incompatible with those purposes. In particular, the specific purposes for which personal data are processed should be explicit and legitimate and determined at the time of the collection of the personal data. However, further processing for archiving purposes in the public interest, scientific, or historical research purposes or statistical purposes (in accordance with Article 89(1) GDPR) is not considered to be incompatible with the initial purposes.\n\n\n8.3.3 Data Minimisation\nProcessing of personal data must be adequate, relevant, and limited to what is necessary in relation to the purposes for which they are processed. Personal data should be processed only if the purpose of the processing could not reasonably be fulfilled by other means. This requires, in particular, ensuring that the period for which the personal data are stored is limited to a strict minimum (see also the principle of ‘Storage Limitation’ below).\n\n\n8.3.4 Accuracy\nControllers must ensure that personal data are accurate and, where necessary, kept up to date; taking every reasonable step to ensure that personal data that are inaccurate, having regard to the purposes for which they are processed, are erased or rectified without delay. In particular, controllers should accurately record information they collect or receive and the source of that information.\n\n\n8.3.5 Storage Limitation\nPersonal data should only be kept in a form which permits identification of data subjects for as long as is necessary for the purposes for which the personal data are processed. In order to ensure that the personal data are not kept longer than necessary, time limits should be established by the controller for erasure or for a periodic review.\n\n\n8.3.6 Integrity and Confidentiality\nPersonal data should be processed in a manner that ensures appropriate security and confidentiality of the personal data, including protection against unauthorised or unlawful access to or use of personal data and the equipment used for the processing and against accidental loss, destruction or damage, using appropriate technical or organisational measures.\n\n\n8.3.7 Accountability\nFinally, the controller is responsible for, and must be able to demonstrate, their compliance with all of the above-named Principles of Data Protection. Controllers must take responsibility for their processing of personal data and how they comply with the GDPR, and be able to demonstrate (through appropriate records and measures) their compliance.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data privacy, security, and protection</span>"
    ]
  },
  {
    "objectID": "data-security.html#types-of-data-security",
    "href": "data-security.html#types-of-data-security",
    "title": "8  Data privacy, security, and protection",
    "section": "8.4 Types of data security",
    "text": "8.4 Types of data security\nTo enable the confidentiality, integrity and availability of sensitive information, organizations can implement the following data security measures:\n\n8.4.1 Encryption\nBy using an algorithm to transform normal text characters into an unreadable format, encryption keys scramble data so that only authorized users can read it. File and database encryption software serve as a final line of defense for sensitive volumes by obscuring their contents through encryption or tokenization. Most encryption tools also include security key management capabilities.\n\n\n8.4.2 Data erasure\nData erasure uses software to completely overwrite data on any storage device, making it more secure than standard data wiping. It verifies that the data is unrecoverable.\n\n\n8.4.3 Data masking\nBy masking data, organizations can allow teams to develop applications or train people that use real data. It masks personally identifiable information (PII) where necessary so that development can occur in environments that are compliant.\n\n\n8.4.4 Data resiliency\nResiliency depends on how well an organization endures or recovers from any type of failure—from hardware problems to power shortages and other events that affect data availability. Speed of recovery is critical to minimize impact.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data privacy, security, and protection</span>"
    ]
  },
  {
    "objectID": "data-security.html#sec-data-security-summary",
    "href": "data-security.html#sec-data-security-summary",
    "title": "8  Data privacy, security, and protection",
    "section": "8.5 Summary",
    "text": "8.5 Summary\nData privacy, security, and protection are fundamental concerns in today’s digital landscape. They involve protecting personal information from unauthorised access, implementing robust security measures, and upholding ethical standards in data handling. Addressing these challenges effectively requires a holistic approach that integrates technical safeguards with ongoing education and ethical practices to maintain trust and prevent significant risks.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data privacy, security, and protection</span>"
    ]
  },
  {
    "objectID": "data-tools.html",
    "href": "data-tools.html",
    "title": "9  Data tools",
    "section": "",
    "text": "9.1 Microsoft Excel and other Excel-like spreadsheet software\nWorking with data is a multi-faceted endeavour that involves collecting, storing, analysing, visualising, securing, and managing data across various domains. The various steps in the data pathway (see Figure 9.1) often require specific tools that are best-suited for the task at hand.\nIn this section, we present the most common data tools, describe their key functionalities, and discuss what each tool is best suited for in relation to the steps in the data pathway.\nMicrosoft Excel is versatile spreadsheet software with robust formula capabilities, pivot tables for quick data summarisation, and Power Query for advanced data cleaning. Other than for data collection, it is also suited for detailed analysis, budgeting, and financial tracking. Suitable for complex data management. On the other hand, some may find that using it presents a steeper learning curve and costs of the subscription-based software-as-a-service (SaaS) model as part of Microsoft 365 can be prohibitive.\nAn estimated 750 million up to 1.5 billion people1 use Microsoft Excel. It has numerous applications, including data entry, analysis, accounting, financial modelling, and reporting. It’s used in various fields like business, education, and personal finance to organise, manage, and visualise data.\nOther than Microsoft Excel, there are Excel-like applications available as part of a suite of office applications that use the Open Document Form (ODF), an open file format for word processing documents, spreadsheets, presentations and graphics and using ZIP-compressed XML files. It was developed with the aim of providing an open, XML-based file format specification for office applications. ODF was based on the Sun Microsystems specification for OpenOffice.org XML, the default format for OpenOffice.org and LibreOffice. This standard was originally developed to provide an open standard for office documents. Versions of Microsoft Excel since 2003 use the ODF XML standard to afford compatibility to other spreadsheets that use the standard. A number of free and proprietary software use the ODF XML standard hence there are various Excel-like spreadsheet alternatives available that use the standard2 and are mostly compatible with Microsoft Office/Microsoft 365 applications including Excel. Although generally compatible in almost all of the basic features, Excel-like spreadsheet applications may not fully implement highly customised Excel spreadsheets that use Visual Basic for Applications (VBA) macros as there are significant differences in syntax and implementation to LibreOffice Calc’s Basic macro system and environment.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-google-sheets",
    "href": "data-tools.html#sec-google-sheets",
    "title": "9  Data tools",
    "section": "9.2 Google Sheets",
    "text": "9.2 Google Sheets\nGoogle Sheets, a free and web-based spreadsheet application, is a versatile data tool used for organising, managing, and analysing data, as well as creating visualisations. It’s part of the Google Workspace suite, along with Google Docs and Google Slides. Sheets offers features like pivot tables, formulas, conditional formatting, and data validation for a variety of data-related tasks.\nGoogle Sheets is technically not an Excel-like spreadsheet (although general use and behaviour is similar to Excel) as it doesn’t use the ODF XML standard but rather has its own proprietary format called the Google Sheets format which can only be accessed or utilised through a web browser rather than through a standalone installer for your computer. In order to access/open a Google Sheets format outside of a browser, one h as to download it as either an Excel file or as a comma-separated value (CSV) file which can then be opened in Excel. Google Sheets has similar features and functionalities as Excel but because of its indirect compatibility with Excel and Excel-like ODF XML-compliant software, advanced features of both applications are not interoperable.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-google-forms",
    "href": "data-tools.html#sec-google-forms",
    "title": "9  Data tools",
    "section": "9.3 Google Forms",
    "text": "9.3 Google Forms\nGoogle Forms is a tool for creating online forms, surveys, and quizzes that can be shared with others to collect data. It allows users to create and edit these forms online, collaborate in real-time, and have the collected data automatically entered into a spreadsheet. Google Forms is part of the free, web-based Google Suite and the software-as-a-service (SaaS) Google Workspace which includes Google Docs, Google Sheets, Google Slides, Google Drawings, Google Sites, and Google Keep. Google Forms is only available as a web application.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-airtable",
    "href": "data-tools.html#sec-airtable",
    "title": "9  Data tools",
    "section": "9.4 Airtable",
    "text": "9.4 Airtable\nAirable is a spreadsheet-database hybrid, with the features of a database but applied to a spreadsheet. The fields in an Airtable table are similar to cells in a spreadsheet, but have types such as ‘checkbox’, ‘phone number’, and ‘drop-down list’, and can reference file attachments like images.\nUsers can create a database, set up column types, add records, link tables to one another, collaborate, sort records and publish views to external websites. Users cannot download their database in full, but can download some of the data by manually downloading CSVs for each table.\nAirtable is user-friendly and is designed for ease of use, making it accessible to a wide range of users, including those without technical backgrounds. It also enables users to build and customise applications for various purposes, such as managing product roadmaps, launching marketing campaigns, and tracking job applications. It facilitates collaboration by allowing multiple users to access and work on the same database. Airtable integrates with various other platforms, enabling data to be shared and workflows to be automated.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-quickbooks",
    "href": "data-tools.html#sec-quickbooks",
    "title": "9  Data tools",
    "section": "9.5 QuickBooks and other accounting-specific software",
    "text": "9.5 QuickBooks and other accounting-specific software\nQuickBooks is a popular accounting software designed to help businesses manage their finances, including tasks like bookkeeping, invoicing, expense tracking, and payroll.\nQuickBooks is a widely used accounting software known for its ease of use and automation capabilities. It’s a solution for small to medium-sized businesses (SMEs), offering features like invoicing, expense tracking, inventory management, and payroll processing.\n\n9.5.1 Other Accounting Software\nBeyond QuickBooks, several other software options exist, each with its strengths and weaknesses:\n\nXero: Offers a user-friendly interface and strong integration capabilities, making it popular among small businesses.\nSage 50: A desktop accounting software with robust reporting and features for larger businesses.\nWave Accounting: A free option that provides basic accounting features, suitable for startups and small businesses.\nZoho Books: A comprehensive online accounting software with various features, including project management.\nFreshBooks: A popular choice for freelancers and sole proprietors, known for its simplicity.\n\n\n\n9.5.2 Key Features of Accounting Software\nCommon features across different accounting software include bookkeeping and recording of financial transactions, invoicing, expense tracking and managing and categorising business expenses, payroll processing financial reporting to generate reports like income statements and balance sheets, and inventory management to track and manage inventory levels.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-business-intelligence",
    "href": "data-tools.html#sec-business-intelligence",
    "title": "9  Data tools",
    "section": "9.6 Business intelligence and analytics platforms",
    "text": "9.6 Business intelligence and analytics platforms\nPower BI, Tableau, and Qlik are classified as business intelligence (BI) tools or data analytics platforms. They share the common goal of enabling users to interact with data, visualise it, analyse it, and ultimately, make data-driven decisions. However, they each have unique strengths and features that cater to different needs and preferences.\n\n9.6.1 PowerBI\nMicrosoft’s BI platform offers a wide range of functionalities, including data connectivity, data modelling, interactive visualisations, and dashboard creation. It’s known for its ease of use and seamless integration with other Microsoft products.\n\n\n9.6.2 Qlik\nThis platform focuses on its associative data model, allowing users to explore relationships within data freely. It also offers strong data integration capabilities and is well-suited for large, complex datasets.\n\n\n9.6.3 Tableau\nTableau is highly regarded for its visual analytics capabilities, enabling users to create stunning and interactive dashboards. It’s known for its user-friendly interface and strong visualisation options.\n\n\n9.6.4 Comparison\n\n9.6.4.1 Ease of Use\nPower BI is generally considered to have a more intuitive interface, while Qlik Sense is more powerful but can have a steeper learning curve. Tableau’s drag-and-drop interface is known for its ease of use.\n\n\n9.6.4.2 Data Integration:\nQlik is particularly strong in data integration and can handle diverse data sources, while Tableau offers a dedicated tool (Tableau Prep) for data preparation. Power BI’s data integration capabilities are also robust, particularly when used in conjunction with other Microsoft products.\n\n\n9.6.4.3 Visualisation\nTableau is renowned for its visual analytics, offering a wide array of visual options and a focus on storytelling through data. Power BI also offers extensive visualisation capabilities, and Qlik provides a unique approach with its associative model.\n\n\n9.6.4.4 Scalability and performance\nAll three tools are scalable, but Qlik is particularly well-suited for large, real-time datasets. Power BI is strong for smaller to medium datasets and can leverage Microsoft Azure for scalability. Tableau’s performance depends on the complexity of the dashboards, but it’s generally robust for complex visualisations.\n\n\n9.6.4.5 Pricing\nPower BI is known for its affordable pricing, while Tableau and Qlik Sense can be more expensive, particularly for enterprise users.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-cloud-storage",
    "href": "data-tools.html#sec-cloud-storage",
    "title": "9  Data tools",
    "section": "9.7 Cloud-based data storage",
    "text": "9.7 Cloud-based data storage\nIn today’s digital age, efficient data storage and quick access are crucial, particularly as remote work becomes more prevalent. Cloud storage solutions like Google Drive, Dropbox, and OneDrive have become vital tools for both businesses and individuals due to their ease of use and collaborative features.\n\n9.7.1 Google Drive\nGoogle Drive is a cloud storage service included in the Google Suite or Google Workspace of tools that allows users to store, sync, and access files across multiple devices and platforms via an internet connection. It also offers features like collaboration tools, document creation, and sharing capabilities.\n\n\n9.7.2 OneDrive\nOneDrive is a cloud storage service by Microsoft included in the Microsoft 365 set of applications that provides collaboration, document creation, and sharing tools. It allows users to store and sync files across multiple devices and offers 5GB of free storage. Paid plans are available for additional storage ranging from 50GB to 1TB.\n\n\n9.7.3 Dropbox\nDropbox is a cloud storage service that allows users to store, share, and sync files across multiple devices. Available on Windows, Mac, iOS, and Android, it offers document creation, collaboration, and sharing tools. With 2GB of free storage, paid plans range from 200GB to 3TB for additional needs.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-database",
    "href": "data-tools.html#sec-database",
    "title": "9  Data tools",
    "section": "9.8 Databases",
    "text": "9.8 Databases\nA database is an organised collection of structured and/or unstructured data, typically stored electronically in a computer system. It’s a system for storing and managing data, and it’s managed by a Database Management System (DBMS). Databases are used to store, retrieve, and manipulate data efficiently.\nHence, the concept of a database is both software, which deals with the handling and management of the data, and hardware, which deals with the physical storage of the data.\n\n9.8.1 SQL and other relational databases\nSQL databases, also known as relational databases, are systems that store collections of tables and organise structured sets of data in a tabular columns-and-rows format, similar to that of a spreadsheet. The databases are built using structured query language (SQL), the query language that not only makes up all relational databases and relational database management systems (RDBMS), but also enables them to “talk to each other”.\nThe history of database technology/relational databases SQL was invented as a language in the early 1970s, which means SQL databases have been around for as long as the Internet itself. Dubbed the structured English query language (SEQUEL), SQL was originally created to streamline access to relational database systems and to assist with the processing of information. Today, SQL remains one of the most popular and widely used query languages in open-source database technology due to its flexibility, ease of use, and seamless integration with a variety of different programming languages. You’ll find SQL being used throughout all types of high-performing, data-centric applications.\n\n\n9.8.2 NoSQL\nNoSQL stands for “Not Only SQL.” It refers to a type of database that doesn’t rely on the traditional relational database models, which are organised into tables with fixed schemas and use SQL for querying. NoSQL databases offer a more flexible approach to data storage and querying, often using document, graph, key-value, or other data models. NoSQL databases are equipped to handle large volumes of structured, semi-structured, and unstructured data from non-traditional sources.\nPopular database management systems include Microsoft SQL Server, PostgreSQL, MongoDB, Redis, Elasticsearch, SQLite, MariaDB, IBM Db2, Oracle Database, and MySQL. In essence, databases are fundamental to modern IT infrastructure, enabling organisations to store, manage, and analyse data efficiently for various applications, including websites, apps, and business processes.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-mis",
    "href": "data-tools.html#sec-mis",
    "title": "9  Data tools",
    "section": "9.9 Management information systems",
    "text": "9.9 Management information systems\nA Management Information System (MIS) is an integrated system that collects, processes, stores, and disseminates information to support managerial decision-making and improve operational efficiency. It essentially acts as a tool for gathering and analysing data, converting it into actionable insights, and making those insights available to the right people within an organisation.\n\n9.9.1 Key Features\n\nData Collection and Storage - MIS systems gather data from various sources, both internal (e.g., sales records, inventory) and external (e.g., market trends, competitor information).\nData Processing and Analysis - The collected data is processed and analysed to identify trends, patterns, and opportunities, often using sophisticated tools and techniques.\nInformation Dissemination - The analysed information is then formatted and delivered to managers and other stakeholders in a way that is easy to understand and use.\nDecision Support - MIS provides the information that managers need to make informed decisions about various aspects of their business, such as sales, marketing, finance, and operations.\nImproved Efficiency - By providing timely and accurate information, MIS helps organisations to operate more efficiently, reduce costs, and improve decision-making.\n\n\n\n9.9.2 Examples of MIS applications\n\nSales and Marketing - Tracking sales figures, analysing marketing campaign effectiveness, and identifying customer trends.\nAccounting and Finance - Managing financial records, generating financial statements, and tracking investments.\nHuman Resources - Managing employee information, tracking performance, and supporting recruitment and training activities.\nInventory Management - Tracking inventory levels, managing warehouses, and forecasting demand.\nHealth records - tracking of patients and clients of various health services. This is often called a Health Management Information System (HMIS).\nCustomer-relationship manager - tracking of clients/customers data and interactions with company (see Section 9.10).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-crm",
    "href": "data-tools.html#sec-crm",
    "title": "9  Data tools",
    "section": "9.10 Customer-relationship Manager",
    "text": "9.10 Customer-relationship Manager\nCustomer Relationship Management (CRM) systems are software applications that help businesses manage and analyse customer data and interactions. They are used to collect, organise, and process information about customers, including their interactions, preferences, and purchase history. The goal is to improve customer service, increase customer retention, and drive sales growth.\n\n9.10.1 Key Features and Functionality\n\n\n9.10.2 Data Management\nCRMs store and organise customer data from various sources, like sales interactions, customer service inquiries, marketing campaigns, and social media.\n\n9.10.2.1 Sales Management\nCRMs help track sales opportunities, pipeline management, and sales activities, enabling sales teams to improve efficiency and close deals faster.\n\n\n9.10.2.2 Customer Service\nCRMs facilitate communication with customers, track service requests, and help resolve issues, leading to improved customer satisfaction.\n\n\n9.10.2.3 Marketing Automation\nCRMs can be integrated with marketing automation tools, allowing businesses to personalise and automate marketing campaigns.\n\n\n9.10.2.4 Reporting and Analytics\nCRMs provide insights into customer behaviour, sales performance, and overall business trends, enabling data-driven decision-making.\n\n\n\n9.10.3 Types of CRM Systems\n\nOperational CRM - Focuses on day-to-day customer interactions, such as sales and customer service.\nAnalytical CRM - Analyses customer data to identify trends, patterns, and opportunities.\nCollaborative CRM - Facilitates communication and collaboration between different departments, such as sales, marketing, and customer service.\nStrategic CRM - Uses customer insights to make strategic decisions about product development, pricing, and market positioning.\n\n\n\n9.10.4 Benefits of using a CRM\n\nImproved Customer Service - By having a centralised database of customer information, companies can provide better and more personalised service.\nIncreased Sales - CRMs help sales teams manage leads, track opportunities, and close deals more effectively.\nEnhanced Customer Retention - By understanding customer preferences and needs, businesses can build stronger relationships and retain customers.\nData-Driven Decision Making - CRMs provide valuable insights into customer behaviour and business performance, enabling data-driven decision-making.\nIncreased Efficiency - Automating tasks and streamlining processes can free up employees to focus on more strategic initiatives.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-stats-packages",
    "href": "data-tools.html#sec-stats-packages",
    "title": "9  Data tools",
    "section": "9.11 Statistical packages",
    "text": "9.11 Statistical packages\nSAS, SPSS, and Stata are popular statistical software packages used for data analysis, but have distinct strengths and target industries. SPSS is known for its user-friendly interface, making it popular in social sciences and market research. Stata is a general-purpose statistical software, often favoured for econometrics, and known for its command-line interface and strong data management features. SAS is a powerful system for advanced analytics, business intelligence, and data management, and is widely used in various industries due to its scalability and robustness.\n\n9.11.1 SPSS\nStatistical Package for the Social Sciences or SPSS has a ser-friendly interface and intuitive data management making it suitable for social sciences and market research. It focuses on descriptive and inferential statistics, data exploration, and model building. Its common uses are for surveys, market research, data mining, and other social science applications. The interface is Menu-driven with a graphical user interface.\n\n\n9.11.2 Stata\nStata is a general-purpose software with strong data management capabilities, and a command-line interface. It is used commonly in econometrics, time series analysis, and statistical modelling. Its most common uses are in economics, biomedicine, and political science research. It has some graphical user interface but full capability is accessed via the command-line. It has a graphical output.\n\n\n9.11.3 SAS\nSAS or Statistical Analysis System is robust, scalable, and suitable for advanced analytics, business intelligence, and data management. It can be used for Multivariate analysis, predictive analytics, and large-scale data processing. It’s common uses are for business analytics, data warehousing, and industry-specific applications. The interface to SAS is primarily as a procedural language.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-programming-languages",
    "href": "data-tools.html#sec-programming-languages",
    "title": "9  Data tools",
    "section": "9.12 Programming languages",
    "text": "9.12 Programming languages\nR, Python, and Julia are powerful programming languages frequently used in data science, scientific computing, and related fields. They offer distinct advantages, making them suitable for various tasks.\n\n9.12.1 R\nR is a language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment which was developed at Bell Laboratories (formerly AT&T, now Lucent Technologies) by John Chambers and colleagues. R can be considered as a different implementation of S. There are some important differences, but much code written for S runs unaltered under R.\nR provides a wide variety of statistical (linear and non-linear modelling, classical statistical tests, time-series analysis, classification, clustering, etc.) and graphical techniques, and is highly extensible. The S language is often the vehicle of choice for research in statistical methodology, and R provides an Open Source route to participation in that activity.\nOne of R’s strengths is the ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed. Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control.\nR is available as Free Software under the terms of the Free Software Foundation’s GNU General Public License in source code form. It compiles and runs on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux), Windows and MacOS.\nR is unique in that it is not general-purpose. It does not compromise by trying to do a lot of things. It does a few things very well, mainly statistical analysis and data visualisation. While you can find data analysis and machine learning libraries for languages like Python, R has many statistical functionalities built into its core. No third-party libraries are needed for much of the core data analysis you can do with the language.\nBut even with this specific use case, it is used in every industry you can think of because a modern business runs on data. Using past data, data scientists and data analysts can determine the health of a business and give business leaders actionable insights into the future of their company.\nJust because R is specifically used for statistical analysis and data visualisation doesn’t mean its use is limited. It’s actually quite popular, ranking 12th in the TIOBE index of the most popular programming languages.\nAcademics, scientists, and researchers use R to analyse the results of experiments. In addition, businesses of all sizes and in every industry use it to extract insights from the increasing amount of daily data they generate.\n\n\n9.12.2 Python\nPython is an interpreted, interactive, object-oriented programming language. It incorporates modules, exceptions, dynamic typing, very high level dynamic data types, and classes. It supports multiple programming paradigms beyond object-oriented programming, such as procedural and functional programming. Python combines remarkable power with very clear syntax. It has interfaces to many system calls and libraries, as well as to various window systems, and is extensible in C or C++. It is also usable as an extension language for applications that need a programmable interface. Finally, Python is portable: it runs on many Unix variants including Linux and macOS, and on Windows.\nPython is a high-level general-purpose programming language that can be applied to many different classes of problems.\nThe language comes with a large standard library that covers areas such as string processing (regular expressions, Unicode, calculating differences between files), internet protocols (HTTP, FTP, SMTP, XML-RPC, POP, IMAP), software engineering (unit testing, logging, profiling, parsing Python code), and operating system interfaces (system calls, filesystems, TCP/IP sockets). Look at the table of contents for The Python Standard Library to get an idea of what’s available. A wide variety of third-party extensions are also available. Consult the Python Package Index to find packages of interest to you.\n\n\n9.12.3 Julia\nJulia is a high-level, open-source, general-purpose programming language designed for technical and scientific computing. It’s known for its fast performance, approaching that of languages like C and Fortran, while remaining relatively easy to use. Julia is particularly well-suited for tasks like numerical analysis, data science, and machine learning.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#footnotes",
    "href": "data-tools.html#footnotes",
    "title": "9  Data tools",
    "section": "",
    "text": "It is challenging to make more precise estimates for this. The lower end of this estimate is most likely very conservative and based on historical information. The upper end of this estimate is based on Microsoft’s own estimation based on subscription to Microsoft 365. These estimates likely don’t include unlicensed or unauthorised usage of the software.↩︎\nTo see a list of free and proprietary software that use the ODF XML standard, see https://en.wikipedia.org/wiki/OpenDocument.↩︎",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "all-about-spreadsheets.html",
    "href": "all-about-spreadsheets.html",
    "title": "10  All about spreadsheets",
    "section": "",
    "text": "10.1 History of spreadsheets\nA spreadsheet is a digital tool for organising, analysing, and storing data in tables, originally developed as an electronic version of paper accounting worksheets. It allows users to enter numerical or textual data and formulas that reference other cells, enabling dynamic calculations.\nSpreadsheets are interactive with users able to modify values and observe immediate changes in calculated results, facilitating “what-if” analysis. Beyond basic arithmetic, spreadsheets offer financial, statistical, and conditional functions, enhancing their analytical capabilities. Composed of rows (numbered) and columns (labelled with letters), cells are referenced by their alphanumeric coordinates (e.g., A1). Modern spreadsheet applications include multiple worksheets within a workbook, allowing for complex data management. It can also display data graphically, aiding in understanding trends and patterns.\nSpreadsheets have revolutionised business processes by replacing manual systems, offering versatility across various applications where tabular data is essential. Their dynamic cell referencing system allows for efficient and flexible data manipulation, making them indispensable in both professional and personal contexts.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>All about spreadsheets</span>"
    ]
  },
  {
    "objectID": "all-about-spreadsheets.html#sec-history-spreadsheet",
    "href": "all-about-spreadsheets.html#sec-history-spreadsheet",
    "title": "10  All about spreadsheets",
    "section": "",
    "text": "10.1.1 Paper spreadsheet\nThe concept of organising data into tabular formats dates back to ancient times, with examples such as Babylonian clay tablets from 1800 BCE as shown in Figure 10.1. In accounting, the term “spread sheet” was used by at least 1906 to describe a grid system in ledgers1.\n\n\n\n\n\n\nFigure 10.1: A Babylonian clay tablet believed to have been written around 1800 BC containing mathematical table written in cuneiform script\n\n\n\nBefore digital spreadsheets, “spread” referred to large, two-page layouts in publications. The evolution of the term “spread-sheet” reflects its transition from physical, oversized ledger pages with examples shown in Figure 10.2, Figure 10.3 to the digital tool we use today, emphasising their role in accounting and data organisation.\n\n\n\n\n\n\n\n\n\nFigure 10.2: Accounting ledger from 1911\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.3: Manifest of passengers of the Titanic\n\n\n\n\n\n\n\n\n10.1.2 Electronic spreadsheet\nFigure 10.4 shows key development milestones of the electronic spreadsheet.\n\n\n\n\n\n\n\n\nFigure 10.4: Timeline of development of the electronic spreadsheet",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>All about spreadsheets</span>"
    ]
  },
  {
    "objectID": "all-about-spreadsheets.html#sec-spreadsheet-database",
    "href": "all-about-spreadsheets.html#sec-spreadsheet-database",
    "title": "10  All about spreadsheets",
    "section": "10.2 Spreadsheets as databases",
    "text": "10.2 Spreadsheets as databases\nSpreadsheets and databases share similarities but are fundamentally different. A spreadsheet is essentially a single table, while a database consists of multiple tables with machine-readable relational structures. Although a spreadsheet workbook containing multiple sheets has interacting tables, it lacks the relational complexity of a database. Spreadsheets and databases are interoperable, however, with spreadsheets being able to be converted into database tables, and database queries being able to be exported to spreadsheets for analysis.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>All about spreadsheets</span>"
    ]
  },
  {
    "objectID": "all-about-spreadsheets.html#sec-spreadsheet-function",
    "href": "all-about-spreadsheets.html#sec-spreadsheet-function",
    "title": "10  All about spreadsheets",
    "section": "10.3 Spreadsheets as multi-function tools",
    "text": "10.3 Spreadsheets as multi-function tools\nSpreadsheets are widely used software tools for every step of the data pathway - data entry, storage, analysis, and visualisation. Spreadsheets are able to implement such functionality through an end-user development approach.\n\n10.3.1 End-user development\nSpreadsheets are designed as end-user development (EUD) tools. EUD refers to techniques in which non-professional developers are able to create automated tasks and complex data objects without in-depth knowledge of a programming language. Many find using spreadsheets for calculations and analysis much easier. This most likely stems from the following key features:\n\nEase of use\nSpreadsheets leverage spatial relationships, making it intuitive to establish program connections, unlike sequential programming which requires extensive text.\n\n\nForgiving nature\nPartial results and functions can operate even if other parts are incomplete or contain errors, simplifying the development process.\n\n\nVisual enhancements\nModern spreadsheets use colours, fonts, and lines to provide visual cues, aiding comprehension and organisation.\n\n\nAdvanced functionality\nExtensions enable users to create complex functions and integrate machine learning models, expanding their capabilities beyond basic calculations.\n\n\nVersatility\nBeyond numerical data, spreadsheets support Boolean logic, graphical design, and even SQL queries through relational data storage and formula-based expressions.\nIn essence, spreadsheets offer a flexible, powerful platform that caters to diverse tasks, making them an invaluable tool for many users despite their limitations compared to traditional programming environments.\n\n\n\n10.3.2 Limitations and shortcomings of spreadsheets\nUnfortunately, the same multi-functionality and features that make spreadsheets user-friendly and easily accessible to most also make them fragile, non-robust, and prone to causing/producing errors.\nIn order to be able to function as a tool for the various steps in the data pathway while still being user-friendly meant combining the data interface functionality (data storage and data access) with the programming/scripting capabilities (for data cleaning/processing, analysis, and visualisation) into a single graphical user interface with no clear distinction between them and no clear mechanism for programming/script testing. This brings about the following limitations and shortcomings:\n\nLack of standard mechanisms for management and quality assurance of spreadsheets produced by organisations\nGiven that data storage and access along with data processing/cleaning, analysis and visualisation capabilities sit side-by-side within the spreadsheet tool/software, developing routine and automated audit mechanisms for both data validity/quality and accuracy/correctness of data processing, analysis, and visualisation is nearly impossible. These audits will need to be done manually and line-by-line making them highly onerous. This is most likely the reason why a survey conducted in 2011 of nearly 1,500 people in the UK saw 72% reporting that no internal department checks their spreadsheets for accuracy, that only 13% said that internal audit reviews their spreadsheets, while a mere 1% receive checks from their risk department (“Spreadsheet Risk Management Within UK Organisations,” n.d.).\n\n\nReliability issues\nAn estimated 1% of all formulas in operational spreadsheets are in error (S. Powell et al., 2009).\n\n\nPractical expressiveness is limited\nWhilst the graphical user interface of a spreadsheet using its cell-at-a-time approach is accessible and user-friendly for most users and for simple data operations, applying the same to a complex data model requiring more complicated calculations require tedious attention to detail. Users will tend to have difficulty remembering the meanings of hundreds or thousands of alphanumeric cell addresses that appear in per cell formulas.\n\n\nFormulas expressed in terms of cell addresses are hard to keep straight and hard to audit\nA research paper critically reviewing spreadsheet errors has shown that auditors who check both numerical results and the cell formulas find no more errors than auditors who only check numerical results (S. G. Powell et al., 2008). By the nature of the cell-at-a-time approach, spreadsheets typically contain many copies of the same formula. Thus, when a formula needs to be edited, these edits will need to be applied to all cells containing that formula. This is in sharp contrast to a well-known principle in programming - do not repeat yourself or DRY - which emphasises the best practice of not repeating code to implement/achieve the same process or output. The DRY approach makes code implementation much more efficient and code auditing much more feasible and robust.\n\n\nMaintenance of volumes of spreadsheets is challenging\nCreating and managing a system to maintain vast amounts of spreadsheets for an individual or for an organisation is a challenging endeavour. Without built-in functionalities for proper security, version control and audit trails, and prevention of unintentional introduction of errors, it is more likely that management of spreadsheets end up being ad hoc, non-systematic, and tedious to implement.\n \n\n\n\n\nMiddleton, J. H. (1933). Baking costs. National Association of Cost Accountants Bulletin, 14(10).\n\n\nPowell, S. G., Baker, K. R., & Lawson, B. (2008). A critical review of the literature on spreadsheet errors. Decision Support Systems, 46(1), 128–138. https://doi.org/10.1016/j.dss.2008.06.001\n\n\nPowell, S., Baker, K., & Lawson, B. (2009). Errors in operational spreadsheets. Journal of Organizational and End User Computing, 21(3), 24–36.\n\n\nSpreadsheet risk management within UK organisations. (n.d.). Actuarial Post: For the Modern Actuary. Retrieved June 12, 2025, from https://www.actuarialpost.co.uk/article/spreadsheet-risk-management-within-uk-organisations-351.htm",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>All about spreadsheets</span>"
    ]
  },
  {
    "objectID": "all-about-spreadsheets.html#footnotes",
    "href": "all-about-spreadsheets.html#footnotes",
    "title": "10  All about spreadsheets",
    "section": "",
    "text": "“We maintain, in our general ledger, a so-called Spread Sheet which is a long sheet with the name of each individual plant in a particular column.” (from Middleton, 1933, p. 763)↩︎",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>All about spreadsheets</span>"
    ]
  },
  {
    "objectID": "project-based-workflow.html",
    "href": "project-based-workflow.html",
    "title": "11  Project-based workflow",
    "section": "",
    "text": "11.1 Data processes as livestock rather than pets\nAs our skills as data analysts grow, we begin to understand that our ability to realise our full potential goes beyond the data tools that we have chosen to use or have been made to use. The importance of surrounding systems and infrastructure for ensuring reproducibility and long-term preservation of our work becomes increasingly significant. However, a lack of formal training or mentorship in managing these systems often leads to either feeling overwhelmed by technology or resorting to self-exploration without proper guidance.\nThis chapter aims to guide you gracefully into the exploration of this realm of efficient, effective, and reproducible data workflows. The concepts and practices discussed here may highlight current and existing practices that you have that are ineffectual, disorganised, and irreproducible. If so, we encourage you not to worry about these past mistakes but instead use them to raise the bar for your new work. Small but meaningful incremental changes add up over time, transforming your data quality of life.\nIn this chapter, we will discuss concepts and practices borrowed from the computational sciences field that use programming languages to record and automate their processes and translate them for use with the spreadsheet software that is sort of a hybrid with data processes implemented through both point-and-click steps via the mouse and through in cell commands or functions for performing calculations and operations. This translation as applied to spreadsheets is not high fidelity given the shortcomings and limitations of spreadsheets (as discussed in Section 10.3.2) but still provides enough structure and rigour compared to the typical and common ad hoc and unstructured use of spreadsheets.\nIn modern data and computing, a common analogy used to describe the management of data and computational processes is that of managing a herd of livestock compared to taking care of an individual household pet. For example, in cloud computing, individual servers are treated like “livestock” in that they can be easily destroyed and replaced via automation.\nIt is recommended that we adopt a similar mindset when managing our data and data processes - design and develop appropriate data systems that manage data and data processes as disposable and rebuilt and re-implemented as needed rather than as precious “pets”. We recommend this approach because if your workflow relies on an individual session or workspace in a non-reproducible way, it creates unnecessary risk and complexity. Instead, the focus should be on saving and relying on code and documentation to ensure reproducibility.\nApplying this approach with spreadsheets is not as straightforward given the peculiarities of the tool compared to programming languages that use code to record each step of the workflow. However, steps can be done that can facilitate as much reproducibility when using spreadsheets.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Project-based workflow</span>"
    ]
  },
  {
    "objectID": "project-based-workflow.html#sec-data-livestock",
    "href": "project-based-workflow.html#sec-data-livestock",
    "title": "11  Project-based workflow",
    "section": "",
    "text": "Figure 11.1: Microsoft Excel files as livestock\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.2: Microsoft excel file as a pet\n\n\n\n\n\n\n\n\n\n11.1.1 Detailed documentation for point-and-click mouse-based steps\nPoint-and-click steps in a workflow implemented using a mouse can be documented either in a specific worksheet within the spreadsheet that is just meant for documentation. The documentation can also be done on a separate document either in Word document (.docx) format or in a text-based format such as Markdown (.md) or text file (.txt) written using a text editor (see Tip 11.1 for recommendations on text editors for different operating software). This separate file should be included within the directory where the spreadsheet file is located (see Figure 11.3). An example of a text file documenting steps for data cleaning is shown in Figure 11.4.\n\n\n\n\n\n\n\n\n\nFigure 11.3: A text file for documentation notes on point-and-click mouse-based steps\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.4: An example documentation of point-and-click mouse-based steps and in cell functions and calculations\n\n\n\n\n\n\n\n\n11.1.2 Saving a source for written functions in spreadsheets\nSaving a text-based source file for the syntax of in cell functions and calculations used in a spreadsheet is one way of recording the non-mouse steps of the spreadsheet workflow. A text editor (rather than a word processor) would be ideal for this as the syntax of the function or formula will be shown more appropriately. If you are already using a text editor for documenting mouse-based steps of the spreadsheet workflow, it would be ideal to use the same text file to record in cell functions and calculations as shown in Figure 11.4.\n\n\n\n\n\n\nTip 11.1: Recommended text editors\n\n\n\nFollowing are recommended text editors for use with different operating software:\nFor Windows\nNotepad++ is a free source code editor and Notepad replacement that supports several languages. Running in the Microsoft Windows environment, its use is governed by GNU General Public License.\nFor Mac\nCodeEdit is an exciting new code editor written entirely and unapologetically for macOS. Develop any project using any language at speeds like never before with increased efficiency and reliability in an editor that feels right at home on your Mac.\n\n\nCreate a text file to associate with every spreadsheet project that you are working on. Save the text file within the same directory as the associated spreadsheet as shown in Figure 11.3.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Project-based workflow</span>"
    ]
  },
  {
    "objectID": "project-based-workflow.html#sec-spreadsheet-projects",
    "href": "project-based-workflow.html#sec-spreadsheet-projects",
    "title": "11  Project-based workflow",
    "section": "11.2 Organise work into projects",
    "text": "11.2 Organise work into projects\nOrganising work into projects is another best practice that provides organisational clarity for our data and data processes. Whilst this can be interpreted in many ways and that some of you may argue that you already organise your work with data in projects, the following key points give a clear indication/definition of what we mean by project-based workflows.\n\n11.2.1 File system discipline\nSimply put, this means putting all the files related to a single project in a designated folder. This applies to data, code, figures, notes, including the documentation and source files described earlier (see Figure 11.5). Depending on complexity of your project and on yours or your team’s/organisation’s preferences, you might enforce further organisation into subfolders. Related and relevant file system practices are discussed in Section 11.2.3.\n\n\n\n\n\n\nFigure 11.5: An example directory for the school nutrition project with all relevant files included\n\n\n\n\n\n11.2.2 File path discipline\nAll paths are relative and, by default, relative to the project’s folder. This is particularly important when you are referencing data found in one spreadsheet from within another spreadsheet for data analysis and visualisation. If files are within the same project, then relative paths make it easy to refer to associated or ancillary spreadsheets required for full analysis, visualisation, and reporting.\n\n\n11.2.3 File naming\n\nFile organisation and naming are powerful weapons against chaos.\n\n\n\n\n\n\n\nFigure 11.6: An example of messy file names\n\n\n\nBest practices in file naming are based on the following three principles:\n\nMachine-readable\nMachine-readable file names avoid spaces, punctuation, accented characters, and case sensitivity. Avoiding these makes file names easier to index and search via use of regular expressions and wild card matching or globbing.\nA regular expression, usually shortened as regex or regexp and sometimes referred to as rational expression, is a sequence of characters that specifies a match pattern in text. Usually such patterns are used by string-searching algorithms for “find” or “find and replace” operations on strings, or for input validation.\n\n\n\n\n\n\nFigure 11.7: Using regular expression to find all files that start with fig followed by a two-digit number\n\n\n\nGlobbing, also known as wildcard matching, is a technique used in computer systems to match multiple files or paths based on patterns containing wildcards like * (asterisk) and ? (question mark). It’s a common way to specify a set of files or paths in command-line interfaces, file managers, and programming languages. In simpler terms: globbing allows you to use patterns to find files that share a common naming structure, without having to specify each file individually.\n\n\n\n\n\n\nFigure 11.8: Using wildcard matching to find all PNG files\n\n\n\nMachine-readable file names have deliberate use of delimiters/space-holders such as underscore (_) or hyphen (-). The general rule is that _ is used to delimit units of metadata while - is used to delimit words so that they are more readable. This system allows for much easier recovery of metadata from file names.\n\n\nHuman-readable\nA file name is human-readable if it contains information on what the file contains. It should be easy to figure out what something is based on its file name. This is a similar concept to a slug from semantic URLs. A URL slug is the unique, identifiable portion of a web address (URL) that follows the domain name (e.g., “google.com”). It essentially acts as a “name tag” for a specific page or resource on a website, helping both users and search engines understand what the page is about.\n\n\nPlays well with default ordering\nFile names should play well with default ordering. This is usually achieved by:\n\nputting something numeric first in a file name;\n\n\n\n\n\n\n\nFigure 11.9: Putting something numeric first in a file name\n\n\n\n\nusing the ISO 8601 standard (YYYY-MM-DD) for dates; and,\npad the left side of other numbers with zeros.\n\n\n\n\n\n\n\nFigure 11.10: Left pad other numbers with zeros",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Project-based workflow</span>"
    ]
  },
  {
    "objectID": "project-based-workflow.html#sec-projects-gains",
    "href": "project-based-workflow.html#sec-projects-gains",
    "title": "11  Project-based workflow",
    "section": "11.3 Gains from project-based workflows",
    "text": "11.3 Gains from project-based workflows\nDeveloping the different project-based workflow habits described above collectively yields the most significant benefits. These practices ensure projects can move seamlessly across different computers or environments while maintaining reliability. Project-based workflow approach is a practical convention for achieving consistent behaviour across users and time comparable to societal norms like agreeing on traffic rules (e.g., driving on the left or right). Adhering to these conventions - whether in computing or in broader civilisation - constrains individual actions slightly for greater functionality and safety.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Project-based workflow</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html",
    "href": "data-entry-storage.html",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "",
    "text": "12.1 Be consistent\nSpreadsheets are often used as a multi-purpose tool for data entry, storage, analysis, and visualisation. Most spreadsheet software available allows users to perform all of these tasks. However, spreadsheets are best suited to data entry and storage. Analysis and visualisation should be done separately either by using other data tools or at least in a separate copy of the data file in order to reduce the risk of contaminating or accidentally changing the raw data in the spreadsheet.\nSpreadsheets, by design, make humans format data to be viewed by the human eye rather than to be readable by a machine (Murrell, 2013). Data structured as such require greater amount of effort, usually in computer code, to be able to extract the information needed for analysis. However, if the initial structure is such that it is easily readable by a machine, the effort leading to analysis is much more simplified.\nIn this chapter, we discuss best practices in using spreadsheets as a data entry and data storage tool and provide specific recommendations for organising spreadsheet data in a way that both humans and computers can read. Following these recommendations allows the creation of spreadsheets that minimise errors, are easy for computers to process, and facilitate collaboration and public access. These well-structured spreadsheets integrate with reproducible methods, serving as a reliable foundation for robust analytic workflows.\nConsistency is key in data organisation. Strive for uniformity in your data entry and organisation practices. By maintaining this consistency from the start, you can save yourself and your collaborators from the hassle of reconciling inconsistencies later on.\nFollowing are some examples of being consistent and why it is important. Some of these examples are also part of the recommendations listed here.\nFor a categorical variable like the sex, use a single common value for males (e.g., “male”), and a single common value for females (e.g., “female”). Do not sometimes write “M,” sometimes “male,” and sometimes “Male.” Pick one and stick to it. In order to limit the occurrence of this inconsistency, you can enforce a data validation rule for this variable (see Section 12.11).\nIt is ideal to have every cell filled in so that distinguishing between truly missing values and unintentionally missing values is more straightforward. Decide right at the outset what value to use for missing values and stick with that value throughout. Do not use a note explaining why a value is missing in place of the data itself. Rather, make a separate column with such notes.\nName variables exactly the same way throughout one file and across every other file relevant to the project. If naming is inconsistent for the same variable, those working with the data will have to work out that these are all really the same thing. See Section 12.2 for more discussion on best practices in naming things within a data file. See Section 11.2.3 for an in-depth discussion on best practices in naming files.\nCreate consistent and unique subject identifiers to avoid extra work in figuring out which subject/record if which.\nIf your data are in multiple files and you use different layouts in different files, it will be extra work for the analyst to combine the files into one dataset for analysis. With a consistent structure, it will be easy to automate this process.\nHave some system for naming files. Keeping a consistent file naming scheme will help ensure that your files remain well organised, and it will make it easier to batch process the files if you need to. See an in-depth discussion of file naming in Section 11.2.3.\nPreferably use the standard format YYYY-MM-DD, for example, 2015-08-01. If sometimes you write 8/1/2015 and sometimes 8-1-15, it will be more difficult to use the dates in analyses or data visualisations. See Section 12.3 for more discussion on this.\nIf you have a separate column of notes (e.g., \"dead\"), be consistent in what you write. Do not sometimes write \"dead\" and sometimes \"Dead\".\nA blank cell is different than a cell that contains a single space. And \"male\" is different from \" male \" (i.e., with spaces at the beginning and end).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-be-consistent",
    "href": "data-entry-storage.html#sec-spreadsheet-data-be-consistent",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "",
    "text": "Use consistent codes for categorical variables.\n\n\n\nUse a consistent fixed code for any missing values.\n\n\n\nUse consistent variable names.\n\n\n\nUse consistent subject identifiers.\n\n\n\nUse a consistent data layout in multiple files.\n\n\n\nUse consistent file names.\n\n\n\nUse a consistent format for all dates.\n\n\n\nUse consistent phrases in your notes.\n\n\n\nBe careful about extra spaces within cells.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-good-names",
    "href": "data-entry-storage.html#sec-spreadsheet-data-good-names",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.2 Choose good names for things",
    "text": "12.2 Choose good names for things\nIt is important to pick good names for things such as variables. This can be hard, and so it is worth putting some time and thought into it. Section 11.2.3 provides some general principles for naming files that can also be used when naming variables in data.\n\n12.2.1 General rules for naming\n\nDo not use spaces, either in variable names or file names\n\nSpaces make programming harder. An analyst will need to surround a name that contains spaces in double quotes in order to refer to it. Use underscores (_) or hyphens (-) instead of spaces. Do not use a mixture of underscores and hyphens; pick one and be consistent.\n\nNo extraneous spaces at the start and/or end of variable names\n\nBe careful about extraneous spaces at the beginning or end of a variable name. \"sex\" is different from \"sex \" (with an extra space at the end) or \" sex\" (with an extra space at the start).\n\nAvoid special characters, except for underscores and hyphens\n\nOther symbols ($, @, %, #, &, *, (, ), !, /, etc.) often have special meaning in programming languages, and so they can be harder to handle. They are also a bit harder to type.\nThe main principle in choosing names, whether for variables or for file names, is short, but meaningful. So not too short. The following table (adapted from The Data Carpentry lesson on using spreadsheets) show good and bad example variables names.\n\n\n\n\nTable 12.1: Examples of good and bad variable names\n\n\n\n\n\n\nGood name\nGood alternative\nAvoid\n\n\n\n\nmax_temp_c\nMaxTemp\nMaximum Temp (°C)\n\n\nprecipitation_mm\nPrecipitation\nprecmm\n\n\nmean_year_growth\nMeanYearGrowth\nMean growth/year\n\n\nsex\nsex\nM/F\n\n\nweight\nweight\nw.\n\n\ncell_type\nCellType\nCell type\n\n\nobservation_01\nfirst_observation\n1st Obs.\n\n\n\n\n\n\n\n\nThe first column of variable names use the snake case naming convention which uses an underscore to replace a space and letters are in lower case. The second column of good alternative variable names use the camel case naming convention in which phrases are written without spaces or punctuation and with capitalised words.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-date",
    "href": "data-entry-storage.html#sec-spreadsheet-data-date",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.3 Write dates as YYYY-MM-DD",
    "text": "12.3 Write dates as YYYY-MM-DD\n\n\n\n\n\n\nFigure 12.1: ISO 8601 from https://xkcd.com/1179/\n\n\n\nWhen entering dates, we strongly recommend using the global ISO 8601 standard, YYYY-MM-DD, such as 2013-02-27.\nMicrosoft Excel’s treatment of dates can cause problems in data. It stores them internally as a number, with different conventions on Windows and Macs (see Note 12.1). So, you may need to manually check the integrity of your data when they come out of Excel.\n\n\n\n\n\n\nNote 12.1: Date systems in Microsoft Excel\n\n\n\nExcel supports two date systems, the 1900 date system and the 1904 date system. Each date system uses a unique starting date from which all other workbook dates are calculated. All newer versions of Excel calculate dates based on the 1900 date system, while older versions used the 1904 system.\nWhen you copy dates from a workbook created in an earlier version to a workbook created in a newer version, they will be converted automatically unless the option to \"Automatically convert date system\" is disabled in Preferences &gt; Edit &gt; Date Options. If this option is disabled, you will receive a message asking whether the dates should be converted when pasted. You have two options. You can convert the dates to use the 1900 date system (recommended). This option makes the dates compatible with other dates in the workbook. Or you can keep the 1904 date system for the pasted dates only.\nThe 1900 date system\nIn the 1900 date system, dates are calculated by using January 1, 1900, as a starting point. When you enter a date, it is converted into a serial number that represents the number of days elapsed since January 1, 1900. For example, if you enter July 5, 2011, Excel converts the date to the serial number 40729. This is the default date system in Excel for Windows, Excel 2016 for Mac, and Excel for Mac 2011. If you choose to convert the pasted data, Excel adjusts the underlying values, and the pasted dates match the dates that you copied.\nThe 1904 date system\nIn the 1904 date system, dates are calculated by using January 1, 1904, as a starting point. When you enter a date, it is converted into a serial number that represents the number of days elapsed since January 1, 1904. For example, if you enter July 5, 2011, Excel converts the date to the serial number 39267. This is the default date system in earlier versions of Excel for Mac. If you choose not to convert the data and keep the 1904 date system, the pasted dates vary from the dates that you copied.\nThe difference between the date systems\nBecause the two date systems use different starting days, the same date is represented by different serial numbers in each date system. For example, July 5, 2011, can have two different serial numbers, as follows:\n\n\n\nTable 12.2: Comparison of Excel’s 1900 and 1904 date systems\n\n\n\n\n\nDate System\nSerial Number\n\n\n\n\n1900\n40729\n\n\n1904\n39267\n\n\n\n\n\n\nThe difference between the two date systems is 1,462 days. This means that the serial number of a date in the 1900 date system is always 1,462 days greater than the serial number of the same date in the 1904 date system. 1,462 days is equal to four years and one day (including one leap day).\n*taken from Microsoft Support documentation\n\n\nTo avoid these issues with dates when using spreadsheets (specifically Excel), we recommend the following:\n\nUse a plain text format for columns in an Excel worksheet that are going to contain dates\n\nDoing so will avoid automatic conversion of these variables into often unpredictable formats. This can be done through the following steps:\n\n\n\n\n\n\n\n\n\nFigure 12.2: Create date variable\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.3: Select date variable\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.4: Set format to text\n\n\n\n\n\n\nStep 1: Create a date variable (see Figure 12.2).\nStep 2. Select the date variable you created (see Figure 12.3).\nStep 3: In the menu bar, select Format –&gt; Cells –&gt; Choose “Text” on the left (see Figure 12.4).\nThis approach will only work if you are creating the date variable first and when no date values have been entered yet. If you do this on a date variable that already contain dates, Excel will convert them to a text value of their underlying numeric representation (as described in Note 12.1).\n\nPlace an apostrophe at the start of a date value entry\n\nAnother way to force Excel to treat dates as text is to begin the date with an apostrophe, like this: '2014-06-14. Excel will treat the cells as text, but the apostrophe will not appear when you view the spreadsheet or export it to other formats. This is a handy trick, but it requires impeccable diligence and consistency.\n\n\n\n\n\n\nFigure 12.5: Enter an apostrophe followed by YYYY-MM-DD date format\n\n\n\n\nCreate three separate columns with year, month, and day\n\nThese will be ordinary numbers, and so Excel will not mess them up. If there is an existing date variable already, you can convert that to year, month, and day columns by using the built-in date functions in Excel that extract year (Figure 12.6), month (Figure 12.7), and day (Figure 12.8) values from a date variable.\n\n\n\n\n\n\n\n\n\nFigure 12.6: Extract year from date value\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.7: Extract month from date value\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.8: Extract year from date value\n\n\n\n\n\n\n\nRepresent dates as an 8-digit integer of the form YYYYMMDD\n\nFor example, 20140614 for 2014-06-14 (see Figure 12.9).\n\n\n\n\n\n\nFigure 12.9: Date value as text in YYYYMMDD format",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-no-empty-cells",
    "href": "data-entry-storage.html#sec-spreadsheet-no-empty-cells",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.4 No empty cells",
    "text": "12.4 No empty cells\nFill in all cells. Use some common code for missing data to make it clear that the data are known to be missing rather than unintentionally left blank.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-one-thing-cell",
    "href": "data-entry-storage.html#sec-spreadsheet-data-one-thing-cell",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.5 Put just one thing in a cell",
    "text": "12.5 Put just one thing in a cell\nYour spreadsheet should have cells that each contain one piece of data only. Putting more than one type of data value in a cell is not considered best practice.\nFor example, you might have a column with information on year (containing values of either 2022, 2023, or 2024) and sex (Male or Female) as year-sex such as 2022-Male, 2022-Female, and so on and so forth. It would be better to separate this into year and sex columns (containing 2022 and Male).\n\n\n\n\n\n\n\n\n\nFigure 12.10: Combined year-sex variable\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.11: Year and sex as separate variables\n\n\n\n\n\n\nOr you might include units alongside measurements such as weight. It is better to have a variable for the weight and then a separate variable for the units.\n\n\n\n\n\n\n\n\n\nFigure 12.12: Combined weight-unit variable\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.13: Separate weight and unit variables\n\n\n\n\n\n\nIt is even better to just have a variable for weight and then document the units used in a separate data dictionary (see Section 12.7 on creating a data dictionary).\nFinally, do not merge cells. It might look pretty, but you end up breaking the rule of no empty cells.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-rectangle",
    "href": "data-entry-storage.html#sec-spreadsheet-data-rectangle",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.6 Make it a rectangle",
    "text": "12.6 Make it a rectangle\nA single big rectangle with rows corresponding to subjects and columns corresponding to variables is the best layout for data within a spreadsheet. The first row should always contain variable names. Do not use more than one row for the variable names.\n\n\n\n\n\n\nFigure 12.14: Data in spreadsheet with rectangular layout",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-dictionary",
    "href": "data-entry-storage.html#sec-spreadsheet-data-dictionary",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.7 Create a dictionary",
    "text": "12.7 Create a dictionary\nHaving a separate file (see Figure 12.15) that outlines all variables can be very helpful, particularly if it’s organised in a structured layout so data analysts can use it effectively in their analyses. We recommend that this data dictionary includes the following information:\n\nThe precise variable names as they appear in the dataset.\nA detailed description explaining what the variable represents.\nThe measurement units associated with each variable.\nThe list of possible values (for categorical variables) and/or typical range of values expected (for numerical variables) for that variable.\n\nAn example of this data dictionary within an accompanying Word document metadata in a project-based workflow is shown in Figure 12.16.\n\n\n\n\n\n\n\n\n\nFigure 12.15: A project-based workflow structure with a separate file for metadata\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.16: A recommended layout and contents of a project metadata containing a data dictionary\n\n\n\n\n\n\nAn alternative to a separate metadata and data dictionary file is to include this documentation as a separate worksheet within the spreadsheet containing the data.\n\n\n\n\n\n\n\n\n\nFigure 12.17: A project-based workflow structure with separate file for data\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.18: Project spreadsheet with raw data in worksheet called “main”\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.19: Project spreadsheet with metadata/dictionary in worksheet called “metadata”",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-calculations",
    "href": "data-entry-storage.html#sec-spreadsheet-data-calculations",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.8 No calculations in the raw data file",
    "text": "12.8 No calculations in the raw data file\nExcel files often come with various calculations and graphs included alongside the data itself. We strongly recommend keeping your primary data file free from any additional content - only raw data should be present. This is because editing the same file for calculations can lead to accidental errors. For instance, when you open a file and start typing without selecting a cell, the text might end up in unexpected cells, causing problems during analysis. To prevent this, protect your main data file from changes, keep it backed up, and refrain from making edits. If you need to perform analyses or create graphs, work on a duplicate of the original file.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-colour-highlighting",
    "href": "data-entry-storage.html#sec-spreadsheet-data-colour-highlighting",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.9 Do not use font colour or highlighting as data",
    "text": "12.9 Do not use font colour or highlighting as data\nYou might be tempted to highlight particular cells with suspicious data, or rows that should be ignored. Or the font or font colour might have some meaning. Instead, add another column with an indicator variable (e.g., ”trusted” with values TRUE or FALSE).\nAnother possible use of highlighting would be to indicate males and females in a mouse study by highlighting the corresponding rows in different colours. But rather than use highlighting to indicate sex, it is better to include a sex column, with values Male or Female.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-backup",
    "href": "data-entry-storage.html#sec-spreadsheet-data-backup",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.10 Make backups",
    "text": "12.10 Make backups\nRegularly back up your data by storing copies in multiple locations. Consider using a formal version control system such as git (though it’s not ideal for data files). Keep every version of your data files and label them with version numbers, like file_v1.xlsx, file_v2.xlsx, and so on and so forth. For this, the guidance on good file names discussed in Section 11.2.3 is a good reference to follow for naming your file versions. Once you’ve finished entering data or if taking a break, protect the file from accidental changes by setting it to read-only as described below.\n\nWindowsmacOS\n\n\n1, Right-click the file in File Explorer or select the file and then click on the triple dot icon in File Explorer as shown in Figure 12.20.\n\n\n\n\n\n\nFigure 12.20: See more file options in File Explorer\n\n\n\n\nIn the drop-down menu, choose Properties as shown in Figure 12.21.\n\n\n\n\n\n\n\nFigure 12.21: Select Properties option in the drop-down menu\n\n\n\n\nIn the pop-up menu, navigate to the General tab, check the Attributes box for Read-only, and confirm with OK as shown in Figure 12.22.\n\n\n\n\n\n\n\nFigure 12.22: Set the file to read-only\n\n\n\n\n\n\nOpen Finder and right-click on the file.\n\n\n\n\n\n\n\nFigure 12.23: Right-click on the spreadsheet file\n\n\n\n\nSelect Get Info then go to Sharing & Permissions.\n\n\n\n\n\n\n\n\n\n\nFigure 12.24: Select Get Info\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.25: Go to Sharing & Permissions\n\n\n\n\n\n\n\nSet your privileges to Read only.\n\n\n\n\n\n\n\nFigure 12.26: Set privileges to Read only\n\n\n\n\n\n\nAnother option is to password protect the worksheet in the spreadsheet that has the data.\n\nSelect the worksheet to password-protect and then go to Review as shown in Figure 12.31.\n\n\n\n\n\n\n\nFigure 12.27: Go to Review\n\n\n\n\nSelect Protect Sheet as shown in Figure 12.28.\n\n\n\n\n\n\n\nFigure 12.28: Select Protect Sheet\n\n\n\n\nEnter password to protect worksheet as shown in Figure 12.29.\n\n\n\n\n\n\n\nFigure 12.29: Enter password to protect worksheet\n\n\n\n\nRe-enter password to confirm protection as shown in Figure 12.30.\n\n\n\n\n\n\n\nFigure 12.30: Re-enter password to confirm protection\n\n\n\nPassword protection can also be applied to the whole spreadsheet workbook.\n\nOpen the spreadsheet to password-protect and then go to Review as shown in Figure 12.31.\n\n\n\n\n\n\n\nFigure 12.31: Go to Review\n\n\n\n\nSelect Protect Workbook as shown in Figure 12.32.\n\n\n\n\n\n\n\nFigure 12.32: Select Protect Workbook\n\n\n\n\nEnter password to protect workbook as shown in Figure 12.33.\n\n\n\n\n\n\n\nFigure 12.33: Enter password to protect workbook\n\n\n\n\nRe-enter password to confirm workbook protection as shown in Figure 12.34.\n\n\n\n\n\n\n\nFigure 12.34: Re-enter password to confirm workbook protection\n\n\n\n\n\n\n\n\n\nImportant 12.1\n\n\n\nRemember, always back up your data!",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-validation",
    "href": "data-entry-storage.html#sec-spreadsheet-data-validation",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.11 Use data validation to avoid errors",
    "text": "12.11 Use data validation to avoid errors\nWhen handling data entry tasks using spreadsheets, it’s crucial to aim for accuracy and comfort to minimise errors and reduce physical strain. Excel provides a helpful Data Validation feature that can prevent errors during data entry.\nTo use this feature:\n\nChoose the column you wish to validate. In Figure 12.35, the data entry for the age variable is to be validated.\n\n\n\n\n\n\n\nFigure 12.35: Add validation to the age column/variable\n\n\n\n\nGo to the menu bar and select Data --&gt; Data Tools --&gt; Data Validation as shown in\n\n\n\n\n\n\n\nFigure 12.36: Select Data Validation in Data Tools\n\n\n\n\nSet up validation criteria such as:\n\nWhole numbers within a specified range\nDecimal numbers within a specified range\nA predefined list of acceptable values\nText with length restrictions\n\n\nIn Figure 12.37, we set validation for age variable to allow only whole numbers ranging from 120 to 191 (inclusive). The Ignore blank option is ticked so that blank entry will be accepted.\n\n\n\n\n\n\nFigure 12.37: Setup validation criteria for age variable\n\n\n\n\nAdd title and message to guide data entry input as shown in Figure 12.38.\n\n\n\n\n\n\n\nFigure 12.38: Add title and message guide for data entry\n\n\n\n\nAdd error alert to show up when incorrect data entry input is made as shown in Figure 12.39\n\n\n\n\n\n\n\nFigure 12.39: Add error alert to show up when incorrect data entry input is made\n\n\n\nMicrosoft Support has further documentation and guidance on how to apply data validation to cells here.\nAdditionally, formatting cells as “Text” can prevent unintended changes to data like dates or names. In Section 12.3, the steps to change formatting of cells is described and demonstrated.\nWhile these steps may seem tedious, they are valuable in maintaining data integrity and minimising errors during entry.\n\n \n\n\n\n\nMurrell, P. (2013). Data intended for human consumption, not machine consumption. In Bad data handbook (pp. 31–51). O’Reilly Media.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "exploratory-data-analysis.html",
    "href": "exploratory-data-analysis.html",
    "title": "13  Exploratory data analysis",
    "section": "",
    "text": "13.1 Definitions\nData analysis involves steps like cleaning, transforming, inspecting, and modelling data to extract meaningful information. This process can serve various purposes, including exploratory and confirmatory analyses, as well as descriptive or predictive tasks.\nBefore building models or making predictions, it’s essential to explore the data to identify underlying patterns and structures. Data analysts employ both numerical and visual techniques to uncover insights that might be hidden within the dataset. However, it’s crucial for analysts to avoid over-interpreting apparent patterns and to ensure that the findings are reliable for the given data and potentially applicable to new datasets as well. Exploratory data analysis fills this role.\nFollowing are a few other definitions of exploratory data analysis (EDA).\nFrom Wikipedia:\nFrom Wickham and Grolemund (2023):\nFrom SAS:",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "exploratory-data-analysis.html#sec-eda-definition",
    "href": "exploratory-data-analysis.html#sec-eda-definition",
    "title": "13  Exploratory data analysis",
    "section": "",
    "text": "In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n\n\n\nEDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends.\n\n\n\nEDA is necessary for the next stage of data research. If there was an analogy to exploratory data analysis, it would be that of a painter examining their tools and available time, before deciding on what best to paint.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "exploratory-data-analysis.html#sec-eda-origins",
    "href": "exploratory-data-analysis.html#sec-eda-origins",
    "title": "13  Exploratory data analysis",
    "section": "13.2 Origins",
    "text": "13.2 Origins\nThe field of EDA got into the forefront with the publication of Tukey’s Exploratory Data Analysis (Tukey, 1977). Tukey’s aim in writing the book was to provide individual and isolated techniques useful to data analysts. All of Tukey’s techniques in the EDA book can be done by hand with pencil and paper.\n\n\n\n\n\n\n\n\n\nFigure 13.1: Book cover of Tukey’s Exploratory Data Analysis\n\n\n\n\n\nFollowing are some quotes by Tukey from the EDA book.\n\n13.2.1 On measures\nIt is important to understand what you can do before you learn to measure how well you seem to have done it.\n\n\n13.2.2 On pictures\nThe greatest value of a picture is when it forces us to notice what we never expected to see.\n\n\n13.2.3 On exploration\nOnce upon a time, statisticians only explored.\n\n\n13.2.4 On not having one right answer\nThere can be many ways to approach a body of data. Not all are equally good.\n \n\n\n\n\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley publ.\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for data science: Import, tidy, transform, visualize, and model data (Second Edition). O’Reilly.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "univariate-statistics.html",
    "href": "univariate-statistics.html",
    "title": "14  Univariate statistics",
    "section": "",
    "text": "14.1 Continuous variables\nIn this chapter, we will focus on exploratory data analysis of a single variable. We discuss the features of a continuous variable and how these features can be explored, elucidated, tested, and visualised. We then discuss ordinal and nominal categorical variables and the various considerations when exploring these types of variables.\nA continuous variable is a type of variable that can take on any value within a given range. It’s characterised by the ability to be measured and can have an infinite number of values between any two given points. Examples include height, weight, temperature, and time.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "univariate-statistics.html#sec-univariate-continuous",
    "href": "univariate-statistics.html#sec-univariate-continuous",
    "title": "14  Univariate statistics",
    "section": "",
    "text": "14.1.1 Measure of central tendency\nIn statistics, measure of central tendency is a central or typical value for a probability distribution. Most common measures are mean, median, and mode but there are many other measures of central tendency. It is important to note that not all measures of central tendency are robust.\n\nMean\nMean is the sum of a set of values divided by the number of values. Mathematically, it is represented as:\n\n\\[ \\bar{x} ~ = ~ \\sum_{i=1}^{n} x_i \\times \\frac{1}{n} \\]\n\nMean is a non-robust measure of location because it is susceptible/sensitive to a few extreme values in the data.\n\n\nMedian\nFor a dataset \\(x\\) of \\(n\\) elements ordered from smallest to greatest, if \\(n\\) is odd\n\n\\[ \\tilde{x} ~ = ~ x_{(n + 1)/2} \\]\n\nand if \\(n\\) is even\n\n\\[ \\tilde{x} ~ = ~ \\frac{x_{\\frac{n}{2}} + x_{\\frac{n}{2} + 1}}{2} \\]\n\nThe median is the value that divides a dataset into two equal halves, separating the higher half from the lower half. It can be seen as the ‘middle’ value in a data sample, population, or probability distribution. Unlike the mean (often referred to as the “average”), the median is not influenced by extremely large or small values, making it more resistant to skewed data and providing a better representation of the dataset’s center.\nFor instance, when analysing income distribution, the median income is often a more accurate measure of the central tendency because increases in the highest incomes do not affect the median. This characteristic makes the median particularly valuable in robust statistics, where its ability to withstand the impact of outliers is highly regarded.\n\n\n\n\n\n\nNote 14.1: Mean vs Median\n\n\n\nConsider this data of test scores of ten students.\n\n\n\n\n\n\nFigure 14.1: Test scores of ten students\n\n\n\n8 out of the 10 students did really well with scores 75 and higher but two students got really low scores.\nCalculating the mean scores using the spreadsheet, we use the AVERAGE() function:\n=AVERAGE(B2:B11)\nwe get 71.7.\nCalculating the median scores using the spreadsheet, we use the MEDIAN() function:\n=MEDIAN(B2:B11)\nwe get 87.5.\nThe mean and the median can be very different from each other.\nIf median &gt; mean, this would indicate that the continuous variable has some extremely low values\nIf median &lt; mean, this would indicate that the continuous variable has some extremely high values\nWe should be using median instead of mean when performing summary measures for continuous variables\n\n\n\n\n\n14.1.2 Measure of dispersion\nIn statistics, measure of dispersion describes the spread or variability of data points within a dataset. It indicates how much individual values deviate from the central tendency. Essentially, it provides insight into whether the data is tightly or loosely clustered around its center. Common measures of dispersion include variance, standard deviation (SD), and interquartile range (IQR).\nStandard deviation and variance are the most popular choice for measure of dispersion but are not robust to extreme values or outliers.\n\nStandard deviation\n\\[ sd ~ = ~ \\sqrt{\\sum_{i=1}^n \\frac{(x_i - \\bar{x}) ^ 2}{n - 1}} \\]\nwhere:\n\\(n ~ = ~ \\text{number of observations in sample}\\)\n\\(x_i ~ = ~ \\text{individual data point}\\)\n\\(\\bar{x} ~ = ~ \\text{sample mean}\\)\n\nIn Excel, you can calculate standard deviation using the STDEV() function. Using the student scores example:\n=STDEV(B2:B11)\nwhich gives 31.6756128.\nA low standard deviation indicates that the values tend to be close to the mean of the set, while a high standard deviation indicates that the values are spread out over a wider range. The standard deviation is commonly used in the determination of what constitutes an outlier and what does not.\n\n\nInterquartile range (IQR)\nIQR is the difference between the 1st and 3rd quartile of the values of the continuous variable and is a more robust measure of spread.\nExcel doesn’t have a function to calculate IQR but it can be calculated with the QUARTILE() function. Using the student scores example:\n=QUARTILE(B2:B11, 3) - QUARTILE(B2:B11, 1)\nwhich gives 13.75.\n\n\n\n\n14.1.3 Distribution\n\nAssessing distribution using boxplots\nA boxplot, also referred to as a box-and-whisker plot, is a graphical tool used to summarise the distribution of a continuous variable. It provides insights into key aspects such as the median, quartiles, and any potential outliers in a clear and concise manner.\n\n\n\n\n\n\n\n\nFigure 14.2: Boxplot of weight of children 10-15 years of age\n\n\n\n\n\n\nKey Components of a Boxplot\n\nThe Box: The box itself illustrates the IQR, which spans the middle 50% of the data. The lower edge of the box represents the first quartile (25th percentile), while the upper edge marks the third quartile (75th percentile).\nThe Median Line: A vertical line within the box indicates the median, or the 50th percentile, which divides the dataset into two equal halves.\nThe Whiskers: Lines extending from the sides of the box, often referred to as whiskers, show the range of the data beyond the IQR. This is based on 1.5 times the IQR value. The lower whisker is measured out as a distance 1.5 times IQR below the lower edge of the box (lower quartile) while the upper whisker is measured out as a distance 1.5 times IQR above the upper edge of the box (upper quartile).\nOutliers: Data points that lie outside the whiskers are considered outliers and are usually plotted as individual points separate from the main plot.\n\n\n\nPlotting boxplots\n\nExcel 2016 and laterPre-Excel 2016\n\n\nStarting with Excel 2016, Microsoft has included a Box and Whiskers chart capability to Excel.\n\nGo to Insert –&gt; Insert Statistics Chart –&gt; Box and Whisker\n\n\n\n\n\n\n\nFigure 14.3: Insert Box and Whisker chart\n\n\n\n\nSelect base chart –&gt; Chart Design –&gt; Select Data\n\n\n\n\n\n\n\nFigure 14.4: Select data to use for boxplot\n\n\n\n\nEdit range of values to use for boxplot.\n\n\n\n\n\n\n\nFigure 14.5: Edit range of values to use for boxplot\n\n\n\n\nSpecify range of weight values for boxplot.\n\nThe data is in Sheet 1 and found in range E2:E268.\n\n\n\n\n\n\n\n\n\nFigure 14.6: Specify range of weight values for boxplot\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.7: Click OK to confirm range of weight values\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.8: Boxplot is now created\n\n\n\n\n\n\n\n\nBefore Excel 2016 version, there was no boxplot chart type available but boxplots can be made through the following steps:\n\nCalculate the lower whisker, quartile 1, median, quartile 3, and upper whisker summary values of the weight variable.\n\n\n\n\n\n\n\n\n\n\nFigure 14.9: Calculate the median weight\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.10: Calculate quartile 1 of weight\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.11: Calculate quartile 3 of weight\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.12: Calculate the lower whisker\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.13: Calculate the upper whisker\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.14: Add a starting point value of 0\n\n\n\n\n\n\n\nCreate a base stacked column chart.\n\nInsert –&gt; Insert Column or Bar Chart –&gt; Stacked Column\n\n\n\n\n\n\nFigure 14.15: Create a base stacked column chart\n\n\n\n\nSelect data to use for stacked column chart.\n\nSelect base chart –&gt; Chart Design –&gt; Select Data\n\n\n\n\n\n\nFigure 14.16: Select data to use for stacked column chart\n\n\n\n\nEdit range of values to use for stacked column chart.\n\n\n\n\n\n\n\n\n\n\nFigure 14.17: Tap to select range of values to use for stacked column chart\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.18: Specify range of values to use for stacked column chart\n\n\n\n\n\n\n\nReverse stacked column chart axis.\n\nClick on Switch Row/Column\n\n\n\n\n\n\n\n\n\nFigure 14.19: Click on Switch Row/Column\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.20: Confirm chart settings\n\n\n\n\n\n\n\nHide the lower column of the stacked column chart.\n\nClick on the lower column of the stacked column chart. Format the data series by removing its fill.\n\n\n\n\n\n\nFigure 14.21: Hide lower column of the stacked column chart\n\n\n\n\nHide the upper column of the stacked column chart.\n\nClick on the upper column of the stacked column chart. Format the data series by removing its fill.\n\n\n\n\n\n\nFigure 14.22: Hide upper column of the stacked column chart\n\n\n\n\nAdd error bars to replace the upper column.\n\nClick on Chart Elements –&gt; Error Bars –&gt; More Options\n\n\n\n\n\n\nFigure 14.23: Add error bars to replace the upper column\n\n\n\n\nFormat error bars to create the upper whisker.\n\n\n\n\n\n\n\n\n\n\nFigure 14.24: Set error bars to solid line\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.25: Format error bars\n\n\n\n\n\n\n\nHide the second lower column of the stacked column chart.\n\nClick on the second lower column of the stacked column chart. Format the data series by removing its fill.\n\n\n\n\n\n\nFigure 14.26: Hide second lower column of the stacked column chart\n\n\n\n\nAdd error bars to replace the second lower column.\n\nClick on Chart Elements –&gt; Error Bars –&gt; More Options\n\n\n\n\n\n\nFigure 14.27: Add error bars to replace the second lower column\n\n\n\n\nFormat error bars to create the lower whisker.\n\n\n\n\n\n\n\n\n\n\nFigure 14.28: Set error bars to solid line\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.29: Format error bars\n\n\n\n\n\n\n\nChange fill and outline colours of the box.\n\n\n\n\n\n\n\n\n\n\nFigure 14.30: Change fill and outline colours of the upper box\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.31: Change fill and outline colours of the lower box\n\n\n\n\n\n\n\nBoxplot is now created.\n\n\n\n\n\n\n\nFigure 14.32: Boxplot is now created\n\n\n\n*Based on Microsoft Support documentation\n\n\n\n\n\nInterpreting boxplots\nThe median, represented by the line within the box, indicates the central value of the data. The height of the box and the length of the whiskers indicate the spread and variability of the data. A wider box or longer whiskers suggest greater variability. The position of the median line within the box can suggest whether the data is symmetrical, skewed left (negative skew), or skewed right (positive skew). Outliers represent data points that are significantly different from the rest of the data and may warrant further investigation. Comparing box plots for different datasets can help reveal differences in their central tendency, spread, and distribution.\n\n\n\nAssessing distribution using histograms\nA histogram is a graphical representation that displays the distribution of numerical data. It uses adjacent bars to show the frequency or count of data points within specified ranges, known as bins. Histograms provide insights into the data distribution, such as whether it is normally distributed (bell-shaped), skewed, or has multiple peaks (multimodal). They help identify patterns like clusters, gaps, and outliers in the data.\n\n\n\n\n\n\n\n\nFigure 14.33: Histogram of weight of children 10-15 years of age\n\n\n\n\n\n\nPlotting the histogram\nOn a graph, the x-axis (horizontal) represents the bins or intervals of the numerical data. The y-axis (vertical) represents the frequency or count of data points in each bin. Bars are drawn adjacent to each other without gaps, as the bins represent continuous ranges.\nIn Excel, the histogram can be plotted as follows:\n\nCreate a base histogram plot.\n\nGo to Insert –&gt; Insert Statistics Chart –&gt; Histogram\n\n\n\n\n\n\nFigure 14.34: Create a base histogram plot\n\n\n\n\nSelect data to use for histogram.\n\nSelect base chart –&gt; Chart Design –&gt; Select Data\n\n\n\n\n\n\nFigure 14.35: Select data to use for histogram\n\n\n\n\nEdit range of values to use for histogram.\n\n\n\n\n\n\n\n\n\n\nFigure 14.36: Tap to select range of values to use for histogram\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.37: Specify range of values to use for histogram\n\n\n\n\n\n\n\nConfirm range of values to use for histogram.\n\nClick on OK\n\n\n\n\n\n\nFigure 14.38: Confirm range of values to use for histogram\n\n\n\n\nHistogram is now created.\n\n\n\n\n\n\n\nFigure 14.39: Histogram is now created\n\n\n\n\n\nInterpretation\nHistograms provide insights into the data distribution, such as whether it is normally distributed (bell-shaped), skewed, or has multiple peaks (multimodal). Histograms help identify patterns like clusters, gaps, and outliers in the data.\n\n\n\nAssessing distribution using QQ plots\nA QQ plot, or Quantile-Quantile plot, is a graphical tool used to assess whether a dataset follows a specific theoretical distribution (like a normal distribution) or to compare the distributions of two datasets. It does this by plotting the quantiles of the observed data against the quantiles of the theoretical distribution (or the other dataset).\n\nPlotting QQ plots to test for normality\nThere is no built-in chart for QQ plots in Excel. To create a QQ plot to test normal distribution in Excel, we recommend creating a new worksheet and then importing the raw dataset into this worksheet before proceeding with the steps below. We use the school nutrition dataset for this demonstration.\n\nSort weight values in ascending order.\n\nWe recommend doing this step on the worksheet where the data is and create a new variable for the sorted values of weight. The values can be sorted using the SORT() function in Excel as follows (Figure 14.40):\n=SORT(E2:E268)\nThis function will fill the new column/variable with the weight values sorted in ascending order without changing the order of the reference/original data. We recommend doing it this way instead of sorting the whole dataset based on weight so that you can perform the same operation for creating QQ plots for other appropriate variable in the dataset (i.e., height).\n\n\n\n\n\n\n\n\n\nFigure 14.40: Sort weight values in ascending order\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.41: Sorted weight values in ascending order in new column\n\n\n\n\n\n\n\nGive a rank to each value from 1 to the total number of rows.\n\nThis should be created in a new column/variable. This can be done easily using the following formula (Figure 14.42):\n=ROW() - 1\nCopy or drag this formula from the first cell to the following cells in the new column/variable to get the rank for all weight records (Figure 14.43).\n\n\n\n\n\n\n\n\n\nFigure 14.42: Formula for assigning a rank to the sorted weight records\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.43: Copy or drag the formula to the rest of the rows\n\n\n\n\n\n\n\nCalculate the empirical/observed cumulative probabilities.\n\nThe empirical cumulative probabilities are calculated as follows:\n\n\\[ F(x) ~ = ~ \\frac{rank - 0.5}{n} \\]\nwhere:\n\\(rank ~ = ~ \\text{rank of the sorted value of the variable}\\)\n\\(n ~ = ~ \\text{number of records/values of the variable}\\)\n\nIn Excel, it can be calculated as follows (Figure 14.44):\n=(I2-0.5)/COUNT($I$2:$I$268)\n\n\n\n\n\n\n\n\n\n\nFigure 14.44: Calculate the empirical cumulative distribution\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.45: Copy or drag the formula to the rest of the rows\n\n\n\n\n\n\n\nCalculate the theoretical quantiles.\n\nThe theoretical quantiles for each of the empirical probabilities are calculated as follows:\n\n\\[ q ~ = ~ F^{-1} \\times p \\]\nwhere:\n\\(q ~ = ~ \\text{theoretical quantile}\\)\n\\(F^{-1} ~ = ~ \\text{inverse of the cumulative distribution function}\\)\n\\(p ~ = ~ \\text{cumulative probability}\\)\n\nIn Excel, it can be calculated as follows (Figure 14.46):\n=NORM.S.INV(J2)\n\n\n\n\n\n\n\n\n\n\nFigure 14.46: Calculate the theortetical quantiles\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.47: Copy or drag the formula to the rest of the rows\n\n\n\n\n\n\n\nCalculate the slope and intercept of the line of agreement (QQ line).\n\nIn a QQ (quantile-quantile) plot, the line of agreement, also known as the 45-degree line or reference line, represents the expected relationship if two datasets being compared have identical distributions. If the points on the QQ plot fall close to this line, it suggests the distributions are similar. Deviations from the line indicate differences in the distributions.\nWe recommend creating a new worksheet for these calculations (Figure 14.48) and then creating layout in which the various values can be calculated as shown in Figure 14.49.\n\n\n\n\n\n\n\n\n\nFigure 14.48: Create new worksheet for QQ line calculations\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.49: Create a layout for the calculations of the various values\n\n\n\n\n\n\n\nCalculate the empirical 25% and 75% quantiles of the weight values (Figure 14.50).\n\nIn Excel, these can be calculated as follows:\n=QUARTILE(qqplot_data!E2:E268,1)     ## 25% quantile\n=QUARTILE(qqplot_data!E2:E268,3)     ## 75% quantile\n\n\n\n\n\n\n\n\n\n\nFigure 14.50: Calculate the empirical 25% quantile\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.51: Calculate the empirical 75% quantile\n\n\n\n\n\n\n\nCalculate the theoretical 25% and 75% quantile of the normal distribution (Figure 14.52).\n\nIn Excel, these can be calculated as follows:\n=NORM.INV(0.25,0,1)     ## 25% quantile\n=NORM.INV(0.75,0,1)     ## 75% quantile\n\n\n\n\n\n\n\n\n\n\nFigure 14.52: Calculate the theoretical 25% quantile of the normal distribution\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.53: Calculate the theoretical 75% quantile of the normal distribution\n\n\n\n\n\n\n\nCalculate the slope and intercept of the QQ line.\n\nThe slope of the QQ line is calculated as follows:\n\n\\[ slope ~ = ~ \\frac{q75_{observed} - q25_{observed}}{q75_{theoretical} - q25_{theoretical}} \\]\n\nThe intercept of the QQ line is calculated as follows:\n\n\\[ intercept ~ = ~ q25_{observed} - slope \\times q25_{theoretical} \\]\n\n\n\n\n\n\n\n\n\n\nFigure 14.54: Calculate the slope of the QQ line\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.55: Calculate the intercept of the QQ line\n\n\n\n\n\n\n\nCreate a base scatter plot.\n\nIn the same worksheet as the calculations, create a base scatter plot as follows (Figure 14.56):\nInsert –&gt; Charts –&gt; Insert Scatter (x, Y) or Bubble Chart –&gt; Scatter\n\n\n\n\n\n\nFigure 14.56: Create a base scatter plot\n\n\n\n\nSelect data to use for scatter plot.\n\nSelect base chart –&gt; Chart Design –&gt; Select Data\n\n\n\n\n\n\nFigure 14.57: Select data to use for scatter plot\n\n\n\n\nAdd a data series to base scatter plot.\n\nClick on Add (Figure 14.58)\n\n\n\n\n\n\nFigure 14.58: Add a data series to base scatter plot\n\n\n\n\nSpecify values for x-axis of the scatter plot.\n\nThe x-axis should use the range of values for the theoretical probabilities.\n\n\n\n\n\n\n\n\n\nFigure 14.59: Select x-axis for editing\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.60: Select range of values for x-axis\n\n\n\n\n\n\n\nSpecify values for y-axis of the scatter plot.\n\nThe y-axis should use the range of values for the ordered values for weight.\n\n\n\n\n\n\n\n\n\nFigure 14.61: Select y-axis for editing\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.62: Select range of values for y-axis\n\n\n\n\n\n\n\nConfirm selection of values for scatter plot.\n\n\n\n\n\n\n\nFigure 14.63: Confirm selection of values for scatter plot\n\n\n\n\nAdd the QQ line to the QQ plot.\n\nChart Elements –&gt; Trendline –&gt; More Options...\nSelect Linear for trendline and tick Set Intercept and then input the value for intercept that we calculated earlier (Figure 14.65).\n\n\n\n\n\n\nFigure 14.64: Add trendline to scatter plot\n\n\n\n\n\n\n\n\n\nFigure 14.65: Edit settings for trendline\n\n\n\n\n\nInterpreting the plot\nIf the data follows the theoretical distribution, the points in the QQ plot will roughly form a straight diagonal line. Deviations from this line indicate that the data does not follow the theoretical distribution, and the nature of the deviations can provide insights into how the data differs from the expected distribution (e.g., skewness, outliers). In simpler terms: A QQ plot is a visual check to see if your data “looks like” it came from a specific distribution. It helps you determine if your data is normally distributed, or if it has some other pattern.\n\n\n\n\n14.1.4 Age Heaping\nAge heaping is the tendency to report children’s ages to the nearest year or adults’ ages to the nearest multiple of five or ten years. Age heaping is very common that is why most reported national statistics use broad age groups.\n\nTesting for age heaping\nThere is no built-in function in Excel that tests for age heaping. The following steps can be performed in Excel to test whether there is significant age heaping in a dataset with age values. These steps use the school nutrition data for demonstration. We recommend that these steps be done in a separate worksheet from the raw data to avoid contamination of original dataset.\n\nDetermine an appropriate divisor.\n\nThe school nutrition data records age in months. A useful way of looking at age heaping when age is recorded in months is to examine the remainders when the ages are divided by 12. So, we set the divisor to 12.\n\nCreate a new variable for the remainder when age variable is divided by 12.\n\nCreate a new worksheet containing the raw dataset and create a new variable as shown in Figure 14.66.\n\n\n\n\n\n\nFigure 14.66: Create a new variable for the remainder value\n\n\n\nApply the modulo operator to the age variable using 12 as the divisor. This can be done using the MOD() function is Excel as follows (Figure 14.67):\n=MOD(C2,12)\n\n\n\n\n\n\n\n\n\n\nFigure 14.67: Get the remainder value when age is divided by 12\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.68: Copy or drag the formula to the rest of the rows\n\n\n\n\n\n\n\nCreate a summary table of the counts per remainder values.\n\n\nIn a separate worksheet (see Figure 14.69), create a summary table using Excel’s pivot table functionality.\n\nInsert –&gt; Pivot Table –&gt; From table/range\n\n\n\n\n\n\n\n\n\nFigure 14.69: Create a new worksheet for the summary table\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.70: Initiate pivot table functionality in Excel\n\n\n\n\n\n\n\nSelect the range of data to summarise via pivot table and insert this pivot table into the new worksheet.\n\n\n\n\n\n\n\n\n\n\nFigure 14.71: Select the range of data to summarise\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.72: Select the range of data to summarise\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.73: Select to insert into new worksheet\n\n\n\n\n\n\n\nSelect the rows of the summary table.\n\n\n\n\n\n\n\n\n\n\nFigure 14.74: Select the variable for the rows of the summary table\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.75: Drag the variable for the rows to the row option of the summary table\n\n\n\n\n\n\n\nSelect the values of the summary table.\n\n\n\n\n\n\n\n\n\n\nFigure 14.76: Select the values of the summary table\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.77: Select the value field settings of the summary table\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.78: Select the appropriate summary measure\n\n\n\n\n\n\n\nThe summary table is now completed.\n\n\n\n\n\n\n\nFigure 14.79: Summary table completed\n\n\n\n\nCalculate the expected counts of the remainder values if there was no age heaping.\n\nThe expected counts can be calculated as follows:\n\n\\[ \\text{expected counts} ~ = ~ \\frac{n}{d-1} \\]\nwhere:\n\\(n ~ = ~ \\text{number of records}\\)\n\\(d ~ = ~ \\text{divisor}\\)\n\nIn Excel, this can be calculated using the following formula (Figure 14.80):\n=$B$14/COUNT($B$2:$B$13)\n\n\n\n\n\n\n\n\n\n\nFigure 14.80: Calculate expected counts if no age heaping\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.81: Copy or drag the formula to the rest of the rows\n\n\n\n\n\n\n\nPerform a chi-square test on the actual vs expected remainder counts.\n\nA chi-square test is performed to test whether the actual remainder values are significantly different from the expected remainder values. This test can be performed in Excel as follows (Figure 14.82):\n=CHISQ.TEST(B2:B13,C2:C13)\n\n\n\n\n\n\n\n\n\n\nFigure 14.82: Perform chi-square test in Excel\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.83: Result is p-value of chi-square test\n\n\n\n\n\n\nThe resulting value when the CHISQ.TEST() is performed in Excel is the p-value of the chi-square test. A p-value of less than 0.05 indicates that there is a significant difference between the actual and expected remainder values for the age in the school nutrition dataset. This points to significant age heaping in the dataset.\n\n\n\n14.1.5 Digit preference\nDigit preference is the observation that the final number in a measurement occurs with a greater frequency than is expected by chance. This can occur because of rounding, the practice of increasing or decreasing the value in a measurement to the nearest whole or half unit, or because data are made up.\n\nTesting for digit preference\nThere is no built-in function in Excel that tests for digit preference. The following steps can be performed in Excel to test whether there is significant digit preference in a continuous variable. These steps use the weight variable in the school nutrition data for demonstration. We recommend that these steps be done in a separate worksheet from the raw data to avoid contamination of original dataset.\n\nCreate a new variable for the last digit of the weight variable.\n\nCreate a new worksheet and then create a new variable for the last digit of the weight variable as shown in Figure 14.84.\n\n\n\n\n\n\n\n\n\nFigure 14.84: Create a new worksheet for the digit preference testing\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.85: Create a new variable for the last digit of the weight variable\n\n\n\n\n\n\nThe last digit of the weight variable can be extracted in Excel using the RIGHT() function as follows (see Figure 14.86):\n=RIGHT(E2,1)\n\n\n\n\n\n\n\n\n\n\nFigure 14.86: Get the last digit of the weight variable\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.87: Copy or drag the formula to the rest of the rows\n\n\n\n\n\n\n\nCreate a summary table of the counts of the last digits.\n\nUsing pivot table in Excel, create a summary table of the counts of the last digits. We recommend that the summary table be inserted into a new worksheet.\n\nInsert –&gt; Pivot Table –&gt; From Table/Range\n\n\n\n\n\n\n\nFigure 14.88: Initiate pivot table functionality in Excel\n\n\n\n\nSelect the range of data to summarise via pivot table and insert this pivot table into the new worksheet.\n\n\n\n\n\n\n\n\n\n\nFigure 14.89: Select the range of data to summarise\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.90: Select the range of data to summarise\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.91: Select to insert into new worksheet\n\n\n\n\n\n\n\nSelect the rows of the summary table.\n\n\n\n\n\n\n\n\n\n\nFigure 14.92: Select the variable for the rows of the summary table\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.93: Drag the variable for the rows to the row option of the summary table\n\n\n\n\n\n\n\nSelect the values of the summary table.\n\n\n\n\n\n\n\nFigure 14.94: Select the values of the summary table\n\n\n\n\nCalculate the expected counts of the last digits if there was no digit preference.\n\nThis can be calculated as:\n\n\\[ \\text{expected counts} ~ = ~ \\frac{n}{10} \\]\nwhere:\n\\(n ~ = ~ \\text{number of records}\\)\n\nIn Excel, this can be calculated as follows (Figure 14.95):\n=B12/10\n\n\n\n\n\n\n\n\n\n\nFigure 14.95: Calculate expected counts if no digit preference\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.96: Copy or drag the formula to the rest of the rows\n\n\n\n\n\n\n\nCalculate the chi-square (\\(\\chi^2\\)) statistic.\n\nThe formula to calculate the chi-square (\\(\\chi^2\\)) statistic is:\n\n\\[ \\chi^2 ~ = ~ \\sum^n_{i = 1} \\frac{(O_i - E_i) ^ 2}{E_i} \\]\n\nIn Excel, this can be calculated by first calculating the square of per digit difference in observed and expected counts divided by the expected counts (Figure 14.97).\n=((B2-C2) ^ 2) / C2\n\n\n\n\n\n\n\n\n\n\nFigure 14.97: Calculate the difference in observed and expected counts\n\n\n\n\n\n\n\n\n\n\n\nFigure 14.98: Copy or drag the formula to the rest of the rows\n\n\n\n\n\n\nThen, these differences are summed (Figure 14.99).\n=SUM(D2:D11)\n\n\n\n\n\n\n\nFigure 14.99: Sum the square of per digit difference in observed and expected counts divided by expected counts\n\n\n\n\nCalculate the digit preference score (DPS).\n\nThe digit preference score (DPS) is a summary measure that reduces the bias in digit preference testing because of sample size. The DPS takes into account sample size as shown in this formula:\n\n\\[ DPS ~ = ~ \\sqrt{\\frac{\\chi^2}{\\sum^n_{i=1} O_i \\times (n_{digits} - 1)}} * 100 \\]\n\nIn Excel, this can be calculated as follows (Figure 14.100):\n=SQRT(F2/(SUM(B2:B11)*(COUNT($B$2:$B$11)-1)))*100\n\n\n\n\n\n\n\nFigure 14.100: Calculate the digit preference score\n\n\n\n\n\nInterpreting the digit preference score\nThe following table shows how to interpret the DPS.\n\n\n\nDPS\nClassification\n\n\n\n\n&lt; 8\nExcellent\n\n\nfrom 8 to &lt; 12\nGood\n\n\nfrom 12 to &lt; 20\nAcceptable\n\n\n20 or higher\nProblematic",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "univariate-statistics.html#sec-univariate-categorical",
    "href": "univariate-statistics.html#sec-univariate-categorical",
    "title": "14  Univariate statistics",
    "section": "14.2 Categorical variables",
    "text": "14.2 Categorical variables\nCategorical variables represent attributes or categories instead of numerical values. They organise data into specific groups or labels, and each observation is placed into one group. Categorical variables don’t have an inherent numerical order or ranking. They consist of a finite number of distinct categories or groups.\nCategorical variables can be further classified as either nominal, ordinal, or binary.\n\nNominal - categories with no inherent order such as colours or types of fruit.\nOrdinal - categories with a meaningful order such as education level - primary, secondary, college.\nBinary - a special case of categorical variable with only two categories such as yes or no.\n\n\n14.2.1 Some considerations when dealing with categorical variables\n\nUse the inherent order of ordinal variables\n\n\nOrder nominal variables meaningfully\n\n\nUse colours for categories appropriately",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Univariate statistics</span>"
    ]
  },
  {
    "objectID": "bivariate-statistics.html",
    "href": "bivariate-statistics.html",
    "title": "15  Bivariate statistics",
    "section": "",
    "text": "15.1 Scatter plots\nBivariate means “involving two variables” in statistics. It’s a method used to analyze relationships between two variables, studying how their values might connect or influence each other.\nOne crucial element of bivariate analysis is evaluating the correlation between two variables, which can be positive (both rise together), negative (one rises while the other falls), or non-existent (no apparent connection).\nBivariate data is often visualised using scatter plots (see Section 15.1), which can help reveal patterns or trends between the two variables.\nDan Kopf\nScatter plots are graphs that show how two numerical datasets relate. Each data point is represented by a dot, positioned based on the values of the two variables being compared. They effectively demonstrate both the strength and direction of connections between these variables.\nScatter plots primarily help visualise and determine potential relationships or correlations between two variables. They reveal if the variables trend upward together (positive correlation), downward together (negative correlation), or exhibit no noticeable connection.\nA scatter plot features two axes—the horizontal x-axis and vertical y-axis. Data points are represented as dots, each positioned according to their respective x and y values.\nExamining the pattern of dots in a scatter plot offers insights into the relationship between variables. For example, dots forming an upward-sloping line indicate a positive correlation, whereas a downward slope suggests a negative correlation.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Bivariate statistics</span>"
    ]
  },
  {
    "objectID": "bivariate-statistics.html#sec-scatter-plot",
    "href": "bivariate-statistics.html#sec-scatter-plot",
    "title": "15  Bivariate statistics",
    "section": "",
    "text": "Scatter plots are glorious. Of all the major chart types, they are by far the most powerful. They allow us to quickly understand relationships that would be nearly impossible to recognize in a table or a different type of chart… Michael Friendly and Daniel Denis, psychologists and historians of graphics, call the scatter plot the most “generally useful invention in the history of statistical graphics.”\n\n\n\n\n\n\n\n15.1.1 Creating scatter plots\n\n\n\n\n\n\n\n\nFigure 15.1: Scatterplot of pressure and speed of cyclones\n\n\n\n\n\nExcel has a built-in functionality to create scatterplots. Following are the steps to create a scatterplot in Excel. For this demonstration, we use the cyclones dataset. We recommend creating a new Excel workbook and import the raw cyclones dataset into this workbook to avoid contamination of the original data.\n\nCreate a base scatterplot.\n\nIn a new worksheet, go to Insert –&gt; Insert Scatter (X, Y) or Bubble Chart –&gt; Scatter\n\n\n\n\n\n\n\n\n\nFigure 15.2: Create new worksheet\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.3: Create base scatterplot\n\n\n\n\n\n\n\nSelect data to use for the scatterplot.\n\n\nSelect base scatterplot –&gt; Chart Design –&gt; Select Data\n\n\n\n\n\n\n\nFigure 15.4: Go into Chart Design options to select data\n\n\n\n\nClick on Add to add a new data series (Figure 15.5).\n\n\n\n\n\n\n\nFigure 15.5: Click on Add to add new data series\n\n\n\n\nSelect the appropriate x-axis and y-axis values. For this demonstration, we will use pressure as the x-axis variable and speed as the y-axis variable.\n\n\n\n\n\n\n\n\n\n\nFigure 15.6: Edit x-axis values\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.7: Assign pressure to x-axis\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.8: Edit y-axis values\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.9: Assign speed to y-axis\n\n\n\n\n\n\n\nAdd a trendline (Figure 15.10).\n\nChart Elements –&gt; Trendline –&gt; Linear\n\n\n\n\n\n\nFigure 15.10: Add a trendline\n\n\n\n\nThe scatterplot is now created (Figure 15.11).\n\n\n\n\n\n\n\nFigure 15.11: The scatterplot is now created",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Bivariate statistics</span>"
    ]
  },
  {
    "objectID": "bivariate-statistics.html#sec-measures-association",
    "href": "bivariate-statistics.html#sec-measures-association",
    "title": "15  Bivariate statistics",
    "section": "15.2 Numerical measures of association",
    "text": "15.2 Numerical measures of association\n\n15.2.1 Correlation\nCorrelation is a statistical measure indicating how strongly and in what direction two variables are connected. While it reveals the extent and nature of a linear relationship, it does not imply that one variable causes the other - it only shows how often they move together without explaining why.\n\n\n15.2.2 Correlation measures\nIn this section, we discuss the three most common numerical measures of correlation - Pearson’s correlation coefficient, Spearman’s rank correlation coefficient, and Kendall’s tau rank correlation coefficient.\nOf these three, Spearman’s and Kendall’s are non-parametric and are considered more robust than Pearson’s.\n\nPearson’s correlation coefficient\nPearson’s correlation coefficient, commonly referred to as Pearson’s \\(\\rho\\), is a statistical tool used to evaluate both the strength and direction of a linear association between two continuous variables. It indicates how closely data points align with a line of best fit. The value of \\(\\rho\\) can range from -1 to +1.\nThe absolute value of Pearson’s \\(\\rho\\) reflects the strength of the linear relationship. A score of 1 or -1 signifies a perfect positive or negative correlation, respectively, implying all data points lie perfectly on a line. Conversely, a score of 0 suggests no linear relationship exists.\nPearson’s \\(\\rho\\) can be calculated as follows:\n\n\\[ \\rho ~ = ~ \\frac{covariance(x_1, x_2)}{(n-1) sd_{x_1} sd_{x_2}} \\]\nwhere\n\\(x_1,x_2 ~ = ~ \\text{continuous variables to test correlation of}\\)\n\\(n ~ = ~ \\text{number of data pairs for } x_1,x_2\\)\n\\(sd_{x_1},sd_{x_2} ~ = ~ \\text{standard deviation for } x_1,x_2\\)\n\nIn Excel, either the PEARSON() or the CORREL() function is used to get Pearson’s \\(\\rho\\). To get Pearson’s \\(\\rho\\) for the correlation between pressure and speed from the cyclones dataset, we use the following calculation:\n=PEARSON(H2:H102,I2:I102)\nor\n=CORREL(H2:H102,I2:I102)\nwith both giving a Pearson’s \\(\\rho\\) of -0.7886634.\nTable 15.1 summarises how to interpret the range of Pearson’s \\(\\rho\\) values.\n\n\n\nTable 15.1: Interpretation of various Pearson’s \\(\\rho\\) values.\n\n\n\n\n\nPearson’s \\(\\rho\\)\nInterpretation\n\n\n\n\n+1\nPerfect positive correlation\n\n\n-1\nPerfect negative correlation\n\n\n0\nNo correlation\n\n\n+/- 0.1 to +/- 0.3\nWeak correlation\n\n\n+/- 0.4 to +/- 0.6\nModerate correlation\n\n\n+/- 0.7 to +/- 0.9\nStrong correlation\n\n\n\n\n\n\nIt’s important to note that Pearson’s \\(\\rho\\) only measures linear relationships. It may not accurately reflect the relationship between variables if the relationship is non-linear. Pearson’s correlation is typically used when dealing with normally distributed data that are measured on interval or ratio scales.\n\n\nSpearman’s rank correlation coefficient\nSpearman’s rank correlation coefficient (Spearman’s \\(\\rho\\)) is a statistical measure that assesses the strength and direction of a monotonic relationship between two ranked variables. It’s a non-parametric test, meaning it doesn’t assume data follows a normal distribution and is often used when data is ordinal or when a linear relationship isn’t assumed. The coefficient ranges from -1 to +1, with -1 indicating a perfect negative correlation, +1 indicating a perfect positive correlation, and 0 indicating no correlation.\nCompared to Pearson’s \\(\\rho\\), Spearman’s \\(\\rho\\) performs the correlation test on the rank of the values of the two variables rather than on the values themselves.\nHence, the values of the two variables are ranked first and then the Spearman’s \\(\\rho\\) is calculated based on these ranks as follows:\n\n\\[ \\rho ~ = ~ 1 - \\frac{6 \\sum d ^ 2}{n(n ^ 2 - 1)} \\]\nwhere:\n\\(d ~ = ~ \\text{difference in ranks}\\)\n\\(n ~ = ~ \\text{number of data pairs}\\)\n\nIn Excel, we can calculate Spearman’s \\(\\rho\\) as follows (using the pressure and speed variables in the cyclones dataset):\n\nRank each of the two variables you are testing for correlations using the RANK.AVG() function.\n\nFor this step, we recommend creating a new worksheet and importing the raw dataset to this worksheet to avoid contamination of the raw data (Figure 15.12).\n\n\n\n\n\n\nFigure 15.12: Import raw data to new worksheet\n\n\n\nCreate a new variable for the ranking of the pressure variable and then rank using RANK.AVG() function (Figure 15.13).\n\n\n\n\n\n\n\n\n\nFigure 15.13: Rank the pressure variable\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.14: Copy/drag formula to rest of rows\n\n\n\n\n\n\nCreate a new variable for the ranking of the speed variable and then rank using RANK.AVG() function (Figure 15.14).\n\n\n\n\n\n\n\n\n\nFigure 15.15: Rank the speed variable\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.16: Copy/drag formula to rest of rows\n\n\n\n\n\n\n\nCalculate Spearman’s \\(\\rho\\).\n\n\nCalculate using the CORREL() function in Excel.\n\nThe CORREL() function should be applied to the ranks for pressure and speed (see Figure 15.17) instead of the actual pressure and speed values (as used in the Pearson’s calculation).\n=CORREL(J2:J102,K2:K102)\n\n\n\n\n\n\n\nFigure 15.17: Calculate Spearman’s \\(\\rho\\) using built-in function\n\n\n\nUsing this method, the Spearman’s \\(\\rho\\) for pressure and speed variable in the cyclones dataset is -0.8289627\n\nCalculate using the formula.\n\nFirst, get the square differences of the pressure and speed ranks (Figure 15.18).\n=(J2-K2)^2\nThen apply the Spearman’s \\(\\rho\\) formula as follows:\n=1-((6*SUM(L2:L102))/(COUNT(L2:L102)*(COUNT(L2:L102)^2-1)))\n\n\n\n\n\n\n\n\n\n\nFigure 15.18: Get squared differences of ranks\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.19: Calculate using the formula\n\n\n\n\n\n\nUsing this method, the Spearman’s \\(\\rho\\) for pressure and speed variable in the cyclones dataset is -0.8226878\n\n\n\n\n\n\nNote 15.1: Difference between built-in function result and the calculated result\n\n\n\nThere is a very small difference (-0.0063) between the result when CORREL() function is used compared to when the formula is used. This is likely due to some differences in the way ranking is performed by the CORREL() function compared to the RANK.AVG() function approach used in the calculation approach.\n\n\nThe reference table in Table 15.1 for the Pearson’s \\(\\rho\\) can also be used to interpret Spearman’s \\(\\rho\\).\n\n\nKendall’s rank correlation coefficient\nKendall’s \\(\\tau\\), alternatively referred to as Kendall’s tau rank correlation coefficient, serves as a statistical tool for evaluating the ordinal relationship between two variables. It examines how closely the rankings of data points align across two datasets, irrespective of their specific values. This measure is non-parametric, implying it does not rely on assumptions about data distribution and remains effective even in the presence of outliers.\nKendall’s \\(\\tau\\) assesses how closely the rankings of two variables align. If one variable’s ranking rises, does the other also tend to rise (positive relationship), fall (negative relationship), or show little pattern (nearly zero correlation)?\nThe method involves comparing concordant pairs - where both variables’ rankings follow the same order - and discordant pairs - where their rankings are opposite in order.\nKendall’s \\(\\tau\\) can be calculated as follows:\n\n\\[ \\tau ~ = ~ \\frac{n_{concordant} - n_{discordant}}{\\frac{n (n-1)}{2}}\\]\nwhere:\n\\(n ~ = ~ \\text{number of data pairs}\\)\n\nThere is no built-in function in Excel that calculates Kendall’s \\(\\tau\\). Following are steps on how to arrive at this value for pressure and speed variables in the cyclones dataset using Excel.\n\nCreate a new worksheet and import cyclones dataset.\n\nWe recommend creating a new worksheet and importing the raw dataset to this worksheet (see Figure 15.20) to avoid contamination of the raw data.\n\n\n\n\n\n\nFigure 15.20: Create new worksheet and import cyclones dataset for Kendall’s tau calculations\n\n\n\n\nRank pressure values.\n\nUsing the RANK.AVG() function in Excel, rank the pressure values as follows (Figure 15.21):\n=RANK.AVG(H2,$H$:$H$102)\n\n\n\n\n\n\n\n\n\n\nFigure 15.21: Rank pressure values\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.22: Copy/drag formula to rest of rows\n\n\n\n\n\n\n\nRank speed values.\n\nUsing the RANK.AVG() function in Excel, rank the speed values as follows (Figure 15.23):\n=RANK.AVG(I2,$I$:$I$102)\n\n\n\n\n\n\n\n\n\n\nFigure 15.23: Rank speed values\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.24: Copy/drag formula to rest of rows\n\n\n\n\n\n\n\nSort the table by ascending order based on the pressure rank.\n\nClick on the sort functionality at the pressure column if available (Figure 15.25) or go to Data –&gt; Sort to sort the table by pressure rank in ascending order.\n\n\n\n\n\n\n\n\n\nFigure 15.25: Sort table by pressure rank\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.26: Table sorted by pressure rank\n\n\n\n\n\n\n\nCreate new variable for counts of concordant pairs.\n\nConcordant pairs are items that are ranked higher in variable X and also ranked higher in variable Y, or items ranked lower in X and also ranked lower in Y.\nSince our cyclones data is now sorted in ascending order based on the pressure ranking, we can go down the array of ranks for speed to check for concordance. For example, in the cyclones dataset, if the value of the speed rank in K2 cell is higher than the value of the rank in the K3 cell, then these two are counted as concordant pairs. We make this comparison for the speed rank in K2 for every cell after it and tally the number of concordant pairs. The sum of the counts of concordant pairs for K2 cell becomes the value for the concordant variable for row 2. We then do the same for the next row until we have counts of concordant pairs for each row of data.\nThis can be implemented in Excel using the following formula (Figure 15.27):\n=COUNTIF(K3:$K$102,\"&gt;\"&K2)\n\n\n\n\n\n\n\n\n\n\nFigure 15.27: Count number of concordant pairs\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.28: Copy/drag formula to rest of rows\n\n\n\n\n\n\n\nCreate new variable for counts of discordant pairs.\n\nDiscordant pairs are items that are ranked higher in variable X and are ranked lower in variable Y, or vice versa.\nSince our cyclones data is now sorted in ascending order based on the pressure ranking, we can go down the array of ranks for speed to check for concordance. For example, in the cyclones dataset, if the value of the speed rank in K2 cell is lower than the value of the rank in the K3 cell, then these two are counted as discordant pairs. We make this comparison for the speed rank in K2 for every cell after it and tally the number of discordant pairs. The sum of the counts of discordant pairs for K2 cell becomes the value for the discordant variable for row 2. We then do the same for the next row until we have counts of discordant pairs for each row of data.\nThis can be implemented in Excel using the following formula (Figure 15.29):\n=COUNTIF(K3:$K$102,\"&lt;\"&K2)\n\n\n\n\n\n\n\n\n\n\nFigure 15.29: Count number of discordant pairs\n\n\n\n\n\n\n\n\n\n\n\nFigure 15.30: Copy/drag formula to rest of rows\n\n\n\n\n\n\n\nCalculate Kendall’s \\(\\tau\\).\n\nNow that we have the counts of concordant and discordant pairs, we can calculate Kendall’s \\(\\tau\\) in Excel as follows (Figure 15.31):\n=(SUM(L2:L102)-SUM(M2:M102))/((COUNT(L2:L102)*(COUNT(L2:L102)-1))/2)\nThis results in a Kendall’s \\(\\tau\\) of -0.642178218.\n\n\n\n\n\n\n\n\nFigure 15.31: Calculate Kendall’s tau\n\n\n\nThe reference table in Table 15.1 for the Pearson’s \\(\\rho\\) can also be used to interpret Kendall’s \\(\\tau\\).",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Bivariate statistics</span>"
    ]
  },
  {
    "objectID": "epi-statistics.html",
    "href": "epi-statistics.html",
    "title": "16  Epidemiological statistics",
    "section": "",
    "text": "16.1 Contingency tables\nIn this chapter, we will cover topics and techniques on statistical methods commonly used by epidemiologists in public health investigations or studies that have wider uses and applications to other fields. These methods are considered bread-and-butter techniques for all epidemiologists and are generally easy to implement.\nWe will cover topics on contingency tables, relative risk ratio, odds ratio, and t-test. These methods can be considered bivariate statistics as they are applied on two variables but with binary categorical variables. To demonstrate these techniques, we will use the fem dataset.\nA contingency table, also known as a cross-tabulation, is used in statistics to display the relationship between two or more categorical variables. It organises data by showing the frequency of observations that fall into various combinations of the categories of the variables being examined. It is also usually called a two-by-two table as its common use is for comparing two categories per group. However, contingency tables can also be created with more than two categories per group.\nFor this topic, we will focus on two-by-two tables for simplicity and to be consistent with exploratory data analysis of bivariates.\nFigure 16.1 demonstrates the structure of a two-by-two contingency table with the exposure variable on the rows and the outcome variable on the columns.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Epidemiological statistics</span>"
    ]
  },
  {
    "objectID": "epi-statistics.html#sec-contingency-table",
    "href": "epi-statistics.html#sec-contingency-table",
    "title": "16  Epidemiological statistics",
    "section": "",
    "text": "Figure 16.1: A diagram of a two-by-two contingency table\n\n\n\n\n\n\n\n\n\n\nNote 16.1: Understanding exposure\n\n\n\nSince contingency tables were developed for disease epidemiology, the term exposure has been used which usually pertains to exposure to a risk factor or known causative agent of a particular disease outcome.\nHowever, exposure in a general sense can also mean exposure to a factor or a condition that is known to be associated to a certain outcome which doesn’t have to be a disease. For example, exposure to being female for an outcome of good grades; exposure to being married for an outcome of owning your house, etc.\n\n\n\n16.1.1 Creating two-by-two contingency tables\nIn Excel, a contingency table can be easily created using pivot tables. Using the fem dataset, we can create a contingency table for the exposure variable of lost interest in sex (SEX variable) and the outcome variable of considered suicide (LIFE variable) through the following steps.\n\nCreate a new worksheet for the contingency table.\n\n\n\n\n\n\n\nFigure 16.2: Create a new worksheet for the contingency table\n\n\n\n\nSetup pivot table.\n\n\nInsert –&gt; Pivot Table –&gt; From table/range\n\n\n\n\n\n\n\nFigure 16.3: Initiate pivot table\n\n\n\n\nSelect table/range to pivot and insert into current worksheet.\n\n\n\n\n\n\n\nFigure 16.4: Select table/range\n\n\n\n\n\n\n\n\n\nFigure 16.5: Select fem raw data table\n\n\n\n\n\n\n\n\n\nFigure 16.6: Insert into current worksheet\n\n\n\n\nSelect exposure variable of contingency table.\n\nThe variable for no interest in sex (SEX) is the exposure variable. Select and drag to the rows setting.\n\n\n\n\n\n\n\n\n\nFigure 16.7: Select SEX as exposure variable\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.8: Drag SEX as row of table\n\n\n\n\n\n\n\nSelect outcome variable of contingency table.\n\nThe variable for considered suicide (LIFE) is the outcome variable. Select and drag to the columns setting.\n\n\n\n\n\n\n\n\n\nFigure 16.9: Select LIFE as outcome variable\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.10: Drag LIFE as column of table\n\n\n\n\n\n\n\nSelect values for the contingency table.\n\n\nDrag the LIFE variable into the values setting.\n\n\n\n\n\n\n\nFigure 16.11: Drag LIFE into the values setting\n\n\n\n\nChange the value setting to the COUNT summary measure.\n\nTap on the settings arrow on the value variable –&gt; Value Edit Settings –&gt; Select COUNT\n\n\n\n\n\n\n\n\n\nFigure 16.12: Go to value edit settings\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.13: Select COUNT as summary measure\n\n\n\n\n\n\n\nRemove empty values in exposure variable.\n\n\nClick on settings button for exposure labels and untick blank.\n\n\n\n\n\n\n\nFigure 16.14: Click on settings for exposure\n\n\n\n\n\n\n\n\n\nFigure 16.15: Untick blank exposure label\n\n\n\n\nRemove empty values in outcome variable.\n\n\nClick on settings button for outcome labels and untick blank.\n\n\n\n\n\n\n\nFigure 16.16: Click settings for outcome\n\n\n\n\n\n\n\n\n\nFigure 16.17: Untick blank outcome label\n\n\n\n\nContingency table is now complete.\n\n\n\n\n\n\n\nFigure 16.18: Contingency table is now complete",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Epidemiological statistics</span>"
    ]
  },
  {
    "objectID": "epi-statistics.html#sec-relative-risk-ratio",
    "href": "epi-statistics.html#sec-relative-risk-ratio",
    "title": "16  Epidemiological statistics",
    "section": "16.2 Relative risk ratio",
    "text": "16.2 Relative risk ratio\nRelative risk ratio (RRR) is a measure of the risk of a certain event happening in one group (usually called the exposed group) compared to the risk of the same event happening in another group (usually called the unexposed group). It indicates how much more likely the outcome is in the exposed group compared to the unexposed group.\n\n16.2.1 Calculating relative risk ratio\nUsing the schema of a two-by-two table in Figure 16.1, the relative risk ratio is calculated as follows:\n\n\\[ RRR ~ = ~ \\frac{\\frac{A}{A + B}}{\\frac{C}{C + D}} ~ = ~ \\frac{A \\times (C + D)}{C \\times (A + B)} \\]\nwhere:\n\\(A ~ = ~ \\text{exposed with outcome}\\)\n\\(B ~ = ~ \\text{exposed with no outcome}\\)\n\\(C ~ = ~ \\text{not exposed with outcome}\\)\n\\(D ~ = ~ \\text{not exposed with no outcome}\\)\n\nUsing the pivot table we created in Section 16.1.1, we can calculate the relative risk ratio as follows:\n\\[ RRR ~ = ~ \\frac{A \\times (C + D)}{C \\times (A + B)} \\]\n\\[ RRR ~ = ~ \\frac{58 \\times (5 + 12)}{5 \\times (58 + 38)} ~ = ~ \\frac{58 \\times 17}{5 \\times 96} ~ = ~ \\frac{986}{480} ~ = ~ 2.054167 \\]\nWe can also perform this calculation in Excel using the pivot table we created.\n=(B5*D6)/(B6*D5)\n\n\n\n\n\n\n\nFigure 16.19: Calculate relative risk ratio\n\n\n\n\nCalculating the confidence interval of the relative risk ratio\nFollowing are the steps to calculating the confidence interval1 of the relative risk ratio.\n\nCalculate the standard error of the natural logarithm of relative risk ratio\n\n\n\\[ SE_{log(RRR)} ~ = ~ \\sqrt{\\frac{C}{A(A+C)} ~ + ~ \\frac{D}{B(B+D)}} \\]\n\nIn Excel, this can be calculated as follows:\n=SQRT(B6/(B5*B7)+C6/(C5*C7))\n\n\n\n\n\n\n\nFigure 16.20: Calculate standard error of natural logarithm of relative risk ratio\n\n\n\n\nCalculate the 95% confidence interval of relative risk ratio.\n\n\n\\[ 95\\% ~ CI ~ = ~ e ^ {\\log(RRR) ~ \\pm ~ 1.96 ~ \\times ~ SE_{\\log(RRR)}} \\]\n\nIn Excel, this can be calculated as follows:\n=EXP(LN(B9)-1.96*B10)     ## 95% LCI\n=EXP(LN(B9)+1.96*B10)     ## 95% UCI\n\n\n\n\n\n\n\n\n\n\nFigure 16.21: Calculate 95% lower confidence interval of relative risk ratio\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.22: Calculate 95% upper confidence interval of relative risk ratio\n\n\n\n\n\n\nThe risk of suicidal ideation for those with no interest in sex is 2.05 (95% CI: 1.7298903-2.4392302) times higher than those who have interest in sex.\n\n\n\n16.2.2 Interpreting the relative risk ratio and its confidence interval\nTable 16.1 provides guidance on how to interpret relative risk ratio.\n\n\n\nTable 16.1: Interpretation of relative risk ratio values\n\n\n\n\n\n\n\n\n\nRisk ratio\nInterpretation\n\n\n\n\nRRR = 1\nExposure does not affect outcome\n\n\nRRR &lt; 1\nRisk of outcome is decreased by the exposure (protective factor)\n\n\nRRR &gt; 1\nRisk of outcome is increased by the exposure (risk factor)\n\n\n\n\n\n\nIf the 95% confidence interval doesn’t contain 1, this means that the risk of the outcome given the exposure is significant.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Epidemiological statistics</span>"
    ]
  },
  {
    "objectID": "epi-statistics.html#sec-odds-ratio",
    "href": "epi-statistics.html#sec-odds-ratio",
    "title": "16  Epidemiological statistics",
    "section": "16.3 Odds ratio",
    "text": "16.3 Odds ratio\nOdds ratio (OR) is a measure of association between an exposure and an outcome. It represents the odds that an outcome will occur given a particular exposure compared to the odds of the outcome occurring in the absence of the exposure.\n\n16.3.1 Calculating odds ratio\nUsing the schema of a two-by-two table in Figure 16.1, the odds ratio is calculated as follows:\n\n\\[ OR ~ = ~ \\frac{A/B}{C/D} ~ = ~ \\frac{A \\times D}{B \\times C} \\]\n\\[ OR ~ = ~ \\frac{58 \\times 12}{38 \\times 5} ~ = ~ \\frac{696}{190} ~ = ~ 3.663158 \\]\n\nWe can also perform this calculation in Excel using the pivot table we created.\n=(B5*D6)/(B6*D5)\n\n\n\n\n\n\n\nFigure 16.23: Calculate odds ratio\n\n\n\n\nCalculating the confidence interval of the odds ratio\nThe 95% confidence interval is calculated as follows:\n\n\\[ 95\\% ~ CI ~ = ~ e ^ {\\log(OR) ~ \\pm ~ 1.96 ~ \\times ~ \\sqrt{\\frac{1}{A} + \\frac{1}{B} + \\frac{1}{C} + \\frac{1}{D}}} \\]\n\nIn Excel, this can be calculated as follows:\n=EXP(LN(E9)-1.96*SQRT(1/B5+1/C5+1/B6+1/C6))     ## 95% LCI\n=EXP(LN(E9)+1.96*SQRT(1/B5+1/C5+1/B6+1/C6))     ## 95% UCI\n\n\n\n\n\n\n\n\n\n\nFigure 16.24: Calculate 95% lower confidence interval of odds ratio\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.25: Calculate 95% upper confidence interval of odds ratio\n\n\n\n\n\n\nThe odds of suicidal ideation for those with no interest in sex is 3.66 (95% CI: 11.2339746-2.4392302) times higher than those who have interest in sex.\n\n\n\n16.3.2 Interpreting the odds ratio and its confidence interval\nTable 16.2 provides guidance on how to interpret odds ratio.\n\n\n\nTable 16.2: Interpretation of odds ratio values\n\n\n\n\n\nOdds ratio\nInterpretation\n\n\n\n\nOR = 1\nExposure does not affect odds of outcome\n\n\nOR &gt; 1\nExposure associated with higher odds of outcome\n\n\nOR &lt; 1\nExposure associated with lower odds of outcome\n\n\n\n\n\n\nIf the 95% confidence interval doesn’t contain 1, this means that the odds of the outcome given the exposure is significant.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Epidemiological statistics</span>"
    ]
  },
  {
    "objectID": "epi-statistics.html#difference-between-relative-risk-ratio-and-odds-ratio",
    "href": "epi-statistics.html#difference-between-relative-risk-ratio-and-odds-ratio",
    "title": "16  Epidemiological statistics",
    "section": "16.4 Difference between relative risk ratio and odds ratio",
    "text": "16.4 Difference between relative risk ratio and odds ratio\nRelative risk ratio approximates odds ratio for outcomes that are rare (&lt; 10%) and as such can be reported interchangeably. In non-rare outcomes, odds ratio will tend to have greater magnitude than relative risk ratio but always in the same direction (negative or positive). In specific study designs, the total population-at-risk is not known hence relative risk ratio cannot be calculated.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Epidemiological statistics</span>"
    ]
  },
  {
    "objectID": "epi-statistics.html#sec-t-test",
    "href": "epi-statistics.html#sec-t-test",
    "title": "16  Epidemiological statistics",
    "section": "16.5 Student t-test",
    "text": "16.5 Student t-test\nSometimes, we want to compare summary numerical values between one group and another. Unlike a contingency table that summarises the counts of the variables, this summary table will usually have the mean or median of the numerical values. We can use the t-test (also known as the Student t-test) to compare whether the mean of the values for one group is different from another group.\n\n16.5.1 Calculating the t-test\nUsing the fem dataset, let’s say for example we wanted to compare the mean age of those who have had thoughts of suicide to those who haven’t had thoughts of suicide. We can use the t-test to compare their mean age. In Excel, there is a built in function that performs the t-test, the T.TEST() function. Following are the steps on how to get the mean age for each group and then how to test if there is a difference between the mean age of the two groups.\n\nSort the fem dataset by the values of the LIFE variable.\n\nWe recommend doing this step on a new worksheet with a fresh instance of the fem dataset imported in (Figure 16.26). Then sort the whole table based on the values of the LIFE variable (Figure 16.27).\n\n\n\n\n\n\n\n\n\nFigure 16.26: Create a new worksheet\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.27: Setup sort\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.28: Sort by LIFE from smallest to largest\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.29: Table is now sorted by LIFE\n\n\n\n\n\n\n\nGet the mean age for the each group value of LIFE variable.\n\n=AVERAGE(B2:B66)     ## Average age of those who thought of suicide\n=AVERAGE(B67:B118)   ## Average age of those who have not thought of suicide\n\n\n\n\n\n\n\n\n\n\nFigure 16.30: Get the mean age for those who have thought of suicide\n\n\n\n\n\n\n\n\n\n\n\nFigure 16.31: Get the mean age for those who have not thought of suicide\n\n\n\n\n\n\n\nPerform t-test on AGE variable between the two groups.\n\nUsing the T.TEST() function:\n=T.TEST(B2:B66,B67:B118,2,2)\n\n\n\n\n\n\n\nFigure 16.32: Perform t-test\n\n\n\n\n\n\n\n\n\nFigure 16.33: Results of the t-test\n\n\n\nThe result of the t-test is the p-value for the test. The result is 0.2691091. These is no significant difference between the mean ages of those who thought of suicide and to those who had no thoughts of suicide.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Epidemiological statistics</span>"
    ]
  },
  {
    "objectID": "epi-statistics.html#footnotes",
    "href": "epi-statistics.html#footnotes",
    "title": "16  Epidemiological statistics",
    "section": "",
    "text": "A confidence interval is a specific range of values, determined using sample data, which probably includes the actual value of an unknown population parameter. It shows how much uncertainty about a sample statistic and provides a likely interval for the corresponding population parameter. For instance, a 95% confidence interval means that if we repeated the sampling and calculation process numerous times, 95% of those intervals would include the true population parameter.↩︎",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Epidemiological statistics</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Bean, R. (2017). How companies say they’re using big data. Harvard\nBusiness Review. https://hbr.org/2017/04/how-companies-say-theyre-using-big-data\n\n\nCarroll, S. R., Rodriguez-Lonebear, D., & Martinez, A. (2019).\nIndigenous data governance: Strategies from united states native\nnations. Data Science Journal, 18(1), 31. https://doi.org/10.5334/dsj-2019-031\n\n\nChoi, Y., Gil-Garcia, J., Burke, G. B., Costello, J., Werthmuller, D.,\n& Aranay, O. (2021). Towards data-driven decision-making in\ngovernment: Identifying opportunities and challenges for data use and\nanalytics. Hawaii international conference on system sciences. https://doi.org/10.24251/HICSS.2021.268\n\n\nIvacko, T. M., Horner, D., & Crawford, M. Q. (2013). Data-driven\ndecision-making in michigan local government. SSRN\nElectronic Journal. https://doi.org/10.2139/ssrn.2351916\n\n\nMiddleton, J. H. (1933). Baking costs. National Association of Cost\nAccountants Bulletin, 14(10).\n\n\nMurrell, P. (2013). Data intended for human consumption, not machine\nconsumption. In Bad data handbook (pp. 31–51). O’Reilly Media.\n\n\nOnunga, J., & Odongo, P. (2025). Digital transformation in public\nadministration and data-driven decision-making: A review of turkana\ncounty government. International Journal of Research and Innovation\nin Applied Science, IX, 234–240. https://doi.org/10.51584/IJRIAS.2024.912022\n\n\nPowell, S. G., Baker, K. R., & Lawson, B. (2008). A critical review\nof the literature on spreadsheet errors. Decision Support\nSystems, 46(1), 128–138. https://doi.org/10.1016/j.dss.2008.06.001\n\n\nPowell, S., Baker, K., & Lawson, B. (2009). Errors in operational\nspreadsheets. Journal of Organizational and End User Computing,\n21(3), 24–36.\n\n\nSayogo, D. S., Yuli, S. B. C., & Amalia, F. A. (2024). Data-driven\ndecision-making challenges of local government in indonesia.\nTransforming Government: People, Process and Policy,\n18(1), 145–156. https://doi.org/10.1108/TG-05-2023-0058\n\n\nSpreadsheet risk management within UK organisations.\n(n.d.). Actuarial Post: For the Modern Actuary. Retrieved June\n12, 2025, from https://www.actuarialpost.co.uk/article/spreadsheet-risk-management-within-uk-organisations-351.htm\n\n\nStobierski, T. (2019, August 26). The advantages of data-driven\ndecision-making. Business insights. https://online.hbs.edu/blog/post/data-driven-decision-making\n\n\nTukey, J. W. (1977). Exploratory data analysis. Addison-Wesley\npubl.\n\n\nUnited Nations General Assembly. (2007). United nations declaration\non the rights of indigenous peoples : Resolution / adopted by the\ngeneral assembly. United Nations. https://www.refworld.org/legal/resolution/unga/2007/en/49353\n\n\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for\ndata science: Import, tidy, transform, visualize, and model data\n(Second Edition). O’Reilly.",
    "crumbs": [
      "References"
    ]
  }
]