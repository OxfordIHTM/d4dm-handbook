[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data for Decision Makers: Data Concepts and Applications",
    "section": "",
    "text": "Preface\nIn today’s data-driven world, the responsibility of public service demands more than experience and intuition; it requires evidence-based decision-making grounded in a deep understanding of data. For government officials at all levels, from local administrators to national policymakers, data is not just a tool - it is an indispensable asset in crafting policies that are effective, equitable, and accountable. Data for Decision Makers is developed with you in mind: to support those entrusted with public leadership in leveraging data to serve communities more effectively.\nAcross the domains of public health, education, transportation, environmental policy, and beyond, the availability of data has never been greater. But with this abundance comes complexity. Making sense of it - identifying relevant patterns, understanding root causes, evaluating outcomes, and anticipating future trends - requires more than access. It demands a strong foundation in the principles and practices of modern data use.\nThis course highlights how data literacy empowers government officials to navigate uncertainty, combat misinformation, and design policies that truly respond to the needs of the public. From statistical reasoning and geographic information systems to predictive modelling and real-time dashboards, the tools of data are transforming governance. Understanding these tools is essential to strengthening transparency, accountability, and public trust.\nThis course bridges the gap between technical expertise and policy leadership. It offers clear, accessible explanations of core data concepts alongside practical examples from the public sector. Whether your role involves strategic planning, budget allocation, programme evaluation, or legislative development, this course will help you make more informed, timely, and impactful decisions.\nPublic service is a profound responsibility. By embracing the potential of data, government leaders can enhance their ability to meet that responsibility with clarity, foresight, and integrity.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Data-driven decision-making\nIn an era defined by information, the ability to make sound decisions increasingly hinges on the intelligent use of data. Across sectors and industries, from healthcare and education to finance and public policy, decision-makers are confronted with unprecedented volumes of information. Yet, it is not the sheer quantity of data that holds value, but our capacity to interpret, understand, and apply it effectively.\nData is more than numbers on a spreadsheet; it is the language of modern insight. When approached with the right tools and understanding, it becomes a powerful asset for identifying patterns, predicting outcomes, evaluating strategies, and ultimately, improving results. For decision-makers, this means developing fluency not just in reading reports, but in questioning assumptions, validating sources, and interpreting results within context.\nUnderstanding modern data concepts - from statistical reasoning and data visualisation to machine learning and real-time analytics - is no longer optional. It is foundational. These concepts empower leaders to move beyond intuition and anecdote, and toward evidence-based action. As data continues to shape the world around us, the ability to engage with it critically and creatively is becoming an essential skill.\nThis course aims to equip its participants with both the conceptual grounding and practical knowledge to navigate this landscape. Whether you are a seasoned executive, a policy analyst, or an emerging leader, this course is designed to bridge the gap between data science and decision-making. It demystifies the tools and techniques of modern data analysis and offers real-world applications that demonstrate how data can drive progress and innovation.\nGood decisions are not just supported by data; they are shaped by those who know how to use it wisely.\nData-driven decision-making or DDDM refers to the process of making decisions based on data and information rather than intuition or experience alone. It involves collecting, analysing, interpreting, and presenting data to support decision-making processes(Stobierski, 2019; Ivacko, Horner & Crawford, 2013; Choi et al., 2021).\nIn this approach, decisions are made by relying on facts, figures, trends patterns, and insights derived from data. The goal is to make objective, evidence-based decisions that are more accurate, consistent, and transparent.\nData-driven decision-making often involves the use of tools, techniques, and technologies such as data analytics, machine learning, artificial intelligence, and visualisation software. By leveraging these tools, organisations can transform raw data into actionable insights that drive better outcomes.\nIn today’s organisations, this approach has become increasingly important as it allows for more objective and accurate decision-making. The process typically includes identifying relevant data sources, applying analytical techniques, and leveraging technologies like machine learning, artificial intelligence, and visualisation tools to transform raw data to actionable insights that drive better outcomes.\nAn organisation that is data-driven also benefits in being able to spot opportunities and threats early. By analysing data regularly, organisations can anticipate changes and act before problems arise.\nSaving costs is another advantage. In a survey of executives of Fortune 1000 companies regarding their data investments since 2012 commissioned by the Harvard Business Review, nearly half (48.4%) of respondents report that they are documenting measurable results from their investments in big data and 80.7% of the executives describing their investments in big data as being successful (Bean, 2017; Stobierski, 2019).",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#sec-data-driven-decision",
    "href": "introduction.html#sec-data-driven-decision",
    "title": "1  Introduction",
    "section": "",
    "text": "Note 1.1: Features of data-driven decision-making\n\n\n\nData-driven decision-making is widely used in various fields such as business, healthcare, finance, education, and government. It allows organisations and individuals to:\n\nInformed Decisions - make decisions based on data rather than assumptions or guesswork;\nImproved Accuracy - educe errors and biases by relying on objective information;\nEfficiency - Optimise resources and processes by identifying trends, patterns, and inefficiencies;\nTransparency - ensure that decisions are made in an open and transparent manner; and,\nScalability - Apply to large-scale operations or complex problems where traditional methods may be insufficient.",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "introduction.html#sec-about-this-course",
    "href": "introduction.html#sec-about-this-course",
    "title": "1  Introduction",
    "section": "1.2 About this course",
    "text": "1.2 About this course\nIn this course, we will explore everything from the basics such as what data is and why it matters to more advanced topics like data collection, storage, analysis, and visualisation. Through practical examples and real-world applications, you’ll learn how to harness the power of data to drive insights, solve problems, and make informed decisions in fields ranging from business and technology to healthcare and beyond. By the end of this course, you’ll not only understand the importance of data but also be prepared to apply these concepts in your own work.\n\n1.2.1 Objectives\nAll these towards the overall objective of making a case for shifting to more data-driven decision-making processes.\nSpecifically, by the end of the course, participants are expected to be able to:\n\nArticulate the value of data driven decision making and programming;\nCritically assess a data by it source, format, structure, types, and classes;\nCritically evaluate the state of their own dataset based on stated best practices;\nOutline the strengths and weaknesses of various types of data tools;\nDemonstrate capacity to use spreadsheet software to clean, process, and structure data; and,\nDemonstrate capacity to use spreadsheet software to perform data analysis.\n\n\n\n1.2.2 Case studies\nTo achieve these objectives, the course employs the case-study method, an approach that involves in-depth examination of a specific individual, group, organisation, or event to understand a complex issue in its real-life context.\nFor this course, the five case studies (one for each of the next five chapters) provide a more nuanced narrative of opportunities and challenges of adopting a data-driven approach to decision-making specifically in the context of governance within governments (rather than just in businesses).\n\n\n1.2.3 The who, what, when, where, how, and why framework\nWhen going through these five case studies, it is recommended to first go through them using the who, what, when, where, how, and why framework as a way to get a firm grounding on the case study details.\nThe “who, what, when, where, how, and why” framework is a systematic approach to understanding and analysing data. Another term that can be used for this framework is descriptive metadata which is data that provides information about other data, but not the content itself. So, if I have an image, the metadata wouldn’t be the actual picture, but the details about who took it, when, or where.\nHere’s a structured explanation of each component within this framework:\n\nWho\nRefers to the individuals or entities involved with the data. This includes stakeholders, users, customers, employees, or business partners who interact with or are affected by the data. More specifically, this may include, among others, information on:\n\nwho owns the data;\nwho manages the data;\nwho collects the data;\nwho stores the data; and,\nwho protects/safeguards the data.\n\n\n\nWhat\nDescribes what the data is about and its type, nature, and provenance. It specifies what information is available, such as numerical data, text, images, etc., which helps in understanding the scope and relevance of the data, and how to work with the data.\n\n\nWhen\nPertains to the timing, period, and/or frequency in which the data was/is being collected, recorded, or analysed.\n\n\nWhere\nIndicates the location where the data is stored or accessed. This could be within a database, on a server, or even from external sources like devices or sensors, providing context about data accessibility and storage.\n\n\nHow\nFocuses on the methods used to collect, process, or extract the data. This includes techniques such as surveys, sensor readings, or existing records, which helps in understanding how reliable and comprehensive the data is.\n\n\nWhy\nAsks for the purpose behind collecting and analysing the data. It clarifies why this information is being gathered i.e., whether it’s for reporting, decision-making, monitoring performance, or other objectives. This in turn guides appropriate actions based on the data insights.\n\n\nSummary\nUsing this structured approach helps clarify each aspect of data, ensuring clarity and focus. It is particularly useful for complex datasets and can help address varying questions based on the user’s role, such as an analyst versus a stakeholder.\nIn summary, using the “who, what, when, where, how, and why” framework provides a systematic method to identify key elements of data, ensuring clarity and focus in data management and analysis.\n\n\n\n\nBean R (2017). How companies say they’re using big data. Harvard Business Review. (https://hbr.org/2017/04/how-companies-say-theyre-using-big-data).\n\n\nChoi Y, Gil-Garcia J, Burke GB, Costello J, Werthmuller D, Aranay O (2021). Towards data-driven decision-making in government: Identifying opportunities and challenges for data use and analytics. Hawaii international conference on system sciences. doi:10.24251/HICSS.2021.268.\n\n\nIvacko TM, Horner D, Crawford MQ (2013). Data-driven decision-making in michigan local government. SSRN Journal. doi:10.2139/ssrn.2351916.\n\n\nStobierski T (2019). The advantages of data-driven decision-making. Business insights [web site]. (https://online.hbs.edu/blog/post/data-driven-decision-making, accessed 12 May 2025).",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "all-about-data.html",
    "href": "all-about-data.html",
    "title": "2  All about data",
    "section": "",
    "text": "2.1 Data Sources\nIn this chapter, we go further into data concepts with a discussion on the sources, formats, structures, types, classes, and systems of data.\nData can be classified as either being of primary or secondary source.\nData sources also refer to where data was obtained or sourced from. These encompass a wide range of information repositories, from traditional databases and files to emerging online platforms and application programming interfaces (APIs).",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All about data</span>"
    ]
  },
  {
    "objectID": "all-about-data.html#sec-data-sources",
    "href": "all-about-data.html#sec-data-sources",
    "title": "2  All about data",
    "section": "",
    "text": "Primary data includes original data collected directly from primary sources such as experiments surveys, or interviews.\nSecondary data exists in various forms like reports, government statistics, or academic publications which are data that have been already collected primarily by some other person and/or organisation/entity who make such data available for others to use for either the same purpose or a totally different use-case altogether from the original purpose.",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All about data</span>"
    ]
  },
  {
    "objectID": "all-about-data.html#sec-data-formats",
    "href": "all-about-data.html#sec-data-formats",
    "title": "2  All about data",
    "section": "2.2 Data Formats",
    "text": "2.2 Data Formats\nData formats define how information is organised, stored, and accessed within a file or database. They determine the structure of data, such as text, numbers, or multimedia, using common formats like CSV, JSON, and XML, each with unique methods for representing data.\nData formats may specifically refer to the following:\n\nRecording format - a format for encoding data for storage on a storage medium\nFile format - a format for encoding data for storage in a computer file\nContainer format (digital) - a format for encoding data for storage by means of a standardised audio/video codecs file format\nContent format - a format for representing media content as data\nAudio format - format for encoded sound data",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All about data</span>"
    ]
  },
  {
    "objectID": "all-about-data.html#sec-data-structure",
    "href": "all-about-data.html#sec-data-structure",
    "title": "2  All about data",
    "section": "2.3 Data Structures",
    "text": "2.3 Data Structures\nA data structure is an organised format for storing data, designed to allow efficient access and modification. It encompasses not just the storage of data but also the relationships between data elements and the operations that can be performed on them. These operations are structured with defined behaviors where operations have specific properties.\nExamples of data structures include:\n\nRelational Databases - Organised into tables with defined relationships (e.g., SQL).\nNoSQL Systems - Flexible storage solutions like document stores or key-value systems.\nHierarchical Structures - Data organised in a tree-like structure, such as XML or JSON.\nFlat Structures - All data resides at the same logical level without hierarchy (e.g., JSON arrays).\nSemi-Structured Formats - Use tags and nested structures for complex data (e.g., JSON).",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All about data</span>"
    ]
  },
  {
    "objectID": "all-about-data.html#sec-data-types",
    "href": "all-about-data.html#sec-data-types",
    "title": "2  All about data",
    "section": "2.4 Data Types",
    "text": "2.4 Data Types\n\nCategorical - Data divided into categories (e.g., gender, color).\nNumerical - Involves numbers, which can be discrete or continuous.\nTemporal - Data with time-based attributes (e.g., dates, times).\nTextual -Includes natural language text and speech data.\nBinary - Represents presence/absence of a feature.\nSpatial - Geospatial data indicating locations (e.g., coordinates).\nMultimedia - Combines multiple types like images, audio, and video.",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All about data</span>"
    ]
  },
  {
    "objectID": "all-about-data.html#sec-data-systems",
    "href": "all-about-data.html#sec-data-systems",
    "title": "2  All about data",
    "section": "2.5 Data Systems",
    "text": "2.5 Data Systems\n\nDatabases - Platforms for managing and querying structured data, including relational (SQL) and NoSQL systems.\nData Lakes - repositories storing raw, unstructured, or semi-structured data in a lake-like structure.\nBig Data Systems - Designed to handle large-scale datasets with distributed processing.\nBusiness Intelligence Tools - Provide analytics capabilities for transforming data into actionable insights.",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All about data</span>"
    ]
  },
  {
    "objectID": "all-about-data.html#sec-data-integration",
    "href": "all-about-data.html#sec-data-integration",
    "title": "2  All about data",
    "section": "2.6 Integration and Considerations",
    "text": "2.6 Integration and Considerations\n\n2.6.1 Data flow\nData is collected from sources, processed or formatted as needed, organised into appropriate types and structures, and managed by suitable systems.\n\n\n2.6.2 Interconnected Components\nEach component (sources, formats, structures) plays a role in ensuring data compatibility with various systems, which are then used for classification based on specific needs.",
    "crumbs": [
      "Data Concepts",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>All about data</span>"
    ]
  },
  {
    "objectID": "case_study_one.html",
    "href": "case_study_one.html",
    "title": "3  Data use and analytics in water quality management",
    "section": "",
    "text": "3.1 Leadership role\nThis is a case study about the Division of Water (DOW), a local government agency in the State of New York, which has attempted to improve its analytic capabilities by developing efficient data management practices, suggest governance models, and identify analytic techniques potentially beneficial to addressing harmful algal blooms (HABs; see Figure 3.1) and high chloride concentrations(Choi et al., 2021).\nThe DOW faces challenges in using its legacy systems and traditional analytical methods effectively in addressing the problems of HABs and high chloride levels. DOW aims to enhance its decision-making processes through DDDM by improving its ability to gather and analyse data more effectively, beyond their current capabilities, to better inform policy decisions.\nFrom this process, nine key factors across four overarching determinants have been observed and articulated as being crucial to consider by an organisation in implementing a comprehensive strategy for DDDM (see Note 3.1). These factors interrelate and influence each other, requiring a holistic approach to ensure successful adoption.\nThese key determinants are interrelated and interdependent. For example, if an organisation has strong data infrastructure (determinant 1) but lacks the right analytical tools or skilled personnel (determinant 2), their DDDM efforts will be hampered. Similarly, even with good internal structures (determinant 3), if external regulations make it hard to access necessary tools or collaborate externally (determinants 7 and 9), progress is still limited. Without proper stakeholder engagement (determinant 6) and user involvement (determinant 5), the organisation might develop solutions in isolation, leading to less effective decisions. Moreover, privacy constraints (determinant 8) can affect data availability, which in turn impacts analytical capabilities since data is a key input.\nWhile DDDM is often seen as a technical issue involving tools and data, it’s also deeply influenced by organisational and institutional factors. This makes sense because any significant change requires not just new technology but also cultural shifts within the organisation to embrace these changes.\nThese determinants also influence the ability of an organisation to adapt over time. For example, if the organisation faces challenges in public procurement, which is a structural issue, this could create delays that affect the organisation’s overall strategy. Conversely, strong stakeholder engagement might mitigate some of these delays by providing alternative solutions or resources.\nLeadership plays a critical part in driving organisational change. Without supportive leadership, many of these determinants could be obstacles rather than opportunities. For instance, if leaders aren’t committed to DDDM, they might not push for necessary cultural shifts or investment in new tools.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data use and analytics in water quality management</span>"
    ]
  },
  {
    "objectID": "case_study_one.html#sec-balance-practices",
    "href": "case_study_one.html#sec-balance-practices",
    "title": "3  Data use and analytics in water quality management",
    "section": "3.2 Balancing existing practices",
    "text": "3.2 Balancing existing practices\nThe balance between existing practices and new methods is important. While the state agency was implementing DDDM, traditional approaches were still relied upon. This blend can be beneficial initially but may need careful management to avoid conflicts or inefficiencies as newer methods prove their worth.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data use and analytics in water quality management</span>"
    ]
  },
  {
    "objectID": "case_study_one.html#sec-measuring-success",
    "href": "case_study_one.html#sec-measuring-success",
    "title": "3  Data use and analytics in water quality management",
    "section": "3.3 Measuring success",
    "text": "3.3 Measuring success\nHow would this state agency assess its progress in implementing DDDM? They might look at metrics like the quality and timeliness of decisions, reduction in issues (like HABs), efficiency improvements, and user satisfaction. These outcomes can help gauge whether their efforts are paying off despite facing various challenges.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data use and analytics in water quality management</span>"
    ]
  },
  {
    "objectID": "case_study_one.html#sec-cs1-conclusion",
    "href": "case_study_one.html#sec-cs1-conclusion",
    "title": "3  Data use and analytics in water quality management",
    "section": "3.4 Conclusion",
    "text": "3.4 Conclusion\nA tailored strategy that evaluates specific organisational strengths and weaknesses across these determinants is essential for effective DDDM implementation. This approach ensures that each organisation maximises opportunities while minimising challenges, leading to more informed and efficient decision-making processes.\n\n\n\n\nChoi Y, Gil-Garcia J, Burke GB, Costello J, Werthmuller D, Aranay O (2021). Towards data-driven decision-making in government: Identifying opportunities and challenges for data use and analytics. Hawaii international conference on system sciences. doi:10.24251/HICSS.2021.268.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data use and analytics in water quality management</span>"
    ]
  },
  {
    "objectID": "case_study_two.html",
    "href": "case_study_two.html",
    "title": "4  Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia",
    "section": "",
    "text": "4.1 Context\nIn an era where technology and data are transforming governance, adopting a data-driven approach is crucial for improving decision-making and fostering transparency. This case study explores Indonesia’s journey toward integrating data into local governance, highlighting both challenges and opportunities, and offers recommendations for mid-level government officials to enhance their governance strategies(Sayogo, Yuli & Amalia, 2024).\nIndonesia, the largest archipelagic nation in the world, operates under a federalist system with provinces and regencies. With a diverse population of over 270 million people, it faces significant challenges such as inequality, environmental degradation, and sustainable development. These issues necessitate effective local governance to ensure equitable growth and environmental preservation.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia</span>"
    ]
  },
  {
    "objectID": "case_study_two.html#sec-cs2-current-situation",
    "href": "case_study_two.html#sec-cs2-current-situation",
    "title": "4  Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia",
    "section": "4.2 Current Situation",
    "text": "4.2 Current Situation\nCurrently, Indonesia’s policy-making is often influenced by top-down directives rather than data-driven insights. Decisions are frequently based on the instructions of superior officials due to a history of autocratic administration. Additionally, there is a lack of standardised data quality frameworks, leading to fragmented and siloed data systems. Limited analytics capacity and reliance on outdated technologies further hinder effective decision-making.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia</span>"
    ]
  },
  {
    "objectID": "case_study_two.html#sec-cs2-challenges",
    "href": "case_study_two.html#sec-cs2-challenges",
    "title": "4  Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia",
    "section": "4.3 Challenges",
    "text": "4.3 Challenges\n\nAutocratic Administration A cultural tendency towards hierarchical decision-making limits the use of data in governance.\nFragmented Data Systems Siloed systems across different levels of government result in data inconsistencies and inefficiencies.\nLack of Skilled Personnel Insufficient training and expertise in data analysis impede effective data utilisation.\nPublic Distrust Concerns about data accuracy and misuse erode public confidence in data-driven decisions.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia</span>"
    ]
  },
  {
    "objectID": "case_study_two.html#sec-cs2-opportunities",
    "href": "case_study_two.html#sec-cs2-opportunities",
    "title": "4  Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia",
    "section": "4.4 Opportunities",
    "text": "4.4 Opportunities\n\nRecent Regulations The 2022 Data Governance Regulation provides a framework to standardize data collection and use.\nInternational Collaboration Partnerships with international organizations offer resources for capacity-building and technological support.\nAvailable Data Sources Rich datasets on demographics, environment, and economy can enhance policy-making, such as managing forest fires or coral reef preservation.\nCapacity-Building Training programs can equip officials with data analysis skills, fostering a culture of evidence-based decision-making.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia</span>"
    ]
  },
  {
    "objectID": "case_study_two.html#sec-cs2-recommendations",
    "href": "case_study_two.html#sec-cs2-recommendations",
    "title": "4  Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia",
    "section": "4.5 Recommendations",
    "text": "4.5 Recommendations\n\nDevelop Data Quality Frameworks Establish standardized protocols to ensure data accuracy and consistency across all levels of government.\nEnhance Analytical Skills Implement training programs to build expertise in data analysis and visualisation tools.\nFoster Public Trust Promote initiatives that demonstrate the benefits of data-driven decisions, such as improving public services or environmental outcomes.\nEncourage Collaboration Facilitate intergovernmental cooperation to share best practices and resources for effective data use.\nAdopt Technology Invest in integrated digital platforms to streamline data collection and sharing processes.\nEstablish Feedback Mechanisms Create channels for public input to ensure that data-driven policies reflect community needs and concerns.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia</span>"
    ]
  },
  {
    "objectID": "case_study_two.html#sec-cs2-conclusion",
    "href": "case_study_two.html#sec-cs2-conclusion",
    "title": "4  Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia",
    "section": "4.6 Conclusion",
    "text": "4.6 Conclusion\nIndonesia’s shift towards data-driven governance presents a transformative opportunity to address pressing challenges and enhance decision-making effectiveness. By overcoming existing barriers and leveraging available resources, Indonesia can set a precedent for other developing nations. Mid-level officials worldwide are encouraged to consider these insights in their own governance strategies, fostering a global culture of transparency, collaboration, and innovation in public service.\n\n\n\n\nSayogo DS, Yuli SBC, Amalia FA (2024). Data-driven decision-making challenges of local government in indonesia. TG, 18(1):145–156. doi:10.1108/TG-05-2023-0058.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Enhancing Local Governance Through Data-Driven Decision-Making in Indonesia</span>"
    ]
  },
  {
    "objectID": "case-study-three.html",
    "href": "case-study-three.html",
    "title": "5  The Use of Data in Local Governance - A Michigan Perspective",
    "section": "",
    "text": "5.1 Michigan Public Policy Survey\nThis case study explores the current state of data-driven decision-making in Michigan’s local governments, highlighting challenges and opportunities for integrating data into policy and governance based on the results of the Michigan Public Policy Survey (MPPS)(Ivacko, Horner & Crawford, 2013).\nThe MPPS, established post the 2009 Great Recession, is the first ongoing survey of local leaders across an entire state in the United States, involving over 1,856 jurisdictions in Michigan. It addresses a critical gap by providing insights into local officials’ perspectives, crucial for informed policymaking. Conducted biannually, it tracks long-term trends on fiscal and operational policies while addressing current issues like the COVID-19 pandemic and infrastructure. Collaborations with key associations enhance its credibility and scope.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Use of Data in Local Governance - A Michigan Perspective</span>"
    ]
  },
  {
    "objectID": "case-study-three.html#sec-cs3-current-situation",
    "href": "case-study-three.html#sec-cs3-current-situation",
    "title": "5  The Use of Data in Local Governance - A Michigan Perspective",
    "section": "5.2 Current Situation of Policy and Decision-Making in Michigan Local Governments",
    "text": "5.2 Current Situation of Policy and Decision-Making in Michigan Local Governments\nMichigan’s local governments have seen significant growth in data-driven decision-making (see Figure 5.1 and Figure 5.2).\n\n\n\n\n\n\n\n\n\nFigure 5.1: Percentage of Michigan jurisdictions reporting use of performance data\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.2: Michigan jurisdictions’ history of data use (among current data users), by population density\n\n\n\n\nThis approach is now widespread across jurisdictions of all population sizes (see Figure 5.3) and across regions, with many jurisdictions using data to inform budgeting and resource allocation.\n\n\n\n\n\n\n\n\n\nFigure 5.3: Percentage of Michigan jurisdictions reporting data use, by population density\n\n\n\n\nDespite this progress, most data use remains informal or ad hoc (see Figure 5.4), particularly among smaller communities (see Figure 5.5).\n\n\n\n\n\n\n\n\n\nFigure 5.4: Percentage of Michigan jurisdictions reporting ad hoc vs. systematic data use (among data users)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5.5: Percentage of Michigan jurisdictions reporting ad hoc vs. systematic data use (among data users), by population size\n\n\n\n\nThe MPPS reveals that while larger jurisdictions are more likely to engage in formal performance measurement, over half of the state’s smallest jurisdictions also incorporate some form of data into their decision-making processes (see Figure 5.2). This indicates a trend towards broader adoption, albeit at varying levels of formality.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Use of Data in Local Governance - A Michigan Perspective</span>"
    ]
  },
  {
    "objectID": "case-study-three.html#sec-cs3-challenges",
    "href": "case-study-three.html#sec-cs3-challenges",
    "title": "5  The Use of Data in Local Governance - A Michigan Perspective",
    "section": "5.3 Challenges and Concerns",
    "text": "5.3 Challenges and Concerns\n\nCost Concerns\n\nMany local governments, especially smaller ones with limited resources, perceive data use as costly. The MPPS found that 62% of non-data users cited cost concerns, though only 28% of current users reported significant issues, suggesting costs may be manageable.\n\nInformal Practices\n\nThe reliance on informal methods can lead to inconsistent outcomes and less accountability. Only about 16% of jurisdictions have formal performance measurement practices, indicating a gap in structured data use.\n\nResource Constraints\n\nSmaller jurisdictions often face limitations in staff and financial resources, hindering their ability to adopt more formal data practices.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Use of Data in Local Governance - A Michigan Perspective</span>"
    ]
  },
  {
    "objectID": "case-study-three.html#sec-cs3-opportunities",
    "href": "case-study-three.html#sec-cs3-opportunities",
    "title": "5  The Use of Data in Local Governance - A Michigan Perspective",
    "section": "5.4 Opportunities and Benefits",
    "text": "5.4 Opportunities and Benefits\n\nFiscal Efficiency\n\nData-driven approaches help identify cost savings and program efficiencies, crucial for jurisdictions grappling with fiscal challenges.\n\nImproved Service Delivery\n\nBy aligning services with community needs, data can enhance service quality and responsiveness.\n\nEnhanced Transparency and Trust\n\nEffective use of data fosters transparency, improving public trust in government decisions.\n\nPolicy Communication\n\nData provides a clear evidence base for policy-making, aiding communication between governments and stakeholders.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Use of Data in Local Governance - A Michigan Perspective</span>"
    ]
  },
  {
    "objectID": "case-study-three.html#sec-cs3-conclusion",
    "href": "case-study-three.html#sec-cs3-conclusion",
    "title": "5  The Use of Data in Local Governance - A Michigan Perspective",
    "section": "5.5 Conclusion",
    "text": "5.5 Conclusion\nThe integration of data into Michigan’s local governance has proven valuable despite challenges like cost concerns and resource limitations. The broader adoption of data-driven practices, even informally, highlights its potential to improve decision-making and service delivery.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Use of Data in Local Governance - A Michigan Perspective</span>"
    ]
  },
  {
    "objectID": "case-study-three.html#sec-csc3-recommendations",
    "href": "case-study-three.html#sec-csc3-recommendations",
    "title": "5  The Use of Data in Local Governance - A Michigan Perspective",
    "section": "5.6 Recommendations",
    "text": "5.6 Recommendations\n\nCapacity Building - Invest in training to enhance technical and analytical skills among local officials.\nEncourage Collaboration - Foster partnerships with academic institutions or tech firms to support data initiatives.\nLeverage Resources - Utilise available tools and frameworks, such as those provided by Michigan’s MPPS, to guide data practices.\nPromote Leadership and Cultural Change - Champion leadership roles that prioritize data use and cultivate a culture of evidence-based decision-making.\n\nBy adopting these strategies, countries can effectively integrate data into local governance, enhancing policy outcomes and public trust.\n\n\n\n\nIvacko TM, Horner D, Crawford MQ (2013). Data-driven decision-making in michigan local government. SSRN Journal. doi:10.2139/ssrn.2351916.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>The Use of Data in Local Governance - A Michigan Perspective</span>"
    ]
  },
  {
    "objectID": "case-study-four.html",
    "href": "case-study-four.html",
    "title": "6  Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County",
    "section": "",
    "text": "6.1 Introduction\nThis case study describes the significant steps that the Turkana County local government are taking to modernising early childhood development and education services management through the use of digital technology(Onunga & Odongo, 2025).\nTurkana County, located in northwest Kenya, is a region marked by significant natural resource wealth and cultural diversity. However, it faces challenges such as poverty, infrastructure gaps, and governance inefficiencies. The county’s recent efforts to embrace data-driven decision-making offer valuable insights for enhancing local governance through improved policy formulation and implementation.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County</span>"
    ]
  },
  {
    "objectID": "case-study-four.html#sec-cs4-background",
    "href": "case-study-four.html#sec-cs4-background",
    "title": "6  Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County",
    "section": "6.2 Background",
    "text": "6.2 Background\nTurkana County was established under Kenya’s devolution framework in 2013, with its administrative structure comprising several wards and sub-counties. The county has made strides in adopting digital tools like the Turkana Early Childhood Development and Education (ECDE) Management Information System or TECDEMIS and the Continuous Database Updating System or CODUSYS for education management, reflecting a commitment to modernise governance.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County</span>"
    ]
  },
  {
    "objectID": "case-study-four.html#sec-cs4-current-situation",
    "href": "case-study-four.html#sec-cs4-current-situation",
    "title": "6  Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County",
    "section": "6.3 Current Situation of Policy and Decision-Making",
    "text": "6.3 Current Situation of Policy and Decision-Making\nPolicy-making in Turkana County is characterised by structured processes involving the County Assembly and Executive. Data utilisation is integral to planning and budgeting, with systems like TECDEMIS facilitating real-time data collection and analysis. These tools support decision-makers in tracking program outcomes and resource allocation efficiency.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County</span>"
    ]
  },
  {
    "objectID": "case-study-four.html#sec-cs4-challenges",
    "href": "case-study-four.html#sec-cs4-challenges",
    "title": "6  Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County",
    "section": "6.4 Challenges in Data Utilisation for Governance",
    "text": "6.4 Challenges in Data Utilisation for Governance\nDespite progress, several challenges impede effective data use:\n\nTechnological Barriers: Limited internet access hampers system functionality.\nInstitutional Weaknesses: Insufficient skilled personnel affect system implementation.\nFinancial Constraints: Inadequate funding limits infrastructure development and capacity building.\nSocio-Political Factors: Resistance to change and lack of awareness about data’s value.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County</span>"
    ]
  },
  {
    "objectID": "case-study-four.html#sec-cs4-opportunities",
    "href": "case-study-four.html#sec-cs4-opportunities",
    "title": "6  Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County",
    "section": "6.5 Opportunities for Enhancing Data Use",
    "text": "6.5 Opportunities for Enhancing Data Use\nThe county presents several opportunities:\n\nInvestments in Digital Infrastructure: Initiatives like TECDEMIS and CODUSYS provide a solid foundation.\nPartnerships with Development Agencies: Collaborations with organisations like the Japan International Cooperation Agency or JICA offer resources and expertise.\nCapacity Building: Training programs enhance staff skills in data management and analysis.\nCommunity Engagement: Involving citizens fosters trust and ownership of data initiatives.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County</span>"
    ]
  },
  {
    "objectID": "case-study-four.html#sec-cs4-conclusion",
    "href": "case-study-four.html#sec-cs4-conclusion",
    "title": "6  Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County",
    "section": "6.6 Conclusion",
    "text": "6.6 Conclusion\nEmbracing data-driven governance is crucial for Turkana County to overcome challenges and achieve sustainable development. Effective data use aligns with broader goals of accountability, service efficiency, and inclusive growth.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County</span>"
    ]
  },
  {
    "objectID": "case-study-four.html#sec-cs4-recommendations",
    "href": "case-study-four.html#sec-cs4-recommendations",
    "title": "6  Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County",
    "section": "6.7 Recommendations",
    "text": "6.7 Recommendations\n\nInvest in IT Infrastructure: Expand internet access and upgrade digital tools.\nEnhance Training Programs: Prioritise skills development in data management and analysis.\nFoster Multi-Sectoral Partnerships: Strengthen collaborations with development agencies and the private sector.\nImprove Stakeholder Engagement: Involve communities to build trust and ownership of data initiatives.\nEstablish Monitoring Frameworks: Develop systems to evaluate the impact of data-driven policies.\n\n\n\n\n\nOnunga J, Odongo P (2025). Digital transformation in public administration and data-driven decision-making: A review of turkana county government. IJRIAS, IX:234–240. doi:10.51584/IJRIAS.2024.912022.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Enhancing Data-Driven Decision-Making in Local Governance - A Focus on Turkana County</span>"
    ]
  },
  {
    "objectID": "case-study-five.html",
    "href": "case-study-five.html",
    "title": "7  Indigenous Data Governance in the United States",
    "section": "",
    "text": "7.1 Introduction\nThis case study describes the challenges and opportunities with regard to Indigenous data governance in the United States(Carroll, Rodriguez-Lonebear & Martinez, 2019).\nIndigenous nations in the United States exercise sovereignty over their data, recognising their right to control and manage their own information. This sovereignty is supported by federal laws such as Native American Graves Protection and Repatriation Act or NAGPRA which provide frameworks for protecting Indigenous rights, including those related to data governance.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Indigenous Data Governance in the United States</span>"
    ]
  },
  {
    "objectID": "case-study-five.html#sec-cs5-current-strategies",
    "href": "case-study-five.html#sec-cs5-current-strategies",
    "title": "7  Indigenous Data Governance in the United States",
    "section": "7.2 Current Strategies",
    "text": "7.2 Current Strategies\n\nTribal Data Sovereignty\n\nIndigenous nations establish policies and institutions, like tribal councils, to own and control their data, ensuring it aligns with cultural values.\n\nCollaboration\n\nPartnerships with federal and state governments are facilitated through initiatives like the National Historic Preservation Act or NHPA promoting shared goals in data governance.\n\nCapacity Building\n\nTraining programs and technological infrastructure development enhance technical skills, though resources vary among tribes.\n\nLegal Frameworks\n\nTreaties and international agreements, such as the United Nations Declaration on the Rights of Indigenous Peoples or UNDRIP(United Nations General Assembly, 2007), provide legal backing for data governance, ensuring respect for Indigenous rights.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Indigenous Data Governance in the United States</span>"
    ]
  },
  {
    "objectID": "case-study-five.html#sec-cs5-challenges",
    "href": "case-study-five.html#sec-cs5-challenges",
    "title": "7  Indigenous Data Governance in the United States",
    "section": "7.3 Challenges",
    "text": "7.3 Challenges\n\nLegal Complexities: Overlapping jurisdictions complicate data governance, requiring clear resolution mechanisms.\nResource Limitations: Financial and technical constraints affect smaller tribes’ ability to implement strategies.\nCultural Preservation: Balancing modern data practices with cultural preservation is complex but crucial.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Indigenous Data Governance in the United States</span>"
    ]
  },
  {
    "objectID": "case-study-five.html#sec-cs5-opportunities",
    "href": "case-study-five.html#sec-cs5-opportunities",
    "title": "7  Indigenous Data Governance in the United States",
    "section": "7.4 Opportunities",
    "text": "7.4 Opportunities\n\nGlobal Networks: Engagement with international bodies like the UNDRIP offers support and recognition, enhancing governance effectiveness.\nCapacity Building: Support through grants and partnerships can bridge resource gaps.\nCollaboration: Inter-tribal agreements strengthen collective data management efforts.",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Indigenous Data Governance in the United States</span>"
    ]
  },
  {
    "objectID": "case-study-five.html#sec-cs5-conclusion",
    "href": "case-study-five.html#sec-cs5-conclusion",
    "title": "7  Indigenous Data Governance in the United States",
    "section": "7.5 Conclusion",
    "text": "7.5 Conclusion\nIndigenous data governance in the U.S. is advancing through sovereignty assertion collaboration, capacity building, and international frameworks. While challenges persist, opportunities for improvement are significant. Government officials must support Indigenous nations by respecting their sovereignty, providing resources, and fostering international engagement to enhance data governance effectively.\n\n\n\n\nCarroll SR, Rodriguez-Lonebear D, Martinez A (2019). Indigenous data governance: Strategies from united states native nations. CODATA, 18(1):31. doi:10.5334/dsj-2019-031.\n\n\nUnited Nations General Assembly (2007). United nations declaration on the rights of indigenous peoples : Resolution / adopted by the general assembly. (https://www.refworld.org/legal/resolution/unga/2007/en/49353).",
    "crumbs": [
      "Data Case Studies",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Indigenous Data Governance in the United States</span>"
    ]
  },
  {
    "objectID": "data-security.html",
    "href": "data-security.html",
    "title": "8  Data privacy, security, and protection",
    "section": "",
    "text": "8.1 Definitions\nThe increasing volume of digital data collected in today’s world necessitates robust protection mechanisms. Breaches can lead to devastating consequences, such as identity theft, financial loss, and potential public health risks, particularly in sectors like healthcare where patient privacy is paramount under regulations such as the General Data Protection Regulation (GDPR).\nFor institutions, safeguarding sensitive data is crucial for maintaining customer trust, preventing identity theft, and avoiding the loss of valuable customers due to data breaches.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data privacy, security, and protection</span>"
    ]
  },
  {
    "objectID": "data-security.html#sec-data-security-definitions",
    "href": "data-security.html#sec-data-security-definitions",
    "title": "8  Data privacy, security, and protection",
    "section": "",
    "text": "8.1.1 Data privacy\nData privacy is about controlling how your personal information is collected, used, and shared. It’s about protecting your right to know who has your data, how it’s being used, and who else it’s being shared with. Essentially, it’s the right to privacy in the digital world.\n\n\n8.1.2 Data protection\nData protection encompasses the security strategies and processes designed to safeguard sensitive data against unauthorised access, misuse, corruption, and loss. It aims to maintain the integrity, availability, and confidentiality of data, while also ensuring compliance with relevant regulations and ethical standards.\n\n\n8.1.3 Data security\nData privacy and data security are distinct but related disciplines. Both are core components of an institution’s broader data governance strategy.\nData privacy focuses on the individual rights of data subjects or the users who own the data. For organisations, the practice of data privacy is a matter of implementing policies and processes that allow users to control their data in accordance with relevant data privacy regulations.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data privacy, security, and protection</span>"
    ]
  },
  {
    "objectID": "data-security.html#sec-data-security-legal-frameworks",
    "href": "data-security.html#sec-data-security-legal-frameworks",
    "title": "8  Data privacy, security, and protection",
    "section": "8.2 Legal frameworks",
    "text": "8.2 Legal frameworks\nThe General Data Protection Regulation (GDPR) is a European Union law that controls how organizations handle the personal data of EU residents. It was adopted in 2016 and became effective on May 25, 2018. It aims to give individuals more control over their personal data and to ensure that organizations are more transparent and accountable for how they process that data.\nSince then, other countries have followed suit in creating their own legislation similar to the GDPR. Generally, countries created such laws as a response to the EU’s rollout of GDPR given that the GDPR applies to any organisation that processes the personal data of EU residents, regardless of whether the organisation is located within the EU.\nThe Seychelles has passed into law the Data Protection Act, 2023 otherwise entitled as\n\nAn act for the protection of individuals with regard to the processing of personal data, to recognise the right to privacy envisaged in article 20 of the constitution, to promote and facilitate responsible and transparent flow of information by private and public entities and to provide for other related matters.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data privacy, security, and protection</span>"
    ]
  },
  {
    "objectID": "data-security.html#sec-data-security-principles",
    "href": "data-security.html#sec-data-security-principles",
    "title": "8  Data privacy, security, and protection",
    "section": "8.3 Principles of data protection",
    "text": "8.3 Principles of data protection\nArticle 5 of the GDPR sets out key principles which lie at the heart of the general data protection regime. These key principles are set out right at the beginning of the GDPR and they both directly and indirectly influence the other rules and obligations found throughout the legislation. Therefore, compliance with these fundamental principles of data protection is the first step for controllers in ensuring that they fulfil their obligations under the GDPR. The following is a brief overview of the Principles of Data Protection found in article 5 GDPR:\n\n8.3.1 Lawfulness, fairness, and transparency\nAny processing of personal data should be lawful and fair. It should be transparent to individuals that personal data concerning them are collected, used, consulted, or otherwise processed and to what extent the personal data are or will be processed. The principle of transparency requires that any information and communication relating to the processing of those personal data be easily accessible and easy to understand, and that clear and plain language be used.\n\n\n8.3.2 Purpose Limitation\nPersonal data should only be collected for specified, explicit, and legitimate purposes and not further processed in a manner that is incompatible with those purposes. In particular, the specific purposes for which personal data are processed should be explicit and legitimate and determined at the time of the collection of the personal data. However, further processing for archiving purposes in the public interest, scientific, or historical research purposes or statistical purposes (in accordance with Article 89(1) GDPR) is not considered to be incompatible with the initial purposes.\n\n\n8.3.3 Data Minimisation\nProcessing of personal data must be adequate, relevant, and limited to what is necessary in relation to the purposes for which they are processed. Personal data should be processed only if the purpose of the processing could not reasonably be fulfilled by other means. This requires, in particular, ensuring that the period for which the personal data are stored is limited to a strict minimum (see also the principle of ‘Storage Limitation’ below).\n\n\n8.3.4 Accuracy\nControllers must ensure that personal data are accurate and, where necessary, kept up to date; taking every reasonable step to ensure that personal data that are inaccurate, having regard to the purposes for which they are processed, are erased or rectified without delay. In particular, controllers should accurately record information they collect or receive and the source of that information.\n\n\n8.3.5 Storage Limitation\nPersonal data should only be kept in a form which permits identification of data subjects for as long as is necessary for the purposes for which the personal data are processed. In order to ensure that the personal data are not kept longer than necessary, time limits should be established by the controller for erasure or for a periodic review.\n\n\n8.3.6 Integrity and Confidentiality\nPersonal data should be processed in a manner that ensures appropriate security and confidentiality of the personal data, including protection against unauthorised or unlawful access to or use of personal data and the equipment used for the processing and against accidental loss, destruction or damage, using appropriate technical or organisational measures.\n\n\n8.3.7 Accountability\nFinally, the controller is responsible for, and must be able to demonstrate, their compliance with all of the above-named Principles of Data Protection. Controllers must take responsibility for their processing of personal data and how they comply with the GDPR, and be able to demonstrate (through appropriate records and measures) their compliance.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data privacy, security, and protection</span>"
    ]
  },
  {
    "objectID": "data-security.html#types-of-data-security",
    "href": "data-security.html#types-of-data-security",
    "title": "8  Data privacy, security, and protection",
    "section": "8.4 Types of data security",
    "text": "8.4 Types of data security\nTo enable the confidentiality, integrity and availability of sensitive information, organizations can implement the following data security measures:\n\n8.4.1 Encryption\nBy using an algorithm to transform normal text characters into an unreadable format, encryption keys scramble data so that only authorized users can read it. File and database encryption software serve as a final line of defense for sensitive volumes by obscuring their contents through encryption or tokenization. Most encryption tools also include security key management capabilities.\n\n\n8.4.2 Data erasure\nData erasure uses software to completely overwrite data on any storage device, making it more secure than standard data wiping. It verifies that the data is unrecoverable.\n\n\n8.4.3 Data masking\nBy masking data, organizations can allow teams to develop applications or train people that use real data. It masks personally identifiable information (PII) where necessary so that development can occur in environments that are compliant.\n\n\n8.4.4 Data resiliency\nResiliency depends on how well an organization endures or recovers from any type of failure—from hardware problems to power shortages and other events that affect data availability. Speed of recovery is critical to minimize impact.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data privacy, security, and protection</span>"
    ]
  },
  {
    "objectID": "data-security.html#sec-data-security-summary",
    "href": "data-security.html#sec-data-security-summary",
    "title": "8  Data privacy, security, and protection",
    "section": "8.5 Summary",
    "text": "8.5 Summary\nData privacy, security, and protection are fundamental concerns in today’s digital landscape. They involve protecting personal information from unauthorised access, implementing robust security measures, and upholding ethical standards in data handling. Addressing these challenges effectively requires a holistic approach that integrates technical safeguards with ongoing education and ethical practices to maintain trust and prevent significant risks.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Data privacy, security, and protection</span>"
    ]
  },
  {
    "objectID": "data-tools.html",
    "href": "data-tools.html",
    "title": "9  Data tools",
    "section": "",
    "text": "9.1 Microsoft Excel and other Excel-like spreadsheet software\nWorking with data is a multi-faceted endeavour that involves collecting, storing, analysing, visualising, securing, and managing data across various domains. The various steps in the data pathway (see Figure 9.1) often require specific tools that are best-suited for the task at hand.\nIn this section, we present the most common data tools, describe their key functionalities, and discuss what each tool is best suited for in relation to the steps in the data pathway.\nMicrosoft Excel is versatile spreadsheet software with robust formula capabilities, pivot tables for quick data summarisation, and Power Query for advanced data cleaning. Other than for data collection, it is also suited for detailed analysis, budgeting, and financial tracking. Suitable for complex data management. On the other hand, some may find that using it presents a steeper learning curve and costs of the subscription-based software-as-a-service (SaaS) model as part of Microsoft 365 can be prohibitive.\nAn estimated 750 million up to 1.5 billion people1 use Microsoft Excel. It has numerous applications, including data entry, analysis, accounting, financial modelling, and reporting. It’s used in various fields like business, education, and personal finance to organise, manage, and visualise data.\nOther than Microsoft Excel, there are Excel-like applications available as part of a suite of office applications that use the Open Document Form (ODF), an open file format for word processing documents, spreadsheets, presentations and graphics and using ZIP-compressed XML files. It was developed with the aim of providing an open, XML-based file format specification for office applications. ODF was based on the Sun Microsystems specification for OpenOffice.org XML, the default format for OpenOffice.org and LibreOffice. This standard was originally developed to provide an open standard for office documents. Versions of Microsoft Excel since 2003 use the ODF XML standard to afford compatibility to other spreadsheets that use the standard. A number of free and proprietary software use the ODF XML standard hence there are various Excel-like spreadsheet alternatives available that use the standard2 and are mostly compatible with Microsoft Office/Microsoft 365 applications including Excel. Although generally compatible in almost all of the basic features, Excel-like spreadsheet applications may not fully implement highly customised Excel spreadsheets that use Visual Basic for Applications (VBA) macros as there are significant differences in syntax and implementation to LibreOffice Calc’s Basic macro system and environment.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-google-sheets",
    "href": "data-tools.html#sec-google-sheets",
    "title": "9  Data tools",
    "section": "9.2 Google Sheets",
    "text": "9.2 Google Sheets\nGoogle Sheets, a free and web-based spreadsheet application, is a versatile data tool used for organising, managing, and analysing data, as well as creating visualisations. It’s part of the Google Workspace suite, along with Google Docs and Google Slides. Sheets offers features like pivot tables, formulas, conditional formatting, and data validation for a variety of data-related tasks.\nGoogle Sheets is technically not an Excel-like spreadsheet (although general use and behaviour is similar to Excel) as it doesn’t use the ODF XML standard but rather has its own proprietary format called the Google Sheets format which can only be accessed or utilised through a web browser rather than through a standalone installer for your computer. In order to access/open a Google Sheets format outside of a browser, one h as to download it as either an Excel file or as a comma-separated value (CSV) file which can then be opened in Excel. Google Sheets has similar features and functionalities as Excel but because of its indirect compatibility with Excel and Excel-like ODF XML-compliant software, advanced features of both applications are not interoperable.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-google-forms",
    "href": "data-tools.html#sec-google-forms",
    "title": "9  Data tools",
    "section": "9.3 Google Forms",
    "text": "9.3 Google Forms\nGoogle Forms is a tool for creating online forms, surveys, and quizzes that can be shared with others to collect data. It allows users to create and edit these forms online, collaborate in real-time, and have the collected data automatically entered into a spreadsheet. Google Forms is part of the free, web-based Google Suite and the software-as-a-service (SaaS) Google Workspace which includes Google Docs, Google Sheets, Google Slides, Google Drawings, Google Sites, and Google Keep. Google Forms is only available as a web application.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-airtable",
    "href": "data-tools.html#sec-airtable",
    "title": "9  Data tools",
    "section": "9.4 Airtable",
    "text": "9.4 Airtable\nAirable is a spreadsheet-database hybrid, with the features of a database but applied to a spreadsheet. The fields in an Airtable table are similar to cells in a spreadsheet, but have types such as ‘checkbox’, ‘phone number’, and ‘drop-down list’, and can reference file attachments like images.\nUsers can create a database, set up column types, add records, link tables to one another, collaborate, sort records and publish views to external websites. Users cannot download their database in full, but can download some of the data by manually downloading CSVs for each table.\nAirtable is user-friendly and is designed for ease of use, making it accessible to a wide range of users, including those without technical backgrounds. It also enables users to build and customise applications for various purposes, such as managing product roadmaps, launching marketing campaigns, and tracking job applications. It facilitates collaboration by allowing multiple users to access and work on the same database. Airtable integrates with various other platforms, enabling data to be shared and workflows to be automated.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-quickbooks",
    "href": "data-tools.html#sec-quickbooks",
    "title": "9  Data tools",
    "section": "9.5 QuickBooks and other accounting-specific software",
    "text": "9.5 QuickBooks and other accounting-specific software\nQuickBooks is a popular accounting software designed to help businesses manage their finances, including tasks like bookkeeping, invoicing, expense tracking, and payroll.\nQuickBooks is a widely used accounting software known for its ease of use and automation capabilities. It’s a solution for small to medium-sized businesses (SMEs), offering features like invoicing, expense tracking, inventory management, and payroll processing.\n\n9.5.1 Other Accounting Software\nBeyond QuickBooks, several other software options exist, each with its strengths and weaknesses:\n\nXero: Offers a user-friendly interface and strong integration capabilities, making it popular among small businesses.\nSage 50: A desktop accounting software with robust reporting and features for larger businesses.\nWave Accounting: A free option that provides basic accounting features, suitable for startups and small businesses.\nZoho Books: A comprehensive online accounting software with various features, including project management.\nFreshBooks: A popular choice for freelancers and sole proprietors, known for its simplicity.\n\n\n\n9.5.2 Key Features of Accounting Software\nCommon features across different accounting software include bookkeeping and recording of financial transactions, invoicing, expense tracking and managing and categorising business expenses, payroll processing financial reporting to generate reports like income statements and balance sheets, and inventory management to track and manage inventory levels.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-business-intelligence",
    "href": "data-tools.html#sec-business-intelligence",
    "title": "9  Data tools",
    "section": "9.6 Business intelligence and analytics platforms",
    "text": "9.6 Business intelligence and analytics platforms\nPower BI, Tableau, and Qlik are classified as business intelligence (BI) tools or data analytics platforms. They share the common goal of enabling users to interact with data, visualise it, analyse it, and ultimately, make data-driven decisions. However, they each have unique strengths and features that cater to different needs and preferences.\n\n9.6.1 PowerBI\nMicrosoft’s BI platform offers a wide range of functionalities, including data connectivity, data modelling, interactive visualisations, and dashboard creation. It’s known for its ease of use and seamless integration with other Microsoft products.\n\n\n9.6.2 Qlik\nThis platform focuses on its associative data model, allowing users to explore relationships within data freely. It also offers strong data integration capabilities and is well-suited for large, complex datasets.\n\n\n9.6.3 Tableau\nTableau is highly regarded for its visual analytics capabilities, enabling users to create stunning and interactive dashboards. It’s known for its user-friendly interface and strong visualisation options.\n\n\n9.6.4 Comparison\n\n9.6.4.1 Ease of Use\nPower BI is generally considered to have a more intuitive interface, while Qlik Sense is more powerful but can have a steeper learning curve. Tableau’s drag-and-drop interface is known for its ease of use.\n\n\n9.6.4.2 Data Integration:\nQlik is particularly strong in data integration and can handle diverse data sources, while Tableau offers a dedicated tool (Tableau Prep) for data preparation. Power BI’s data integration capabilities are also robust, particularly when used in conjunction with other Microsoft products.\n\n\n9.6.4.3 Visualisation\nTableau is renowned for its visual analytics, offering a wide array of visual options and a focus on storytelling through data. Power BI also offers extensive visualisation capabilities, and Qlik provides a unique approach with its associative model.\n\n\n9.6.4.4 Scalability and performance\nAll three tools are scalable, but Qlik is particularly well-suited for large, real-time datasets. Power BI is strong for smaller to medium datasets and can leverage Microsoft Azure for scalability. Tableau’s performance depends on the complexity of the dashboards, but it’s generally robust for complex visualisations.\n\n\n9.6.4.5 Pricing\nPower BI is known for its affordable pricing, while Tableau and Qlik Sense can be more expensive, particularly for enterprise users.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-cloud-storage",
    "href": "data-tools.html#sec-cloud-storage",
    "title": "9  Data tools",
    "section": "9.7 Cloud-based data storage",
    "text": "9.7 Cloud-based data storage\nIn today’s digital age, efficient data storage and quick access are crucial, particularly as remote work becomes more prevalent. Cloud storage solutions like Google Drive, Dropbox, and OneDrive have become vital tools for both businesses and individuals due to their ease of use and collaborative features.\n\n9.7.1 Google Drive\nGoogle Drive is a cloud storage service included in the Google Suite or Google Workspace of tools that allows users to store, sync, and access files across multiple devices and platforms via an internet connection. It also offers features like collaboration tools, document creation, and sharing capabilities.\n\n\n9.7.2 OneDrive\nOneDrive is a cloud storage service by Microsoft included in the Microsoft 365 set of applications that provides collaboration, document creation, and sharing tools. It allows users to store and sync files across multiple devices and offers 5GB of free storage. Paid plans are available for additional storage ranging from 50GB to 1TB.\n\n\n9.7.3 Dropbox\nDropbox is a cloud storage service that allows users to store, share, and sync files across multiple devices. Available on Windows, Mac, iOS, and Android, it offers document creation, collaboration, and sharing tools. With 2GB of free storage, paid plans range from 200GB to 3TB for additional needs.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-database",
    "href": "data-tools.html#sec-database",
    "title": "9  Data tools",
    "section": "9.8 Databases",
    "text": "9.8 Databases\nA database is an organised collection of structured and/or unstructured data, typically stored electronically in a computer system. It’s a system for storing and managing data, and it’s managed by a Database Management System (DBMS). Databases are used to store, retrieve, and manipulate data efficiently.\nHence, the concept of a database is both software, which deals with the handling and management of the data, and hardware, which deals with the physical storage of the data.\n\n9.8.1 SQL and other relational databases\nSQL databases, also known as relational databases, are systems that store collections of tables and organise structured sets of data in a tabular columns-and-rows format, similar to that of a spreadsheet. The databases are built using structured query language (SQL), the query language that not only makes up all relational databases and relational database management systems (RDBMS), but also enables them to “talk to each other”.\nThe history of database technology/relational databases SQL was invented as a language in the early 1970s, which means SQL databases have been around for as long as the Internet itself. Dubbed the structured English query language (SEQUEL), SQL was originally created to streamline access to relational database systems and to assist with the processing of information. Today, SQL remains one of the most popular and widely used query languages in open-source database technology due to its flexibility, ease of use, and seamless integration with a variety of different programming languages. You’ll find SQL being used throughout all types of high-performing, data-centric applications.\n\n\n9.8.2 NoSQL\nNoSQL stands for “Not Only SQL.” It refers to a type of database that doesn’t rely on the traditional relational database models, which are organised into tables with fixed schemas and use SQL for querying. NoSQL databases offer a more flexible approach to data storage and querying, often using document, graph, key-value, or other data models. NoSQL databases are equipped to handle large volumes of structured, semi-structured, and unstructured data from non-traditional sources.\nPopular database management systems include Microsoft SQL Server, PostgreSQL, MongoDB, Redis, Elasticsearch, SQLite, MariaDB, IBM Db2, Oracle Database, and MySQL. In essence, databases are fundamental to modern IT infrastructure, enabling organisations to store, manage, and analyse data efficiently for various applications, including websites, apps, and business processes.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-mis",
    "href": "data-tools.html#sec-mis",
    "title": "9  Data tools",
    "section": "9.9 Management information systems",
    "text": "9.9 Management information systems\nA Management Information System (MIS) is an integrated system that collects, processes, stores, and disseminates information to support managerial decision-making and improve operational efficiency. It essentially acts as a tool for gathering and analysing data, converting it into actionable insights, and making those insights available to the right people within an organisation.\n\n9.9.1 Key Features\n\nData Collection and Storage - MIS systems gather data from various sources, both internal (e.g., sales records, inventory) and external (e.g., market trends, competitor information).\nData Processing and Analysis - The collected data is processed and analysed to identify trends, patterns, and opportunities, often using sophisticated tools and techniques.\nInformation Dissemination - The analysed information is then formatted and delivered to managers and other stakeholders in a way that is easy to understand and use.\nDecision Support - MIS provides the information that managers need to make informed decisions about various aspects of their business, such as sales, marketing, finance, and operations.\nImproved Efficiency - By providing timely and accurate information, MIS helps organisations to operate more efficiently, reduce costs, and improve decision-making.\n\n\n\n9.9.2 Examples of MIS applications\n\nSales and Marketing - Tracking sales figures, analysing marketing campaign effectiveness, and identifying customer trends.\nAccounting and Finance - Managing financial records, generating financial statements, and tracking investments.\nHuman Resources - Managing employee information, tracking performance, and supporting recruitment and training activities.\nInventory Management - Tracking inventory levels, managing warehouses, and forecasting demand.\nHealth records - tracking of patients and clients of various health services. This is often called a Health Management Information System (HMIS).\nCustomer-relationship manager - tracking of clients/customers data and interactions with company (see Section 9.10).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-crm",
    "href": "data-tools.html#sec-crm",
    "title": "9  Data tools",
    "section": "9.10 Customer-relationship Manager",
    "text": "9.10 Customer-relationship Manager\nCustomer Relationship Management (CRM) systems are software applications that help businesses manage and analyse customer data and interactions. They are used to collect, organise, and process information about customers, including their interactions, preferences, and purchase history. The goal is to improve customer service, increase customer retention, and drive sales growth.\n\n9.10.1 Key Features and Functionality\n\n\n9.10.2 Data Management\nCRMs store and organise customer data from various sources, like sales interactions, customer service inquiries, marketing campaigns, and social media.\n\n9.10.2.1 Sales Management\nCRMs help track sales opportunities, pipeline management, and sales activities, enabling sales teams to improve efficiency and close deals faster.\n\n\n9.10.2.2 Customer Service\nCRMs facilitate communication with customers, track service requests, and help resolve issues, leading to improved customer satisfaction.\n\n\n9.10.2.3 Marketing Automation\nCRMs can be integrated with marketing automation tools, allowing businesses to personalise and automate marketing campaigns.\n\n\n9.10.2.4 Reporting and Analytics\nCRMs provide insights into customer behaviour, sales performance, and overall business trends, enabling data-driven decision-making.\n\n\n\n9.10.3 Types of CRM Systems\n\nOperational CRM - Focuses on day-to-day customer interactions, such as sales and customer service.\nAnalytical CRM - Analyses customer data to identify trends, patterns, and opportunities.\nCollaborative CRM - Facilitates communication and collaboration between different departments, such as sales, marketing, and customer service.\nStrategic CRM - Uses customer insights to make strategic decisions about product development, pricing, and market positioning.\n\n\n\n9.10.4 Benefits of using a CRM\n\nImproved Customer Service - By having a centralised database of customer information, companies can provide better and more personalised service.\nIncreased Sales - CRMs help sales teams manage leads, track opportunities, and close deals more effectively.\nEnhanced Customer Retention - By understanding customer preferences and needs, businesses can build stronger relationships and retain customers.\nData-Driven Decision Making - CRMs provide valuable insights into customer behaviour and business performance, enabling data-driven decision-making.\nIncreased Efficiency - Automating tasks and streamlining processes can free up employees to focus on more strategic initiatives.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-stats-packages",
    "href": "data-tools.html#sec-stats-packages",
    "title": "9  Data tools",
    "section": "9.11 Statistical packages",
    "text": "9.11 Statistical packages\nSAS, SPSS, and Stata are popular statistical software packages used for data analysis, but have distinct strengths and target industries. SPSS is known for its user-friendly interface, making it popular in social sciences and market research. Stata is a general-purpose statistical software, often favoured for econometrics, and known for its command-line interface and strong data management features. SAS is a powerful system for advanced analytics, business intelligence, and data management, and is widely used in various industries due to its scalability and robustness.\n\n9.11.1 SPSS\nStatistical Package for the Social Sciences or SPSS has a ser-friendly interface and intuitive data management making it suitable for social sciences and market research. It focuses on descriptive and inferential statistics, data exploration, and model building. Its common uses are for surveys, market research, data mining, and other social science applications. The interface is Menu-driven with a graphical user interface.\n\n\n9.11.2 Stata\nStata is a general-purpose software with strong data management capabilities, and a command-line interface. It is used commonly in econometrics, time series analysis, and statistical modelling. Its most common uses are in economics, biomedicine, and political science research. It has some graphical user interface but full capability is accessed via the command-line. It has a graphical output.\n\n\n9.11.3 SAS\nSAS or Statistical Analysis System is robust, scalable, and suitable for advanced analytics, business intelligence, and data management. It can be used for Multivariate analysis, predictive analytics, and large-scale data processing. It’s common uses are for business analytics, data warehousing, and industry-specific applications. The interface to SAS is primarily as a procedural language.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#sec-programming-languages",
    "href": "data-tools.html#sec-programming-languages",
    "title": "9  Data tools",
    "section": "9.12 Programming languages",
    "text": "9.12 Programming languages\nR, Python, and Julia are powerful programming languages frequently used in data science, scientific computing, and related fields. They offer distinct advantages, making them suitable for various tasks.\n\n9.12.1 R\nR is a language and environment for statistical computing and graphics. It is a GNU project which is similar to the S language and environment which was developed at Bell Laboratories (formerly AT&T, now Lucent Technologies) by John Chambers and colleagues. R can be considered as a different implementation of S. There are some important differences, but much code written for S runs unaltered under R.\nR provides a wide variety of statistical (linear and non-linear modelling, classical statistical tests, time-series analysis, classification, clustering, etc.) and graphical techniques, and is highly extensible. The S language is often the vehicle of choice for research in statistical methodology, and R provides an Open Source route to participation in that activity.\nOne of R’s strengths is the ease with which well-designed publication-quality plots can be produced, including mathematical symbols and formulae where needed. Great care has been taken over the defaults for the minor design choices in graphics, but the user retains full control.\nR is available as Free Software under the terms of the Free Software Foundation’s GNU General Public License in source code form. It compiles and runs on a wide variety of UNIX platforms and similar systems (including FreeBSD and Linux), Windows and MacOS.\nR is unique in that it is not general-purpose. It does not compromise by trying to do a lot of things. It does a few things very well, mainly statistical analysis and data visualisation. While you can find data analysis and machine learning libraries for languages like Python, R has many statistical functionalities built into its core. No third-party libraries are needed for much of the core data analysis you can do with the language.\nBut even with this specific use case, it is used in every industry you can think of because a modern business runs on data. Using past data, data scientists and data analysts can determine the health of a business and give business leaders actionable insights into the future of their company.\nJust because R is specifically used for statistical analysis and data visualisation doesn’t mean its use is limited. It’s actually quite popular, ranking 12th in the TIOBE index of the most popular programming languages.\nAcademics, scientists, and researchers use R to analyse the results of experiments. In addition, businesses of all sizes and in every industry use it to extract insights from the increasing amount of daily data they generate.\n\n\n9.12.2 Python\nPython is an interpreted, interactive, object-oriented programming language. It incorporates modules, exceptions, dynamic typing, very high level dynamic data types, and classes. It supports multiple programming paradigms beyond object-oriented programming, such as procedural and functional programming. Python combines remarkable power with very clear syntax. It has interfaces to many system calls and libraries, as well as to various window systems, and is extensible in C or C++. It is also usable as an extension language for applications that need a programmable interface. Finally, Python is portable: it runs on many Unix variants including Linux and macOS, and on Windows.\nPython is a high-level general-purpose programming language that can be applied to many different classes of problems.\nThe language comes with a large standard library that covers areas such as string processing (regular expressions, Unicode, calculating differences between files), internet protocols (HTTP, FTP, SMTP, XML-RPC, POP, IMAP), software engineering (unit testing, logging, profiling, parsing Python code), and operating system interfaces (system calls, filesystems, TCP/IP sockets). Look at the table of contents for The Python Standard Library to get an idea of what’s available. A wide variety of third-party extensions are also available. Consult the Python Package Index to find packages of interest to you.\n\n\n9.12.3 Julia\nJulia is a high-level, open-source, general-purpose programming language designed for technical and scientific computing. It’s known for its fast performance, approaching that of languages like C and Fortran, while remaining relatively easy to use. Julia is particularly well-suited for tasks like numerical analysis, data science, and machine learning.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "data-tools.html#footnotes",
    "href": "data-tools.html#footnotes",
    "title": "9  Data tools",
    "section": "",
    "text": "It is challenging to make more precise estimates for this. The lower end of this estimate is most likely very conservative and based on historical information. The upper end of this estimate is based on Microsoft’s own estimation based on subscription to Microsoft 365. These estimates likely don’t include unlicensed or unauthorised usage of the software.↩︎\nTo see a list of free and proprietary software that use the ODF XML standard, see https://en.wikipedia.org/wiki/OpenDocument.↩︎",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Data tools</span>"
    ]
  },
  {
    "objectID": "all-about-spreadsheets.html",
    "href": "all-about-spreadsheets.html",
    "title": "10  All about spreadsheets",
    "section": "",
    "text": "10.1 History of spreadsheets\nA spreadsheet is a digital tool for organising, analysing, and storing data in tables, originally developed as an electronic version of paper accounting worksheets. It allows users to enter numerical or textual data and formulas that reference other cells, enabling dynamic calculations.\nSpreadsheets are interactive with users able to modify values and observe immediate changes in calculated results, facilitating “what-if” analysis. Beyond basic arithmetic, spreadsheets offer financial, statistical, and conditional functions, enhancing their analytical capabilities. Composed of rows (numbered) and columns (labelled with letters), cells are referenced by their alphanumeric coordinates (e.g., A1). Modern spreadsheet applications include multiple worksheets within a workbook, allowing for complex data management. It can also display data graphically, aiding in understanding trends and patterns.\nSpreadsheets have revolutionised business processes by replacing manual systems, offering versatility across various applications where tabular data is essential. Their dynamic cell referencing system allows for efficient and flexible data manipulation, making them indispensable in both professional and personal contexts.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>All about spreadsheets</span>"
    ]
  },
  {
    "objectID": "all-about-spreadsheets.html#sec-history-spreadsheet",
    "href": "all-about-spreadsheets.html#sec-history-spreadsheet",
    "title": "10  All about spreadsheets",
    "section": "",
    "text": "10.1.1 Paper spreadsheet\nThe concept of organising data into tabular formats dates back to ancient times, with examples such as Babylonian clay tablets from 1800 BCE as shown in Figure 10.1. In accounting, the term “spread sheet” was used by at least 1906 to describe a grid system in ledgers1.\n\n\n\n\n\n\nFigure 10.1: A Babylonian clay tablet believed to have been written around 1800 BC containing mathematical table written in cuneiform script\n\n\n\nBefore digital spreadsheets, “spread” referred to large, two-page layouts in publications. The evolution of the term “spread-sheet” reflects its transition from physical, oversized ledger pages with examples shown in Figure 10.2, Figure 10.3 to the digital tool we use today, emphasising their role in accounting and data organisation.\n\n\n\n\n\n\n\n\n\nFigure 10.2: Accounting ledger from 1911\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.3: Manifest of passengers of the Titanic\n\n\n\n\n\n\n\n\n10.1.2 Electronic spreadsheet\nFigure 10.4 shows key development milestones of the electronic spreadsheet.\n\n\n\n\n\n\n\n\nFigure 10.4: Timeline of development of the electronic spreadsheet",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>All about spreadsheets</span>"
    ]
  },
  {
    "objectID": "all-about-spreadsheets.html#sec-spreadsheet-database",
    "href": "all-about-spreadsheets.html#sec-spreadsheet-database",
    "title": "10  All about spreadsheets",
    "section": "10.2 Spreadsheets as databases",
    "text": "10.2 Spreadsheets as databases\nSpreadsheets and databases share similarities but are fundamentally different. A spreadsheet is essentially a single table, while a database consists of multiple tables with machine-readable relational structures. Although a spreadsheet workbook containing multiple sheets has interacting tables, it lacks the relational complexity of a database. Spreadsheets and databases are interoperable, however, with spreadsheets being able to be converted into database tables, and database queries being able to be exported to spreadsheets for analysis.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>All about spreadsheets</span>"
    ]
  },
  {
    "objectID": "all-about-spreadsheets.html#sec-spreadsheet-function",
    "href": "all-about-spreadsheets.html#sec-spreadsheet-function",
    "title": "10  All about spreadsheets",
    "section": "10.3 Spreadsheets as multi-function tools",
    "text": "10.3 Spreadsheets as multi-function tools\nSpreadsheets are widely used software tools for every step of the data pathway - data entry, storage, analysis, and visualisation. Spreadsheets are able to implement such functionality through an end-user development approach.\n\n10.3.1 End-user development\nSpreadsheets are designed as end-user development (EUD) tools. EUD refers to techniques in which non-professional developers are able to create automated tasks and complex data objects without in-depth knowledge of a programming language. Many find using spreadsheets for calculations and analysis much easier. This most likely stems from the following key features:\n\nEase of use\nSpreadsheets leverage spatial relationships, making it intuitive to establish program connections, unlike sequential programming which requires extensive text.\n\n\nForgiving nature\nPartial results and functions can operate even if other parts are incomplete or contain errors, simplifying the development process.\n\n\nVisual enhancements\nModern spreadsheets use colours, fonts, and lines to provide visual cues, aiding comprehension and organisation.\n\n\nAdvanced functionality\nExtensions enable users to create complex functions and integrate machine learning models, expanding their capabilities beyond basic calculations.\n\n\nVersatility\nBeyond numerical data, spreadsheets support Boolean logic, graphical design, and even SQL queries through relational data storage and formula-based expressions.\nIn essence, spreadsheets offer a flexible, powerful platform that caters to diverse tasks, making them an invaluable tool for many users despite their limitations compared to traditional programming environments.\n\n\n\n10.3.2 Limitations and shortcomings of spreadsheets\nUnfortunately, the same multi-functionality and features that make spreadsheets user-friendly and easily accessible to most also make them fragile, non-robust, and prone to causing/producing errors.\nIn order to be able to function as a tool for the various steps in the data pathway while still being user-friendly meant combining the data interface functionality (data storage and data access) with the programming/scripting capabilities (for data cleaning/processing, analysis, and visualisation) into a single graphical user interface with no clear distinction between them and no clear mechanism for programming/script testing. This brings about the following limitations and shortcomings:\n\nLack of standard mechanisms for management and quality assurance of spreadsheets produced by organisations\nGiven that data storage and access along with data processing/cleaning, analysis and visualisation capabilities sit side-by-side within the spreadsheet tool/software, developing routine and automated audit mechanisms for both data validity/quality and accuracy/correctness of data processing, analysis, and visualisation is nearly impossible. These audits will need to be done manually and line-by-line making them highly onerous. This is most likely the reason why a survey conducted in 2011 of nearly 1,500 people in the UK saw 72% reporting that no internal department checks their spreadsheets for accuracy, that only 13% said that internal audit reviews their spreadsheets, while a mere 1% receive checks from their risk department (Anon).\n\n\nReliability issues\nAn estimated 1% of all formulas in operational spreadsheets are in error (Powell, Baker & Lawson, 2009).\n\n\nPractical expressiveness is limited\nWhilst the graphical user interface of a spreadsheet using its cell-at-a-time approach is accessible and user-friendly for most users and for simple data operations, applying the same to a complex data model requiring more complicated calculations require tedious attention to detail. Users will tend to have difficulty remembering the meanings of hundreds or thousands of alphanumeric cell addresses that appear in per cell formulas.\n\n\nFormulas expressed in terms of cell addresses are hard to keep straight and hard to audit\nA research paper critically reviewing spreadsheet errors has shown that auditors who check both numerical results and the cell formulas find no more errors than auditors who only check numerical results (Powell, Baker & Lawson, 2008). By the nature of the cell-at-a-time approach, spreadsheets typically contain many copies of the same formula. Thus, when a formula needs to be edited, these edits will need to be applied to all cells containing that formula. This is in sharp contrast to a well-known principle in programming - do not repeat yourself or DRY - which emphasises the best practice of not repeating code to implement/achieve the same process or output. The DRY approach makes code implementation much more efficient and code auditing much more feasible and robust.\n\n\nMaintenance of volumes of spreadsheets is challenging\nCreating and managing a system to maintain vast amounts of spreadsheets for an individual or for an organisation is a challenging endeavour. Without built-in functionalities for proper security, version control and audit trails, and prevention of unintentional introduction of errors, it is more likely that management of spreadsheets end up being ad hoc, non-systematic, and tedious to implement.\n \n\n\n\n\nAnon. Spreadsheet risk management within UK organisations. Actuarial Post: For the Modern Actuary. (https://www.actuarialpost.co.uk/article/spreadsheet-risk-management-within-uk-organisations-351.htm, accessed 12 June 2025).\n\n\nMiddleton JH (1933). Baking costs. National Association of Cost Accountants Bulletin, 14(10).\n\n\nPowell S, Baker K, Lawson B (2009). Errors in operational spreadsheets. Journal of Organizational and End User Computing, 21(3):24–36.\n\n\nPowell SG, Baker KR, Lawson B (2008). A critical review of the literature on spreadsheet errors. Decision Support Systems, 46(1):128–138. doi:10.1016/j.dss.2008.06.001.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>All about spreadsheets</span>"
    ]
  },
  {
    "objectID": "all-about-spreadsheets.html#footnotes",
    "href": "all-about-spreadsheets.html#footnotes",
    "title": "10  All about spreadsheets",
    "section": "",
    "text": "“We maintain, in our general ledger, a so-called Spread Sheet which is a long sheet with the name of each individual plant in a particular column.” (from Middleton, 1933)↩︎",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>All about spreadsheets</span>"
    ]
  },
  {
    "objectID": "project-based-workflow.html",
    "href": "project-based-workflow.html",
    "title": "11  Project-based workflow",
    "section": "",
    "text": "11.1 Data processes as livestock rather than pets\nAs our skills as data analysts grow, we begin to understand that our ability to realise our full potential goes beyond the data tools that we have chosen to use or have been made to use. The importance of surrounding systems and infrastructure for ensuring reproducibility and long-term preservation of our work becomes increasingly significant. However, a lack of formal training or mentorship in managing these systems often leads to either feeling overwhelmed by technology or resorting to self-exploration without proper guidance.\nThis chapter aims to guide you gracefully into the exploration of this realm of efficient, effective, and reproducible data workflows. The concepts and practices discussed here may highlight current and existing practices that you have that are ineffectual, disorganised, and irreproducible. If so, we encourage you not to worry about these past mistakes but instead use them to raise the bar for your new work. Small but meaningful incremental changes add up over time, transforming your data quality of life.\nIn this chapter, we will discuss concepts and practices borrowed from the computational sciences field that use programming languages to record and automate their processes and translate them for use with the spreadsheet software that is sort of a hybrid with data processes implemented through both point-and-click steps via the mouse and through in cell commands or functions for performing calculations and operations. This translation as applied to spreadsheets is not high fidelity given the shortcomings and limitations of spreadsheets (as discussed in Section 10.3.2) but still provides enough structure and rigour compared to the typical and common ad hoc and unstructured use of spreadsheets.\nIn modern data and computing, a common analogy used to describe the management of data and computational processes is that of managing a herd of livestock compared to taking care of an individual household pet. For example, in cloud computing, individual servers are treated like “livestock” in that they can be easily destroyed and replaced via automation.\nIt is recommended that we adopt a similar mindset when managing our data and data processes - design and develop appropriate data systems that manage data and data processes as disposable and rebuilt and re-implemented as needed rather than as precious “pets”. We recommend this approach because if your workflow relies on an individual session or workspace in a non-reproducible way, it creates unnecessary risk and complexity. Instead, the focus should be on saving and relying on code and documentation to ensure reproducibility.\nApplying this approach with spreadsheets is not as straightforward given the peculiarities of the tool compared to programming languages that use code to record each step of the workflow. However, steps can be done that can facilitate as much reproducibility when using spreadsheets.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Project-based workflow</span>"
    ]
  },
  {
    "objectID": "project-based-workflow.html#sec-data-livestock",
    "href": "project-based-workflow.html#sec-data-livestock",
    "title": "11  Project-based workflow",
    "section": "",
    "text": "Figure 11.1: Microsoft Excel files as livestock\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.2: Microsoft excel file as a pet\n\n\n\n\n\n\n\n\n\n11.1.1 Detailed documentation for point-and-click mouse-based steps\nPoint-and-click steps in a workflow implemented using a mouse can be documented either in a specific worksheet within the spreadsheet that is just meant for documentation. The documentation can also be done on a separate document either in Word document (.docx) format or in a text-based format such as Markdown (.md) or text file (.txt) written using a text editor (see Tip 11.1 for recommendations on text editors for different operating software). This separate file should be included within the directory where the spreadsheet file is located (see Figure 11.3). An example of a text file documenting steps for data cleaning is shown in Figure 11.4.\n\n\n\n\n\n\n\n\n\nFigure 11.3: A text file for documentation notes on point-and-click mouse-based steps\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.4: An example documentation of point-and-click mouse-based steps and in cell functions and calculations\n\n\n\n\n\n\n\n\n11.1.2 Saving a source for written functions in spreadsheets\nSaving a text-based source file for the syntax of in cell functions and calculations used in a spreadsheet is one way of recording the non-mouse steps of the spreadsheet workflow. A text editor (rather than a word processor) would be ideal for this as the syntax of the function or formula will be shown more appropriately. If you are already using a text editor for documenting mouse-based steps of the spreadsheet workflow, it would be ideal to use the same text file to record in cell functions and calculations as shown in Figure 11.4.\n\n\n\n\n\n\nTip 11.1: Recommended text editors\n\n\n\nFollowing are recommended text editors for use with different operating software:\nFor Windows\nNotepad++ is a free source code editor and Notepad replacement that supports several languages. Running in the Microsoft Windows environment, its use is governed by GNU General Public License.\nFor Mac\nCodeEdit is an exciting new code editor written entirely and unapologetically for macOS. Develop any project using any language at speeds like never before with increased efficiency and reliability in an editor that feels right at home on your Mac.\n\n\nCreate a text file to associate with every spreadsheet project that you are working on. Save the text file within the same directory as the associated spreadsheet as shown in Figure 11.3.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Project-based workflow</span>"
    ]
  },
  {
    "objectID": "project-based-workflow.html#sec-spreadsheet-projects",
    "href": "project-based-workflow.html#sec-spreadsheet-projects",
    "title": "11  Project-based workflow",
    "section": "11.2 Organise work into projects",
    "text": "11.2 Organise work into projects\nOrganising work into projects is another best practice that provides organisational clarity for our data and data processes. Whilst this can be interpreted in many ways and that some of you may argue that you already organise your work with data in projects, the following key points give a clear indication/definition of what we mean by project-based workflows.\n\n11.2.1 File system discipline\nSimply put, this means putting all the files related to a single project in a designated folder. This applies to data, code, figures, notes, including the documentation and source files described earlier (see Figure 11.5). Depending on complexity of your project and on yours or your team’s/organisation’s preferences, you might enforce further organisation into subfolders. Related and relevant file system practices are discussed in Section 11.2.3.\n\n\n\n\n\n\nFigure 11.5: An example directory for the school nutrition project with all relevant files included\n\n\n\n\n\n11.2.2 File path discipline\nAll paths are relative and, by default, relative to the project’s folder. This is particularly important when you are referencing data found in one spreadsheet from within another spreadsheet for data analysis and visualisation. If files are within the same project, then relative paths make it easy to refer to associated or ancillary spreadsheets required for full analysis, visualisation, and reporting.\n\n\n11.2.3 File naming\n\nFile organisation and naming are powerful weapons against chaos.\n\n\n\n\n\n\n\nFigure 11.6: An example of messy file names\n\n\n\nBest practices in file naming are based on the following three principles:\n\nMachine-readable\nMachine-readable file names avoid spaces, punctuation, accented characters, and case sensitivity. Avoiding these makes file names easier to index and search via use of regular expressions and wild card matching or globbing.\nA regular expression, usually shortened as regex or regexp and sometimes referred to as rational expression, is a sequence of characters that specifies a match pattern in text. Usually such patterns are used by string-searching algorithms for “find” or “find and replace” operations on strings, or for input validation.\n\n\n\n\n\n\nFigure 11.7: Using regular expression to find all files that start with fig followed by a two-digit number\n\n\n\nGlobbing, also known as wildcard matching, is a technique used in computer systems to match multiple files or paths based on patterns containing wildcards like * (asterisk) and ? (question mark). It’s a common way to specify a set of files or paths in command-line interfaces, file managers, and programming languages. In simpler terms: globbing allows you to use patterns to find files that share a common naming structure, without having to specify each file individually.\n\n\n\n\n\n\nFigure 11.8: Using wildcard matching to find all PNG files\n\n\n\nMachine-readable file names have deliberate use of delimiters/space-holders such as underscore (_) or hyphen (-). The general rule is that _ is used to delimit units of metadata while - is used to delimit words so that they are more readable. This system allows for much easier recovery of metadata from file names.\n\n\nHuman-readable\nA file name is human-readable if it contains information on what the file contains. It should be easy to figure out what something is based on its file name. This is a similar concept to a slug from semantic URLs. A URL slug is the unique, identifiable portion of a web address (URL) that follows the domain name (e.g., “google.com”). It essentially acts as a “name tag” for a specific page or resource on a website, helping both users and search engines understand what the page is about.\n\n\nPlays well with default ordering\nFile names should play well with default ordering. This is usually achieved by:\n\nputting something numeric first in a file name;\n\n\n\n\n\n\n\nFigure 11.9: Putting something numeric first in a file name\n\n\n\n\nusing the ISO 8601 standard (YYYY-MM-DD) for dates; and,\npad the left side of other numbers with zeros.\n\n\n\n\n\n\n\nFigure 11.10: Left pad other numbers with zeros",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Project-based workflow</span>"
    ]
  },
  {
    "objectID": "project-based-workflow.html#sec-projects-gains",
    "href": "project-based-workflow.html#sec-projects-gains",
    "title": "11  Project-based workflow",
    "section": "11.3 Gains from project-based workflows",
    "text": "11.3 Gains from project-based workflows\nDeveloping the different project-based workflow habits described above collectively yields the most significant benefits. These practices ensure projects can move seamlessly across different computers or environments while maintaining reliability. Project-based workflow approach is a practical convention for achieving consistent behaviour across users and time comparable to societal norms like agreeing on traffic rules (e.g., driving on the left or right). Adhering to these conventions - whether in computing or in broader civilisation - constrains individual actions slightly for greater functionality and safety.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Project-based workflow</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html",
    "href": "data-entry-storage.html",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "",
    "text": "12.1 Be consistent\nSpreadsheets are often used as a multi-purpose tool for data entry, storage, analysis, and visualisation. Most spreadsheet software available allows users to perform all of these tasks. However, spreadsheets are best suited to data entry and storage. Analysis and visualisation should be done separately either by using other data tools or at least in a separate copy of the data file in order to reduce the risk of contaminating or accidentally changing the raw data in the spreadsheet.\nSpreadsheets, by design, make humans format data to be viewed by the human eye rather than to be readable by a machine (Murrell, 2013). Data structured as such require greater amount of effort, usually in computer code, to be able to extract the information needed for analysis. However, if the initial structure is such that it is easily readable by a machine, the effort leading to analysis is much more simplified.\nIn this chapter, we discuss best practices in using spreadsheets as a data entry and data storage tool and provide specific recommendations for organising spreadsheet data in a way that both humans and computers can read. Following these recommendations allows the creation of spreadsheets that minimise errors, are easy for computers to process, and facilitate collaboration and public access. These well-structured spreadsheets integrate with reproducible methods, serving as a reliable foundation for robust analytic workflows.\nConsistency is key in data organisation. Strive for uniformity in your data entry and organisation practices. By maintaining this consistency from the start, you can save yourself and your collaborators from the hassle of reconciling inconsistencies later on.\nFollowing are some examples of being consistent and why it is important. Some of these examples are also part of the recommendations listed here.\nFor a categorical variable like the sex, use a single common value for males (e.g., “male”), and a single common value for females (e.g., “female”). Do not sometimes write “M,” sometimes “male,” and sometimes “Male.” Pick one and stick to it. In order to limit the occurrence of this inconsistency, you can enforce a data validation rule for this variable (see Section 12.11).\nIt is ideal to have every cell filled in so that distinguishing between truly missing values and unintentionally missing values is more straightforward. Decide right at the outset what value to use for missing values and stick with that value throughout. Do not use a note explaining why a value is missing in place of the data itself. Rather, make a separate column with such notes.\nName variables exactly the same way throughout one file and across every other file relevant to the project. If naming is inconsistent for the same variable, those working with the data will have to work out that these are all really the same thing. See Section 12.2 for more discussion on best practices in naming things within a data file. See Section 11.2.3 for an in-depth discussion on best practices in naming files.\nCreate consistent and unique subject identifiers to avoid extra work in figuring out which subject/record if which.\nIf your data are in multiple files and you use different layouts in different files, it will be extra work for the analyst to combine the files into one dataset for analysis. With a consistent structure, it will be easy to automate this process.\nHave some system for naming files. Keeping a consistent file naming scheme will help ensure that your files remain well organised, and it will make it easier to batch process the files if you need to. See an in-depth discussion of file naming in Section 11.2.3.\nPreferably use the standard format YYYY-MM-DD, for example, 2015-08-01. If sometimes you write 8/1/2015 and sometimes 8-1-15, it will be more difficult to use the dates in analyses or data visualisations. See Section 12.3 for more discussion on this.\nIf you have a separate column of notes (e.g., \"dead\"), be consistent in what you write. Do not sometimes write \"dead\" and sometimes \"Dead\".\nA blank cell is different than a cell that contains a single space. And \"male\" is different from \" male \" (i.e., with spaces at the beginning and end).",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-be-consistent",
    "href": "data-entry-storage.html#sec-spreadsheet-data-be-consistent",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "",
    "text": "Use consistent codes for categorical variables.\n\n\n\nUse a consistent fixed code for any missing values.\n\n\n\nUse consistent variable names.\n\n\n\nUse consistent subject identifiers.\n\n\n\nUse a consistent data layout in multiple files.\n\n\n\nUse consistent file names.\n\n\n\nUse a consistent format for all dates.\n\n\n\nUse consistent phrases in your notes.\n\n\n\nBe careful about extra spaces within cells.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-good-names",
    "href": "data-entry-storage.html#sec-spreadsheet-data-good-names",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.2 Choose good names for things",
    "text": "12.2 Choose good names for things\nIt is important to pick good names for things such as variables. This can be hard, and so it is worth putting some time and thought into it. Section 11.2.3 provides some general principles for naming files that can also be used when naming variables in data.\n\n12.2.1 General rules for naming\n\nDo not use spaces, either in variable names or file names\n\nSpaces make programming harder. An analyst will need to surround a name that contains spaces in double quotes in order to refer to it. Use underscores (_) or hyphens (-) instead of spaces. Do not use a mixture of underscores and hyphens; pick one and be consistent.\n\nNo extraneous spaces at the start and/or end of variable names\n\nBe careful about extraneous spaces at the beginning or end of a variable name. \"sex\" is different from \"sex \" (with an extra space at the end) or \" sex\" (with an extra space at the start).\n\nAvoid special characters, except for underscores and hyphens\n\nOther symbols ($, @, %, #, &, *, (, ), !, /, etc.) often have special meaning in programming languages, and so they can be harder to handle. They are also a bit harder to type.\nThe main principle in choosing names, whether for variables or for file names, is short, but meaningful. So not too short. The following table (adapted from The Data Carpentry lesson on using spreadsheets) show good and bad example variables names.\n\n\n\n\nTable 12.1: Examples of good and bad variable names\n\n\n\n\n\n\nGood name\nGood alternative\nAvoid\n\n\n\n\nmax_temp_c\nMaxTemp\nMaximum Temp (°C)\n\n\nprecipitation_mm\nPrecipitation\nprecmm\n\n\nmean_year_growth\nMeanYearGrowth\nMean growth/year\n\n\nsex\nsex\nM/F\n\n\nweight\nweight\nw.\n\n\ncell_type\nCellType\nCell type\n\n\nobservation_01\nfirst_observation\n1st Obs.\n\n\n\n\n\n\n\n\nThe first column of variable names use the snake case naming convention which uses an underscore to replace a space and letters are in lower case. The second column of good alternative variable names use the camel case naming convention in which phrases are written without spaces or punctuation and with capitalised words.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-date",
    "href": "data-entry-storage.html#sec-spreadsheet-data-date",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.3 Write dates as YYYY-MM-DD",
    "text": "12.3 Write dates as YYYY-MM-DD\n\n\n\n\n\n\nFigure 12.1: ISO 8601 from https://xkcd.com/1179/\n\n\n\nWhen entering dates, we strongly recommend using the global ISO 8601 standard, YYYY-MM-DD, such as 2013-02-27.\nMicrosoft Excel’s treatment of dates can cause problems in data. It stores them internally as a number, with different conventions on Windows and Macs (see Note 12.1). So, you may need to manually check the integrity of your data when they come out of Excel.\n\n\n\n\n\n\nNote 12.1: Date systems in Microsoft Excel\n\n\n\nExcel supports two date systems, the 1900 date system and the 1904 date system. Each date system uses a unique starting date from which all other workbook dates are calculated. All newer versions of Excel calculate dates based on the 1900 date system, while older versions used the 1904 system.\nWhen you copy dates from a workbook created in an earlier version to a workbook created in a newer version, they will be converted automatically unless the option to \"Automatically convert date system\" is disabled in Preferences &gt; Edit &gt; Date Options. If this option is disabled, you will receive a message asking whether the dates should be converted when pasted. You have two options. You can convert the dates to use the 1900 date system (recommended). This option makes the dates compatible with other dates in the workbook. Or you can keep the 1904 date system for the pasted dates only.\nThe 1900 date system\nIn the 1900 date system, dates are calculated by using January 1, 1900, as a starting point. When you enter a date, it is converted into a serial number that represents the number of days elapsed since January 1, 1900. For example, if you enter July 5, 2011, Excel converts the date to the serial number 40729. This is the default date system in Excel for Windows, Excel 2016 for Mac, and Excel for Mac 2011. If you choose to convert the pasted data, Excel adjusts the underlying values, and the pasted dates match the dates that you copied.\nThe 1904 date system\nIn the 1904 date system, dates are calculated by using January 1, 1904, as a starting point. When you enter a date, it is converted into a serial number that represents the number of days elapsed since January 1, 1904. For example, if you enter July 5, 2011, Excel converts the date to the serial number 39267. This is the default date system in earlier versions of Excel for Mac. If you choose not to convert the data and keep the 1904 date system, the pasted dates vary from the dates that you copied.\nThe difference between the date systems\nBecause the two date systems use different starting days, the same date is represented by different serial numbers in each date system. For example, July 5, 2011, can have two different serial numbers, as follows:\n\n\n\nTable 12.2: Comparison of Excel’s 1900 and 1904 date systems\n\n\n\n\n\nDate System\nSerial Number\n\n\n\n\n1900\n40729\n\n\n1904\n39267\n\n\n\n\n\n\nThe difference between the two date systems is 1,462 days. This means that the serial number of a date in the 1900 date system is always 1,462 days greater than the serial number of the same date in the 1904 date system. 1,462 days is equal to four years and one day (including one leap day).\n*taken from Microsoft Support documentation\n\n\nTo avoid these issues with dates when using spreadsheets (specifically Excel), we recommend the following:\n\nUse a plain text format for columns in an Excel worksheet that are going to contain dates\n\nDoing so will avoid automatic conversion of these variables into often unpredictable formats. This can be done through the following steps:\n\n\n\n\n\n\n\n\n\nFigure 12.2: Create date variable\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.3: Select date variable\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.4: Set format to text\n\n\n\n\n\n\nStep 1: Create a date variable (see Figure 12.2).\nStep 2. Select the date variable you created (see Figure 12.3).\nStep 3: In the menu bar, select Format –&gt; Cells –&gt; Choose “Text” on the left (see Figure 12.4).\nThis approach will only work if you are creating the date variable first and when no date values have been entered yet. If you do this on a date variable that already contain dates, Excel will convert them to a text value of their underlying numeric representation (as described in Note 12.1).\n\nPlace an apostrophe at the start of a date value entry\n\nAnother way to force Excel to treat dates as text is to begin the date with an apostrophe, like this: '2014-06-14. Excel will treat the cells as text, but the apostrophe will not appear when you view the spreadsheet or export it to other formats. This is a handy trick, but it requires impeccable diligence and consistency.\n\n\n\n\n\n\nFigure 12.5: Enter an apostrophe followed by YYYY-MM-DD date format\n\n\n\n\nCreate three separate columns with year, month, and day\n\nThese will be ordinary numbers, and so Excel will not mess them up. If there is an existing date variable already, you can convert that to year, month, and day columns by using the built-in date functions in Excel that extract year (Figure 12.6), month (Figure 12.7), and day (Figure 12.8) values from a date variable.\n\n\n\n\n\n\n\n\n\nFigure 12.6: Extract year from date value\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.7: Extract month from date value\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.8: Extract year from date value\n\n\n\n\n\n\n\nRepresent dates as an 8-digit integer of the form YYYYMMDD\n\nFor example, 20140614 for 2014-06-14 (see Figure 12.9).\n\n\n\n\n\n\nFigure 12.9: Date value as text in YYYYMMDD format",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-no-empty-cells",
    "href": "data-entry-storage.html#sec-spreadsheet-no-empty-cells",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.4 No empty cells",
    "text": "12.4 No empty cells\nFill in all cells. Use some common code for missing data to make it clear that the data are known to be missing rather than unintentionally left blank.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-one-thing-cell",
    "href": "data-entry-storage.html#sec-spreadsheet-data-one-thing-cell",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.5 Put just one thing in a cell",
    "text": "12.5 Put just one thing in a cell\nYour spreadsheet should have cells that each contain one piece of data only. Putting more than one type of data value in a cell is not considered best practice.\nFor example, you might have a column with information on year (containing values of either 2022, 2023, or 2024) and sex (Male or Female) as year-sex such as 2022-Male, 2022-Female, and so on and so forth. It would be better to separate this into year and sex columns (containing 2022 and Male).\n\n\n\n\n\n\n\n\n\nFigure 12.10: Combined year-sex variable\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.11: Year and sex as separate variables\n\n\n\n\n\n\nOr you might include units alongside measurements such as weight. It is better to have a variable for the weight and then a separate variable for the units.\n\n\n\n\n\n\n\n\n\nFigure 12.12: Combined weight-unit variable\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.13: Separate weight and unit variables\n\n\n\n\n\n\nIt is even better to just have a variable for weight and then document the units used in a separate data dictionary (see Section 12.7 on creating a data dictionary).\nFinally, do not merge cells. It might look pretty, but you end up breaking the rule of no empty cells.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-rectangle",
    "href": "data-entry-storage.html#sec-spreadsheet-data-rectangle",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.6 Make it a rectangle",
    "text": "12.6 Make it a rectangle\nA single big rectangle with rows corresponding to subjects and columns corresponding to variables is the best layout for data within a spreadsheet. The first row should always contain variable names. Do not use more than one row for the variable names.\n\n\n\n\n\n\nFigure 12.14: Data in spreadsheet with rectangular layout",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-dictionary",
    "href": "data-entry-storage.html#sec-spreadsheet-data-dictionary",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.7 Create a dictionary",
    "text": "12.7 Create a dictionary\nHaving a separate file (see Figure 12.15) that outlines all variables can be very helpful, particularly if it’s organised in a structured layout so data analysts can use it effectively in their analyses. We recommend that this data dictionary includes the following information:\n\nThe precise variable names as they appear in the dataset.\nA detailed description explaining what the variable represents.\nThe measurement units associated with each variable.\nThe list of possible values (for categorical variables) and/or typical range of values expected (for numerical variables) for that variable.\n\nAn example of this data dictionary within an accompanying Word document metadata in a project-based workflow is shown in Figure 12.16.\n\n\n\n\n\n\n\n\n\nFigure 12.15: A project-based workflow structure with a separate file for metadata\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.16: A recommended layout and contents of a project metadata containing a data dictionary\n\n\n\n\n\n\nAn alternative to a separate metadata and data dictionary file is to include this documentation as a separate worksheet within the spreadsheet containing the data.\n\n\n\n\n\n\n\n\n\nFigure 12.17: A project-based workflow structure with separate file for data\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.18: Project spreadsheet with raw data in worksheet called “main”\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.19: Project spreadsheet with metadata/dictionary in worksheet called “metadata”",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-calculations",
    "href": "data-entry-storage.html#sec-spreadsheet-data-calculations",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.8 No calculations in the raw data file",
    "text": "12.8 No calculations in the raw data file\nExcel files often come with various calculations and graphs included alongside the data itself. We strongly recommend keeping your primary data file free from any additional content - only raw data should be present. This is because editing the same file for calculations can lead to accidental errors. For instance, when you open a file and start typing without selecting a cell, the text might end up in unexpected cells, causing problems during analysis. To prevent this, protect your main data file from changes, keep it backed up, and refrain from making edits. If you need to perform analyses or create graphs, work on a duplicate of the original file.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-colour-highlighting",
    "href": "data-entry-storage.html#sec-spreadsheet-data-colour-highlighting",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.9 Do not use font colour or highlighting as data",
    "text": "12.9 Do not use font colour or highlighting as data\nYou might be tempted to highlight particular cells with suspicious data, or rows that should be ignored. Or the font or font colour might have some meaning. Instead, add another column with an indicator variable (e.g., ”trusted” with values TRUE or FALSE).\nAnother possible use of highlighting would be to indicate males and females in a mouse study by highlighting the corresponding rows in different colours. But rather than use highlighting to indicate sex, it is better to include a sex column, with values Male or Female.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-backup",
    "href": "data-entry-storage.html#sec-spreadsheet-data-backup",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.10 Make backups",
    "text": "12.10 Make backups\nRegularly back up your data by storing copies in multiple locations. Consider using a formal version control system such as git (though it’s not ideal for data files). Keep every version of your data files and label them with version numbers, like file_v1.xlsx, file_v2.xlsx, and so on and so forth. For this, the guidance on good file names discussed in Section 11.2.3 is a good reference to follow for naming your file versions. Once you’ve finished entering data or if taking a break, protect the file from accidental changes by setting it to read-only as described below.\n\nWindowsmacOS\n\n\n1, Right-click the file in File Explorer or select the file and then click on the triple dot icon in File Explorer as shown in Figure 12.20.\n\n\n\n\n\n\nFigure 12.20: See more file options in File Explorer\n\n\n\n\nIn the drop-down menu, choose Properties as shown in Figure 12.21.\n\n\n\n\n\n\n\nFigure 12.21: Select Properties option in the drop-down menu\n\n\n\n\nIn the pop-up menu, navigate to the General tab, check the Attributes box for Read-only, and confirm with OK as shown in Figure 12.22.\n\n\n\n\n\n\n\nFigure 12.22: Set the file to read-only\n\n\n\n\n\n\nOpen Finder and right-click on the file.\n\n\n\n\n\n\n\nFigure 12.23: Right-click on the spreadsheet file\n\n\n\n\nSelect Get Info then go to Sharing & Permissions.\n\n\n\n\n\n\n\n\n\n\nFigure 12.24: Select Get Info\n\n\n\n\n\n\n\n\n\n\n\nFigure 12.25: Go to Sharing & Permissions\n\n\n\n\n\n\n\nSet your privileges to Read only.\n\n\n\n\n\n\n\nFigure 12.26: Set privileges to Read only\n\n\n\n\n\n\nAnother option is to password protect the worksheet in the spreadsheet that has the data.\n\nSelect the worksheet to password-protect and then go to Review as shown in Figure 12.31.\n\n\n\n\n\n\n\nFigure 12.27: Go to Review\n\n\n\n\nSelect Protect Sheet as shown in Figure 12.28.\n\n\n\n\n\n\n\nFigure 12.28: Select Protect Sheet\n\n\n\n\nEnter password to protect worksheet as shown in Figure 12.29.\n\n\n\n\n\n\n\nFigure 12.29: Enter password to protect worksheet\n\n\n\n\nRe-enter password to confirm protection as shown in Figure 12.30.\n\n\n\n\n\n\n\nFigure 12.30: Re-enter password to confirm protection\n\n\n\nPassword protection can also be applied to the whole spreadsheet workbook.\n\nOpen the spreadsheet to password-protect and then go to Review as shown in Figure 12.31.\n\n\n\n\n\n\n\nFigure 12.31: Go to Review\n\n\n\n\nSelect Protect Workbook as shown in Figure 12.32.\n\n\n\n\n\n\n\nFigure 12.32: Select Protect Workbook\n\n\n\n\nEnter password to protect workbook as shown in Figure 12.33.\n\n\n\n\n\n\n\nFigure 12.33: Enter password to protect workbook\n\n\n\n\nRe-enter password to confirm workbook protection as shown in Figure 12.34.\n\n\n\n\n\n\n\nFigure 12.34: Re-enter password to confirm workbook protection\n\n\n\n\n\n\n\n\n\nImportant 12.1\n\n\n\nRemember, always back up your data!",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "data-entry-storage.html#sec-spreadsheet-data-validation",
    "href": "data-entry-storage.html#sec-spreadsheet-data-validation",
    "title": "12  Data entry/collection and storage using spreadsheets",
    "section": "12.11 Use data validation to avoid errors",
    "text": "12.11 Use data validation to avoid errors\nWhen handling data entry tasks using spreadsheets, it’s crucial to aim for accuracy and comfort to minimise errors and reduce physical strain. Excel provides a helpful Data Validation feature that can prevent errors during data entry.\nTo use this feature:\n\nChoose the column you wish to validate. In Figure 12.35, the data entry for the age variable is to be validated.\n\n\n\n\n\n\n\nFigure 12.35: Add validation to the age column/variable\n\n\n\n\nGo to the menu bar and select Data --&gt; Data Tools --&gt; Data Validation as shown in\n\n\n\n\n\n\n\nFigure 12.36: Select Data Validation in Data Tools\n\n\n\n\nSet up validation criteria such as:\n\nWhole numbers within a specified range\nDecimal numbers within a specified range\nA predefined list of acceptable values\nText with length restrictions\n\n\nIn Figure 12.37, we set validation for age variable to allow only whole numbers ranging from 120 to 191 (inclusive). The Ignore blank option is ticked so that blank entry will be accepted.\n\n\n\n\n\n\nFigure 12.37: Setup validation criteria for age variable\n\n\n\n\nAdd title and message to guide data entry input as shown in Figure 12.38.\n\n\n\n\n\n\n\nFigure 12.38: Add title and message guide for data entry\n\n\n\n\nAdd error alert to show up when incorrect data entry input is made as shown in Figure 12.39\n\n\n\n\n\n\n\nFigure 12.39: Add error alert to show up when incorrect data entry input is made\n\n\n\nMicrosoft Support has further documentation and guidance on how to apply data validation to cells here.\nAdditionally, formatting cells as “Text” can prevent unintended changes to data like dates or names. In Section 12.3, the steps to change formatting of cells is described and demonstrated.\nWhile these steps may seem tedious, they are valuable in maintaining data integrity and minimising errors during entry.\n\n\n\n\n\nMurrell P (2013). Data intended for human consumption, not machine consumption. Bad data handbook. Sebastopol, CA: O’Reilly Media; 2013:31–51.",
    "crumbs": [
      "Data Management",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data entry/collection and storage using spreadsheets</span>"
    ]
  },
  {
    "objectID": "exploratory-data-analysis.html",
    "href": "exploratory-data-analysis.html",
    "title": "13  Exploratory data analysis",
    "section": "",
    "text": "13.1 Definitions\nData analysis involves steps like cleaning, transforming, inspecting, and modelling data to extract meaningful information. This process can serve various purposes, including exploratory and confirmatory analyses, as well as descriptive or predictive tasks.\nBefore building models or making predictions, it’s essential to explore the data to identify underlying patterns and structures. Data analysts employ both numerical and visual techniques to uncover insights that might be hidden within the dataset. However, it’s crucial for analysts to avoid over-interpreting apparent patterns and to ensure that the findings are reliable for the given data and potentially applicable to new datasets as well. Exploratory data analysis fills this role.\nFollowing are a few other definitions of exploratory data analysis (EDA).\nFrom Wikipedia:\nFrom Wickham and Grolemund (2023):\nFrom SAS:",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "exploratory-data-analysis.html#sec-eda-definition",
    "href": "exploratory-data-analysis.html#sec-eda-definition",
    "title": "13  Exploratory data analysis",
    "section": "",
    "text": "In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n\n\n\nEDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends.\n\n\n\nEDA is necessary for the next stage of data research. If there was an analogy to exploratory data analysis, it would be that of a painter examining their tools and available time, before deciding on what best to paint.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "exploratory-data-analysis.html#sec-eda-origins",
    "href": "exploratory-data-analysis.html#sec-eda-origins",
    "title": "13  Exploratory data analysis",
    "section": "13.2 Origins",
    "text": "13.2 Origins\nThe field of EDA got into the forefront with the publication of Tukey’s Exploratory Data Analysis (Tukey, 1977). Tukey’s aim in writing the book was to provide individual and isolated techniques useful to data analysts. All of Tukey’s techniques in the EDA book can be done by hand with pencil and paper.\n\n\n\n\n\n\n\n\n\nFigure 13.1: Book cover of Tukey’s Exploratory Data Analysis\n\n\n\n\n\nFollowing are some quotes by Tukey from the EDA book.\n\n13.2.1 On measures\nIt is important to understand what you can do before you learn to measure how well you seem to have done it.\n\n\n13.2.2 On pictures\nThe greatest value of a picture is when it forces us to notice what we never expected to see.\n\n\n13.2.3 On exploration\nOnce upon a time, statisticians only explored.\n\n\n13.2.4 On not having one right answer\nThere can be many ways to approach a body of data. Not all are equally good.\n\n\n\n\nTukey JW (1977). Exploratory data analysis. Reading (Mass.) Menlo Park (Calif.) London [etc.]: Addison-Wesley publ Addison-wesley series in behavioral science.\n\n\nWickham H, Çetinkaya-Rundel M, Grolemund G (2023). R for data science: Import, tidy, transform, visualize, and model data, Second Edition. Bejing: O’Reilly.",
    "crumbs": [
      "Exploratory Data Analysis",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Exploratory data analysis</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Anon. Spreadsheet risk management within UK organisations.\nActuarial Post: For the Modern Actuary. (https://www.actuarialpost.co.uk/article/spreadsheet-risk-management-within-uk-organisations-351.htm,\naccessed 12 June 2025).\n\n\nBean R (2017). How companies say they’re using big data. Harvard\nBusiness Review. (https://hbr.org/2017/04/how-companies-say-theyre-using-big-data).\n\n\nCarroll SR, Rodriguez-Lonebear D, Martinez A (2019). Indigenous data\ngovernance: Strategies from united states native nations.\nCODATA, 18(1):31. doi:10.5334/dsj-2019-031.\n\n\nChoi Y, Gil-Garcia J, Burke GB, Costello J, Werthmuller D, Aranay O\n(2021). Towards data-driven decision-making in government: Identifying\nopportunities and challenges for data use and analytics. Hawaii\ninternational conference on system sciences. doi:10.24251/HICSS.2021.268.\n\n\nIvacko TM, Horner D, Crawford MQ (2013). Data-driven decision-making in\nmichigan local government. SSRN Journal. doi:10.2139/ssrn.2351916.\n\n\nMiddleton JH (1933). Baking costs. National Association of Cost\nAccountants Bulletin, 14(10).\n\n\nMurrell P (2013). Data intended for human consumption, not machine\nconsumption. Bad data handbook. Sebastopol, CA:\nO’Reilly Media; 2013:31–51.\n\n\nOnunga J, Odongo P (2025). Digital transformation in public\nadministration and data-driven decision-making: A review of turkana\ncounty government. IJRIAS, IX:234–240. doi:10.51584/IJRIAS.2024.912022.\n\n\nPowell S, Baker K, Lawson B (2009). Errors in operational spreadsheets.\nJournal of Organizational and End User Computing, 21(3):24–36.\n\n\nPowell SG, Baker KR, Lawson B (2008). A critical review of the\nliterature on spreadsheet errors. Decision Support Systems,\n46(1):128–138. doi:10.1016/j.dss.2008.06.001.\n\n\nSayogo DS, Yuli SBC, Amalia FA (2024). Data-driven decision-making\nchallenges of local government in indonesia. TG,\n18(1):145–156. doi:10.1108/TG-05-2023-0058.\n\n\nStobierski T (2019). The advantages of data-driven decision-making.\nBusiness insights [web site]. (https://online.hbs.edu/blog/post/data-driven-decision-making,\naccessed 12 May 2025).\n\n\nTukey JW (1977). Exploratory data analysis. Reading (Mass.)\nMenlo Park (Calif.) London [etc.]: Addison-Wesley publ Addison-wesley\nseries in behavioral science.\n\n\nUnited Nations General Assembly (2007). United nations declaration on\nthe rights of indigenous peoples : Resolution / adopted by the general\nassembly. (https://www.refworld.org/legal/resolution/unga/2007/en/49353).\n\n\nWickham H, Çetinkaya-Rundel M, Grolemund G (2023). R for data\nscience: Import, tidy, transform, visualize, and model data, Second\nEdition. Bejing: O’Reilly.",
    "crumbs": [
      "References"
    ]
  }
]